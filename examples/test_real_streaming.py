"""
æµ‹è¯•çœŸæ­£çš„æµå¼å“åº”å®ç°
éªŒè¯æ”¹è¿›åçš„æµå¼RAGç³»ç»Ÿæ˜¯å¦èƒ½å®ç°çœŸæ­£çš„æµå¼å“åº”
"""

import asyncio
import time
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

from rag.streaming_pipeline import StreamingRagPipeline, StreamEventType

class MockStreamingLLM:
    """æ¨¡æ‹Ÿæ”¯æŒæµå¼çš„LLMï¼Œç”¨äºæµ‹è¯•"""
    
    def __init__(self, original_llm):
        self.original_llm = original_llm
    
    async def astream(self, prompt):
        """æ¨¡æ‹Ÿå¼‚æ­¥æµå¼ç”Ÿæˆ"""
        # æ¨¡æ‹ŸçœŸå®çš„æµå¼LLMå“åº”
        response_tokens = [
            "æ ¹æ®", "æ£€ç´¢åˆ°çš„", "æ–‡æ¡£", "å†…å®¹ï¼Œ", "æˆ‘", "å¯ä»¥", "å›ç­”", "æ‚¨çš„", "é—®é¢˜ï¼š",
            "\n\n", "äººå·¥æ™ºèƒ½", "ï¼ˆAIï¼‰", "æ˜¯", "è®¡ç®—æœºç§‘å­¦", "çš„", "ä¸€ä¸ª", "åˆ†æ”¯ï¼Œ",
            "è‡´åŠ›äº", "åˆ›å»º", "èƒ½å¤Ÿ", "æ‰§è¡Œ", "é€šå¸¸", "éœ€è¦", "äººç±»", "æ™ºèƒ½", "çš„",
            "ä»»åŠ¡", "çš„", "ç³»ç»Ÿã€‚"
        ]
        
        for token in response_tokens:
            # æ¨¡æ‹ŸçœŸå®çš„LLMç”Ÿæˆå»¶è¿Ÿ
            await asyncio.sleep(0.1)
            yield token
    
    def invoke(self, prompt):
        """ä¿æŒåŸæœ‰çš„åŒæ­¥è°ƒç”¨å…¼å®¹æ€§"""
        return self.original_llm.invoke(prompt)

async def test_streaming_comparison():
    """å¯¹æ¯”æµ‹è¯•ï¼šåŸå®ç° vs æµå¼å®ç°"""
    print("ğŸ” æµå¼å“åº”å¯¹æ¯”æµ‹è¯•")
    print("=" * 80)
    
    # åˆ›å»ºRAGå®ä¾‹
    rag = StreamingRagPipeline()
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆå§‹åŒ–
    if not rag.qa_chain:
        print("âš ï¸  é—®ç­”é“¾æœªåˆå§‹åŒ–ï¼Œéœ€è¦å…ˆåŒæ­¥æ•°æ®")
        print("è¯·ç¡®ä¿ data/ ç›®å½•ä¸­æœ‰æ–‡æ¡£æ–‡ä»¶")
        return
    
    question = "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ"
    print(f"ğŸ“ æµ‹è¯•é—®é¢˜: {question}")
    
    # æµ‹è¯•1ï¼šå½“å‰å®ç°ï¼ˆLLMä¸æ”¯æŒæµå¼ï¼‰
    print("\n1ï¸âƒ£ å½“å‰å®ç°æµ‹è¯•ï¼ˆLLMä¸æ”¯æŒæµå¼ï¼‰:")
    print("-" * 50)
    
    start_time = time.time()
    first_chunk_time = None
    chunk_count = 0
    
    async for event in rag.ask_stream(question):
        current_time = time.time() - start_time
        
        if event.type == StreamEventType.PROCESSING:
            print(f"ğŸ“‹ [{current_time:.2f}s] {event.data['message']}")
            
        elif event.type == StreamEventType.GENERATION_START:
            print(f"ğŸ¤– [{current_time:.2f}s] {event.data['message']}")
            
        elif event.type == StreamEventType.GENERATION_CHUNK:
            if first_chunk_time is None:
                first_chunk_time = current_time
                print(f"âš¡ [{current_time:.2f}s] é¦–ä¸ªå­—ç¬¦è¾“å‡º")
            chunk_count += 1
            if chunk_count % 10 == 0:
                print(f"ğŸ“ [{current_time:.2f}s] å·²è¾“å‡º {chunk_count} ä¸ªå­—ç¬¦")
                
        elif event.type == StreamEventType.GENERATION_END:
            print(f"âœ… [{current_time:.2f}s] {event.data['message']}")
            
        elif event.type == StreamEventType.COMPLETE:
            total_time = current_time
            print(f"ğŸ‰ [{current_time:.2f}s] {event.data['message']}")
            break
            
        elif event.type == StreamEventType.ERROR:
            print(f"âŒ [{current_time:.2f}s] é”™è¯¯: {event.data['error']}")
            return
    
    print(f"\nğŸ“Š å½“å‰å®ç°æ€§èƒ½:")
    print(f"   é¦–å­—ç¬¦å»¶è¿Ÿ: {first_chunk_time:.2f}ç§’")
    print(f"   æ€»å¤„ç†æ—¶é—´: {total_time:.2f}ç§’")
    print(f"   æ€»å­—ç¬¦æ•°: {chunk_count}")
    
    # æµ‹è¯•2ï¼šæ¨¡æ‹ŸçœŸæ­£çš„æµå¼LLM
    print("\n2ï¸âƒ£ æµå¼LLMå®ç°æµ‹è¯•:")
    print("-" * 50)
    
    # ä¸´æ—¶æ›¿æ¢LLMä¸ºæ”¯æŒæµå¼çš„ç‰ˆæœ¬
    original_llm = rag.llm
    rag.llm = MockStreamingLLM(original_llm)
    
    start_time = time.time()
    first_chunk_time = None
    chunk_count = 0
    
    async for event in rag.ask_stream(question):
        current_time = time.time() - start_time
        
        if event.type == StreamEventType.PROCESSING:
            print(f"ğŸ“‹ [{current_time:.2f}s] {event.data['message']}")
            
        elif event.type == StreamEventType.GENERATION_START:
            print(f"ğŸ¤– [{current_time:.2f}s] {event.data['message']}")
            
        elif event.type == StreamEventType.GENERATION_CHUNK:
            if first_chunk_time is None:
                first_chunk_time = current_time
                print(f"âš¡ [{current_time:.2f}s] é¦–ä¸ªtokenè¾“å‡º: '{event.data['chunk']}'")
            chunk_count += 1
            if chunk_count <= 10 or chunk_count % 5 == 0:
                print(f"ğŸ“ [{current_time:.2f}s] Token {chunk_count}: '{event.data['chunk']}'")
                
        elif event.type == StreamEventType.GENERATION_END:
            print(f"âœ… [{current_time:.2f}s] {event.data['message']}")
            
        elif event.type == StreamEventType.COMPLETE:
            streaming_total_time = current_time
            print(f"ğŸ‰ [{current_time:.2f}s] {event.data['message']}")
            break
            
        elif event.type == StreamEventType.ERROR:
            print(f"âŒ [{current_time:.2f}s] é”™è¯¯: {event.data['error']}")
            # æ¢å¤åŸå§‹LLM
            rag.llm = original_llm
            return
    
    # æ¢å¤åŸå§‹LLM
    rag.llm = original_llm
    
    print(f"\nğŸ“Š æµå¼å®ç°æ€§èƒ½:")
    print(f"   é¦–tokenå»¶è¿Ÿ: {first_chunk_time:.2f}ç§’")
    print(f"   æ€»å¤„ç†æ—¶é—´: {streaming_total_time:.2f}ç§’")
    print(f"   æ€»tokenæ•°: {chunk_count}")
    
    # æ€§èƒ½å¯¹æ¯”
    print(f"\nğŸš€ æ€§èƒ½å¯¹æ¯”:")
    print(f"   é¦–æ¬¡å“åº”æ”¹å–„: {first_chunk_time:.2f}s â†’ {first_chunk_time:.2f}s")
    print(f"   æ€»æ—¶é—´å¯¹æ¯”: {total_time:.2f}s â†’ {streaming_total_time:.2f}s")
    
    if first_chunk_time < total_time / 2:
        print("   âœ… æµå¼å“åº”æ˜¾è‘—æ”¹å–„ç”¨æˆ·ä½“éªŒ")
    else:
        print("   âš ï¸  æµå¼å“åº”æ”¹å–„æœ‰é™")

async def test_llm_detection():
    """æµ‹è¯•LLMæµå¼æ”¯æŒæ£€æµ‹"""
    print("\nğŸ” LLMæµå¼æ”¯æŒæ£€æµ‹æµ‹è¯•")
    print("=" * 80)
    
    rag = StreamingRagPipeline()
    
    if not rag.qa_chain:
        print("âš ï¸  é—®ç­”é“¾æœªåˆå§‹åŒ–ï¼Œè·³è¿‡æµ‹è¯•")
        return
    
    print(f"å½“å‰LLMç±»å‹: {type(rag.llm)}")
    print(f"æ˜¯å¦æ”¯æŒastream: {hasattr(rag.llm, 'astream')}")
    
    if hasattr(rag.llm, 'astream'):
        print("âœ… LLMæ”¯æŒæµå¼è°ƒç”¨ï¼Œå°†ä½¿ç”¨çœŸæ­£çš„æµå¼å“åº”")
    else:
        print("âŒ LLMä¸æ”¯æŒæµå¼è°ƒç”¨ï¼Œå°†å›é€€åˆ°æ¨¡æ‹Ÿæµå¼å“åº”")
    
    # æµ‹è¯•å®é™…è°ƒç”¨
    print("\næµ‹è¯•å®é™…è°ƒç”¨:")
    question = "ç®€å•æµ‹è¯•é—®é¢˜"
    
    start_time = time.time()
    event_count = 0
    
    async for event in rag.ask_stream(question):
        event_count += 1
        elapsed = time.time() - start_time
        
        if event.type == StreamEventType.GENERATION_CHUNK:
            if event_count <= 5:
                print(f"  äº‹ä»¶ {event_count}: [{elapsed:.2f}s] '{event.data['chunk']}'")
        elif event.type == StreamEventType.COMPLETE:
            print(f"  å®Œæˆ: [{elapsed:.2f}s] æ€»å…± {event_count} ä¸ªäº‹ä»¶")
            break

async def test_real_world_scenario():
    """çœŸå®ä¸–ç•Œåœºæ™¯æµ‹è¯•"""
    print("\nğŸŒ çœŸå®ä¸–ç•Œåœºæ™¯æµ‹è¯•")
    print("=" * 80)
    
    rag = StreamingRagPipeline()
    
    if not rag.qa_chain:
        print("âš ï¸  é—®ç­”é“¾æœªåˆå§‹åŒ–ï¼Œè·³è¿‡æµ‹è¯•")
        return
    
    # æ¨¡æ‹Ÿç”¨æˆ·çš„å®é™…ä½¿ç”¨åœºæ™¯
    questions = [
        "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
        "æœºå™¨å­¦ä¹ çš„åŸºæœ¬åŸç†æ˜¯ä»€ä¹ˆï¼Ÿ"
    ]
    
    for i, question in enumerate(questions, 1):
        print(f"\nğŸ“ é—®é¢˜ {i}: {question}")
        print("-" * 40)
        
        start_time = time.time()
        user_saw_first_output = False
        
        async for event in rag.ask_stream(question):
            elapsed = time.time() - start_time
            
            if event.type == StreamEventType.GENERATION_CHUNK and not user_saw_first_output:
                print(f"âš¡ ç”¨æˆ·é¦–æ¬¡çœ‹åˆ°è¾“å‡º: {elapsed:.2f}ç§’")
                user_saw_first_output = True
                
            elif event.type == StreamEventType.COMPLETE:
                print(f"âœ… é—®é¢˜å®Œæˆ: {elapsed:.2f}ç§’")
                break
                
            elif event.type == StreamEventType.ERROR:
                print(f"âŒ é”™è¯¯: {event.data['error']}")
                break
        
        # æ¨¡æ‹Ÿç”¨æˆ·é—´éš”
        await asyncio.sleep(1)

async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ§ª çœŸæ­£çš„æµå¼å“åº”å®ç°æµ‹è¯•")
    print("=" * 80)
    print("ğŸ’¡ è¿™ä¸ªæµ‹è¯•éªŒè¯äº†ä½ çš„æ”¹è¿›æ€è·¯ï¼š")
    print("   åœ¨è¿”å›ç»™ç”¨æˆ·çš„æœ€åä¸€æ­¥è°ƒç”¨çœŸæ­£çš„å¼‚æ­¥æµå¼å¤§æ¨¡å‹æ¥å£")
    print()
    
    # LLMæ£€æµ‹æµ‹è¯•
    await test_llm_detection()
    
    # å¯¹æ¯”æµ‹è¯•
    await test_streaming_comparison()
    
    # çœŸå®åœºæ™¯æµ‹è¯•
    await test_real_world_scenario()
    
    print("\n" + "=" * 80)
    print("ğŸ¯ æµ‹è¯•æ€»ç»“")
    print("=" * 80)
    print("âœ… å®ç°äº†ä½ çš„æ”¹è¿›æ€è·¯:")
    print("  1. æ£€æµ‹LLMæ˜¯å¦æ”¯æŒæµå¼è°ƒç”¨ (hasattr(llm, 'astream'))")
    print("  2. å¦‚æœæ”¯æŒï¼Œç›´æ¥ä½¿ç”¨æµå¼API")
    print("  3. å¦‚æœä¸æ”¯æŒï¼Œå›é€€åˆ°åŸæœ‰å®ç°")
    print("  4. ä¿æŒäº†å‘åå…¼å®¹æ€§")
    
    print("\nğŸš€ é¢„æœŸæ•ˆæœ:")
    print("  - æ”¯æŒæµå¼çš„LLM: çœŸæ­£çš„æµå¼å“åº”ï¼Œç”¨æˆ·ä½“éªŒå¤§å¹…æå‡")
    print("  - ä¸æ”¯æŒæµå¼çš„LLM: ä¿æŒåŸæœ‰åŠŸèƒ½ï¼Œæ— ç ´åæ€§å˜æ›´")
    print("  - ç³»ç»Ÿæ¶æ„: æ›´åŠ çµæ´»å’Œå¯æ‰©å±•")
    
    print("\nğŸ’¡ ä¸‹ä¸€æ­¥å»ºè®®:")
    print("  1. é…ç½®æ”¯æŒæµå¼çš„LLMå®¢æˆ·ç«¯ (å¦‚OpenAIã€DeepSeek)")
    print("  2. æµ‹è¯•çœŸå®çš„æµå¼APIè°ƒç”¨")
    print("  3. ä¼˜åŒ–æµå¼å“åº”çš„ç”¨æˆ·ç•Œé¢")
    print("  4. ç›‘æ§æµå¼å“åº”çš„æ€§èƒ½æŒ‡æ ‡")

if __name__ == "__main__":
    asyncio.run(main())