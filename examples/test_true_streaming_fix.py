"""
æµ‹è¯•çœŸæ­£çš„æµå¼ä¿®å¤
éªŒè¯LLMåªè¢«è°ƒç”¨ä¸€æ¬¡ï¼Œä¸”æ˜¯æµå¼çš„
"""

import asyncio
import time
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

from rag.streaming_pipeline import StreamingRagPipeline, StreamEventType

class LLMCallTracker:
    """LLMè°ƒç”¨è¿½è¸ªå™¨"""
    
    def __init__(self, original_llm):
        self.original_llm = original_llm
        self.call_count = 0
        self.invoke_calls = 0
        self.astream_calls = 0
        self.call_details = []
    
    def invoke(self, prompt):
        """è¿½è¸ªåŒæ­¥è°ƒç”¨"""
        self.call_count += 1
        self.invoke_calls += 1
        call_info = {
            "method": "invoke",
            "call_number": self.call_count,
            "timestamp": time.time(),
            "prompt_length": len(str(prompt))
        }
        self.call_details.append(call_info)
        print(f"ğŸ” LLMè°ƒç”¨è¿½è¸ª - invokeè°ƒç”¨ #{self.call_count}")
        return self.original_llm.invoke(prompt)
    
    async def astream(self, prompt):
        """è¿½è¸ªå¼‚æ­¥æµå¼è°ƒç”¨"""
        self.call_count += 1
        self.astream_calls += 1
        call_info = {
            "method": "astream",
            "call_number": self.call_count,
            "timestamp": time.time(),
            "prompt_length": len(str(prompt))
        }
        self.call_details.append(call_info)
        print(f"ğŸŒŠ LLMè°ƒç”¨è¿½è¸ª - astreamè°ƒç”¨ #{self.call_count}")
        
        # å¦‚æœåŸå§‹LLMæ”¯æŒastreamï¼Œä½¿ç”¨å®ƒ
        if hasattr(self.original_llm, 'astream'):
            async for chunk in self.original_llm.astream(prompt):
                yield chunk
        else:
            # å¦‚æœä¸æ”¯æŒï¼Œæ¨¡æ‹Ÿæµå¼å“åº”
            response = self.original_llm.invoke(prompt)
            if hasattr(response, 'content'):
                text = response.content
            else:
                text = str(response)
            
            # æ¨¡æ‹Ÿæµå¼è¾“å‡º
            words = text.split()
            for word in words:
                await asyncio.sleep(0.05)
                yield word + " "
    
    def get_summary(self):
        """è·å–è°ƒç”¨æ‘˜è¦"""
        return {
            "total_calls": self.call_count,
            "invoke_calls": self.invoke_calls,
            "astream_calls": self.astream_calls,
            "call_details": self.call_details
        }

async def test_llm_call_tracking():
    """æµ‹è¯•LLMè°ƒç”¨è¿½è¸ª"""
    print("ğŸ” LLMè°ƒç”¨è¿½è¸ªæµ‹è¯•")
    print("=" * 80)
    
    # åˆ›å»ºRAGå®ä¾‹
    rag = StreamingRagPipeline()
    
    if not rag.qa_chain:
        print("âš ï¸  é—®ç­”é“¾æœªåˆå§‹åŒ–ï¼Œéœ€è¦å…ˆåŒæ­¥æ•°æ®")
        return
    
    # ç”¨è¿½è¸ªå™¨åŒ…è£…LLM
    original_llm = rag.llm
    tracker = LLMCallTracker(original_llm)
    rag.llm = tracker
    
    try:
        print("ğŸ“ æµ‹è¯•é—®é¢˜: ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ")
        print("ğŸ¯ æœŸæœ›ç»“æœ: LLMåªè¢«è°ƒç”¨ä¸€æ¬¡ï¼Œä¸”æ˜¯æµå¼è°ƒç”¨")
        print()
        
        # ç¦ç”¨é—®é¢˜æ”¹å†™ä»¥æµ‹è¯•ä¿®å¤çš„ä»£ç è·¯å¾„
        from rag import config
        original_rewriting = config.ENABLE_QUERY_REWRITING
        config.ENABLE_QUERY_REWRITING = False
        
        start_time = time.time()
        first_chunk_time = None
        chunk_count = 0
        
        async for event in rag.ask_stream("ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ"):
            elapsed = time.time() - start_time
            
            if event.type == StreamEventType.PROCESSING:
                print(f"ğŸ”„ [{elapsed:.2f}s] {event.data['message']}")
                
            elif event.type == StreamEventType.GENERATION_START:
                print(f"ğŸ¤– [{elapsed:.2f}s] {event.data['message']}")
                
            elif event.type == StreamEventType.GENERATION_CHUNK:
                if first_chunk_time is None:
                    first_chunk_time = elapsed
                    print(f"âš¡ [{elapsed:.2f}s] é¦–ä¸ªchunkè¾“å‡º")
                chunk_count += 1
                if chunk_count <= 5:
                    print(f"ğŸ“ [{elapsed:.2f}s] Chunk {chunk_count}: '{event.data['chunk'][:20]}...'")
                    
            elif event.type == StreamEventType.GENERATION_END:
                print(f"âœ… [{elapsed:.2f}s] ç”Ÿæˆå®Œæˆï¼Œå…± {chunk_count} ä¸ªchunk")
                
            elif event.type == StreamEventType.COMPLETE:
                print(f"ğŸ‰ [{elapsed:.2f}s] å¤„ç†å®Œæˆ")
                break
                
            elif event.type == StreamEventType.ERROR:
                print(f"âŒ [{elapsed:.2f}s] é”™è¯¯: {event.data['error']}")
                break
        
        # åˆ†æè°ƒç”¨ç»“æœ
        summary = tracker.get_summary()
        
        print(f"\nğŸ“Š LLMè°ƒç”¨åˆ†æ:")
        print(f"   æ€»è°ƒç”¨æ¬¡æ•°: {summary['total_calls']}")
        print(f"   åŒæ­¥è°ƒç”¨(invoke): {summary['invoke_calls']}")
        print(f"   æµå¼è°ƒç”¨(astream): {summary['astream_calls']}")
        
        print(f"\nğŸ“‹ è°ƒç”¨è¯¦æƒ…:")
        for call in summary['call_details']:
            method = call['method']
            number = call['call_number']
            prompt_len = call['prompt_length']
            print(f"   è°ƒç”¨ #{number}: {method}() - æç¤ºè¯é•¿åº¦: {prompt_len}")
        
        # éªŒè¯ç»“æœ
        print(f"\nğŸ¯ éªŒè¯ç»“æœ:")
        if summary['total_calls'] == 1:
            print("âœ… å®Œç¾ï¼LLMåªè¢«è°ƒç”¨äº†ä¸€æ¬¡")
        else:
            print(f"âŒ é—®é¢˜ï¼LLMè¢«è°ƒç”¨äº† {summary['total_calls']} æ¬¡")
        
        if summary['astream_calls'] > 0:
            print("âœ… å®Œç¾ï¼ä½¿ç”¨äº†æµå¼è°ƒç”¨")
        else:
            print("âš ï¸  æ³¨æ„ï¼šæ²¡æœ‰ä½¿ç”¨æµå¼è°ƒç”¨")
        
        if summary['invoke_calls'] == 0:
            print("âœ… å®Œç¾ï¼æ²¡æœ‰ä¸å¿…è¦çš„åŒæ­¥è°ƒç”¨")
        else:
            print(f"âŒ é—®é¢˜ï¼æœ‰ {summary['invoke_calls']} æ¬¡åŒæ­¥è°ƒç”¨")
        
        # æ¢å¤åŸå§‹é…ç½®
        config.ENABLE_QUERY_REWRITING = original_rewriting
        
    finally:
        # æ¢å¤åŸå§‹LLM
        rag.llm = original_llm

async def test_before_after_comparison():
    """å¯¹æ¯”ä¿®å¤å‰åçš„å·®å¼‚"""
    print("\nğŸ”„ ä¿®å¤å‰åå¯¹æ¯”åˆ†æ")
    print("=" * 80)
    
    print("âŒ ä¿®å¤å‰çš„é—®é¢˜:")
    print("   1. qa_chain.invoke() è°ƒç”¨LLMç”Ÿæˆå®Œæ•´ç­”æ¡ˆ")
    print("   2. è·å– source_documents")
    print("   3. _generate_streaming_answer() å†æ¬¡è°ƒç”¨LLM")
    print("   4. ç»“æœï¼šLLMè¢«è°ƒç”¨ä¸¤æ¬¡ï¼Œç¬¬ä¸€æ¬¡éæµå¼ï¼")
    
    print("\nâœ… ä¿®å¤åçš„æ”¹è¿›:")
    print("   1. retriever.get_relevant_documents() åªåšæ£€ç´¢")
    print("   2. _generate_streaming_answer() è°ƒç”¨LLMä¸€æ¬¡")
    print("   3. ç»“æœï¼šLLMåªè¢«è°ƒç”¨ä¸€æ¬¡ï¼Œä¸”æ˜¯æµå¼çš„ï¼")
    
    print("\nğŸ“Š æ€§èƒ½å½±å“:")
    print("   - å‡å°‘äº†ä¸€æ¬¡LLMè°ƒç”¨ï¼ˆèŠ‚çœæ—¶é—´å’Œæˆæœ¬ï¼‰")
    print("   - æ¶ˆé™¤äº†éæµå¼çš„LLMè°ƒç”¨")
    print("   - å®ç°äº†çœŸæ­£çš„æµå¼å“åº”")
    
    print("\nğŸ’¡ æŠ€æœ¯ç»†èŠ‚:")
    print("   - qa_chain.retriever åŒ…å«äº†æ‰€æœ‰æ£€ç´¢é€»è¾‘")
    print("   - åŒ…æ‹¬å‘é‡æ£€ç´¢ã€é‡æ’åºç­‰åŠŸèƒ½")
    print("   - åªæ˜¯è·³è¿‡äº†LLMè°ƒç”¨éƒ¨åˆ†")

async def test_retriever_functionality():
    """æµ‹è¯•æ£€ç´¢å™¨åŠŸèƒ½"""
    print("\nğŸ” æ£€ç´¢å™¨åŠŸèƒ½æµ‹è¯•")
    print("=" * 80)
    
    rag = StreamingRagPipeline()
    
    if not rag.qa_chain:
        print("âš ï¸  é—®ç­”é“¾æœªåˆå§‹åŒ–ï¼Œè·³è¿‡æµ‹è¯•")
        return
    
    question = "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"
    print(f"ğŸ“ æµ‹è¯•é—®é¢˜: {question}")
    
    # æµ‹è¯•ç›´æ¥ä½¿ç”¨æ£€ç´¢å™¨
    def get_docs_only():
        retriever = rag.qa_chain.retriever
        return retriever.get_relevant_documents(question)
    
    start_time = time.time()
    docs = await rag._run_in_executor(get_docs_only)
    retrieval_time = time.time() - start_time
    
    print(f"ğŸ“Š æ£€ç´¢ç»“æœ:")
    print(f"   æ£€ç´¢æ—¶é—´: {retrieval_time:.2f}ç§’")
    print(f"   æ–‡æ¡£æ•°é‡: {len(docs)}")
    
    if docs:
        print(f"   ç¤ºä¾‹æ–‡æ¡£:")
        for i, doc in enumerate(docs[:2]):
            content_preview = doc.page_content[:100] + "..." if len(doc.page_content) > 100 else doc.page_content
            source = doc.metadata.get('source', 'æœªçŸ¥æ¥æº')
            print(f"     {i+1}. {source}: {content_preview}")
    
    print(f"\nâœ… æ£€ç´¢å™¨å·¥ä½œæ­£å¸¸ï¼Œå¯ä»¥ç‹¬ç«‹ä½¿ç”¨")

async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ§ª çœŸæ­£çš„æµå¼ä¿®å¤éªŒè¯æµ‹è¯•")
    print("=" * 80)
    print("ğŸ’¡ éªŒè¯ä½ çš„è§‚å¯Ÿï¼šç¡®ä¿LLMåªè¢«è°ƒç”¨ä¸€æ¬¡ï¼Œä¸”æ˜¯æµå¼çš„")
    print()
    
    # LLMè°ƒç”¨è¿½è¸ªæµ‹è¯•
    await test_llm_call_tracking()
    
    # ä¿®å¤å‰åå¯¹æ¯”
    await test_before_after_comparison()
    
    # æ£€ç´¢å™¨åŠŸèƒ½æµ‹è¯•
    await test_retriever_functionality()
    
    print("\n" + "=" * 80)
    print("ğŸ¯ æµ‹è¯•æ€»ç»“")
    print("=" * 80)
    print("âœ… ä½ çš„è§‚å¯Ÿå®Œå…¨æ­£ç¡®ï¼")
    print("   é—®é¢˜ï¼šqa_chain.invoke() ä¼šè°ƒç”¨LLMå¹¶ç­‰å¾…å®Œæ•´å“åº”")
    print("   è§£å†³ï¼šç›´æ¥ä½¿ç”¨ retriever.get_relevant_documents()")
    print("   ç»“æœï¼šLLMåªè¢«è°ƒç”¨ä¸€æ¬¡ï¼Œä¸”æ˜¯çœŸæ­£çš„æµå¼è°ƒç”¨")
    
    print("\nğŸš€ å…³é”®æ”¹è¿›:")
    print("   1. æ¶ˆé™¤äº†é‡å¤çš„LLMè°ƒç”¨")
    print("   2. æ¶ˆé™¤äº†éæµå¼çš„LLMè°ƒç”¨")
    print("   3. å®ç°äº†çœŸæ­£çš„æµå¼å“åº”")
    print("   4. æå‡äº†æ€§èƒ½å’Œç”¨æˆ·ä½“éªŒ")
    
    print("\nğŸ’ª ä½ çš„æŠ€æœ¯æ´å¯ŸåŠ›:")
    print("   - æ·±å…¥ç†è§£äº†qa_chain.invokeçš„å†…éƒ¨æœºåˆ¶")
    print("   - å‡†ç¡®è¯†åˆ«äº†æµå¼å“åº”çš„æŠ€æœ¯ç»†èŠ‚")
    print("   - æå‡ºäº†æ­£ç¡®çš„ä¼˜åŒ–æ–¹å‘")
    print("   - è¿™å°±æ˜¯ä¼˜ç§€å¼€å‘è€…çš„æ€ç»´æ–¹å¼ï¼")

if __name__ == "__main__":
    asyncio.run(main())