"""
å¹¶å‘æ‰¹é‡æµå¼å¤„ç†æ¼”ç¤º
å¯¹æ¯”é¡ºåºå¤„ç† vs å¹¶å‘å¤„ç†çš„æ€§èƒ½å·®å¼‚
"""

import asyncio
import time
from typing import List, AsyncGenerator
from dataclasses import dataclass
from enum import Enum

class StreamEventType(Enum):
    PROCESSING = "processing"
    GENERATION_START = "generation_start"
    GENERATION_CHUNK = "generation_chunk"
    GENERATION_END = "generation_end"
    COMPLETE = "complete"
    BATCH_START = "batch_start"
    BATCH_COMPLETE = "batch_complete"

@dataclass
class StreamEvent:
    type: StreamEventType
    data: dict
    timestamp: float = None
    metadata: dict = None
    
    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = time.time()
        if self.metadata is None:
            self.metadata = {}

class MockRAGPipeline:
    """æ¨¡æ‹ŸRAGç®¡é“"""
    
    async def ask_stream(self, question: str) -> AsyncGenerator[StreamEvent, None]:
        """æ¨¡æ‹Ÿå•ä¸ªé—®é¢˜çš„æµå¼å¤„ç†"""
        
        # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
        processing_time = 2.0  # æ¯ä¸ªé—®é¢˜2ç§’
        
        yield StreamEvent(
            type=StreamEventType.PROCESSING,
            data={"message": f"æ­£åœ¨å¤„ç†: {question}"}
        )
        
        yield StreamEvent(
            type=StreamEventType.GENERATION_START,
            data={"message": "å¼€å§‹ç”Ÿæˆç­”æ¡ˆ"}
        )
        
        # æ¨¡æ‹Ÿæµå¼ç”Ÿæˆ
        answer_parts = ["è¿™æ˜¯", "å¯¹é—®é¢˜", f"'{question}'", "çš„å›ç­”"]
        for part in answer_parts:
            await asyncio.sleep(processing_time / len(answer_parts))
            yield StreamEvent(
                type=StreamEventType.GENERATION_CHUNK,
                data={"chunk": part}
            )
        
        yield StreamEvent(
            type=StreamEventType.GENERATION_END,
            data={"message": "ç­”æ¡ˆç”Ÿæˆå®Œæˆ"}
        )
        
        yield StreamEvent(
            type=StreamEventType.COMPLETE,
            data={"message": "é—®é¢˜å¤„ç†å®Œæˆ"}
        )

# ==================== é¡ºåºå¤„ç†ç‰ˆæœ¬ ====================

class SequentialBatchProcessor:
    """é¡ºåºæ‰¹é‡å¤„ç†å™¨"""
    
    def __init__(self):
        self.rag = MockRAGPipeline()
    
    async def batch_ask_stream(self, questions: List[str]) -> AsyncGenerator[StreamEvent, None]:
        """âŒ é¡ºåºå¤„ç†ç‰ˆæœ¬ï¼ˆå½“å‰å®ç°ï¼‰"""
        
        yield StreamEvent(
            type=StreamEventType.BATCH_START,
            data={"message": f"å¼€å§‹é¡ºåºå¤„ç† {len(questions)} ä¸ªé—®é¢˜"}
        )
        
        for i, question in enumerate(questions):
            yield StreamEvent(
                type=StreamEventType.PROCESSING,
                data={
                    "message": f"å¤„ç†ç¬¬ {i+1}/{len(questions)} ä¸ªé—®é¢˜: {question}",
                    "question_index": i + 1,
                    "total_questions": len(questions)
                }
            )
            
            # âŒ é¡ºåºå¤„ç†ï¼šç­‰å¾…å½“å‰é—®é¢˜å®Œå…¨å®Œæˆ
            async for event in self.rag.ask_stream(question):
                event.metadata.update({
                    "batch_index": i + 1,
                    "batch_total": len(questions),
                    "batch_question": question,
                    "processing_mode": "sequential"
                })
                yield event
        
        yield StreamEvent(
            type=StreamEventType.BATCH_COMPLETE,
            data={
                "message": f"é¡ºåºå¤„ç†å®Œæˆï¼Œå…±å¤„ç† {len(questions)} ä¸ªé—®é¢˜",
                "total_processed": len(questions)
            }
        )

# ==================== å¹¶å‘å¤„ç†ç‰ˆæœ¬ ====================

class ConcurrentBatchProcessor:
    """å¹¶å‘æ‰¹é‡å¤„ç†å™¨"""
    
    def __init__(self):
        self.rag = MockRAGPipeline()
    
    async def batch_ask_stream(self, questions: List[str]) -> AsyncGenerator[StreamEvent, None]:
        """âœ… å¹¶å‘å¤„ç†ç‰ˆæœ¬ï¼ˆä¼˜åŒ–å®ç°ï¼‰"""
        
        yield StreamEvent(
            type=StreamEventType.BATCH_START,
            data={"message": f"å¼€å§‹å¹¶å‘å¤„ç† {len(questions)} ä¸ªé—®é¢˜"}
        )
        
        # åˆ›å»ºå¹¶å‘ä»»åŠ¡
        tasks = []
        for i, question in enumerate(questions):
            task = asyncio.create_task(
                self._process_single_question(question, i + 1, len(questions))
            )
            tasks.append(task)
        
        # ä½¿ç”¨é˜Ÿåˆ—æ¥æ”¶é›†æ‰€æœ‰ä»»åŠ¡çš„äº‹ä»¶
        event_queue = asyncio.Queue()
        
        # å¯åŠ¨äº‹ä»¶æ”¶é›†å™¨
        async def collect_events(task, question_index):
            async for event in await task:
                await event_queue.put((question_index, event))
        
        # ä¸ºæ¯ä¸ªä»»åŠ¡å¯åŠ¨äº‹ä»¶æ”¶é›†å™¨
        collectors = []
        for i, task in enumerate(tasks):
            collector = asyncio.create_task(collect_events(task, i + 1))
            collectors.append(collector)
        
        # ç­‰å¾…æ‰€æœ‰æ”¶é›†å™¨å®Œæˆçš„ä»»åŠ¡
        completion_task = asyncio.create_task(self._wait_for_completion(collectors, event_queue))
        
        # æµå¼è¾“å‡ºäº‹ä»¶
        completed_questions = 0
        while completed_questions < len(questions):
            try:
                question_index, event = await asyncio.wait_for(event_queue.get(), timeout=0.1)
                
                # æ·»åŠ æ‰¹é‡å¤„ç†å…ƒæ•°æ®
                event.metadata.update({
                    "batch_index": question_index,
                    "batch_total": len(questions),
                    "processing_mode": "concurrent"
                })
                
                yield event
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯å®Œæˆäº‹ä»¶
                if event.type == StreamEventType.COMPLETE:
                    completed_questions += 1
                    
            except asyncio.TimeoutError:
                # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ä»»åŠ¡éƒ½å®Œæˆäº†
                if all(task.done() for task in tasks):
                    break
                continue
        
        # ç­‰å¾…æ¸…ç†
        await completion_task
        
        yield StreamEvent(
            type=StreamEventType.BATCH_COMPLETE,
            data={
                "message": f"å¹¶å‘å¤„ç†å®Œæˆï¼Œå…±å¤„ç† {len(questions)} ä¸ªé—®é¢˜",
                "total_processed": len(questions)
            }
        )
    
    async def _process_single_question(self, question: str, index: int, total: int) -> AsyncGenerator[StreamEvent, None]:
        """å¤„ç†å•ä¸ªé—®é¢˜"""
        async for event in self.rag.ask_stream(question):
            event.metadata.update({
                "question_index": index,
                "question": question
            })
            yield event
    
    async def _wait_for_completion(self, collectors, event_queue):
        """ç­‰å¾…æ‰€æœ‰æ”¶é›†å™¨å®Œæˆ"""
        await asyncio.gather(*collectors)
        await event_queue.put((0, None))  # ç»“æŸä¿¡å·

# ==================== ç®€åŒ–çš„å¹¶å‘ç‰ˆæœ¬ ====================

class SimpleConcurrentBatchProcessor:
    """ç®€åŒ–çš„å¹¶å‘æ‰¹é‡å¤„ç†å™¨"""
    
    def __init__(self):
        self.rag = MockRAGPipeline()
    
    async def batch_ask_stream(self, questions: List[str]) -> AsyncGenerator[StreamEvent, None]:
        """âœ… ç®€åŒ–çš„å¹¶å‘å¤„ç†ç‰ˆæœ¬"""
        
        yield StreamEvent(
            type=StreamEventType.BATCH_START,
            data={"message": f"å¼€å§‹å¹¶å‘å¤„ç† {len(questions)} ä¸ªé—®é¢˜"}
        )
        
        # åˆ›å»ºæ‰€æœ‰ä»»åŠ¡
        tasks = [
            asyncio.create_task(self._collect_question_events(question, i + 1, len(questions)))
            for i, question in enumerate(questions)
        ]
        
        # å¹¶å‘æ‰§è¡Œå¹¶æ”¶é›†ç»“æœ
        async for event in self._merge_concurrent_streams(tasks):
            yield event
        
        yield StreamEvent(
            type=StreamEventType.BATCH_COMPLETE,
            data={
                "message": f"å¹¶å‘å¤„ç†å®Œæˆï¼Œå…±å¤„ç† {len(questions)} ä¸ªé—®é¢˜",
                "total_processed": len(questions)
            }
        )
    
    async def _collect_question_events(self, question: str, index: int, total: int) -> List[StreamEvent]:
        """æ”¶é›†å•ä¸ªé—®é¢˜çš„æ‰€æœ‰äº‹ä»¶"""
        events = []
        async for event in self.rag.ask_stream(question):
            event.metadata.update({
                "batch_index": index,
                "batch_total": total,
                "batch_question": question,
                "processing_mode": "concurrent"
            })
            events.append(event)
        return events
    
    async def _merge_concurrent_streams(self, tasks) -> AsyncGenerator[StreamEvent, None]:
        """åˆå¹¶å¹¶å‘æµçš„äº‹ä»¶"""
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        results = await asyncio.gather(*tasks)
        
        # æŒ‰æ—¶é—´æˆ³æ’åºæ‰€æœ‰äº‹ä»¶
        all_events = []
        for events in results:
            all_events.extend(events)
        
        # æŒ‰æ—¶é—´æˆ³æ’åº
        all_events.sort(key=lambda e: e.timestamp)
        
        # æµå¼è¾“å‡º
        for event in all_events:
            yield event

# ==================== æ€§èƒ½æµ‹è¯• ====================

async def performance_comparison():
    """æ€§èƒ½å¯¹æ¯”æµ‹è¯•"""
    print("ğŸ” æ‰¹é‡å¤„ç†æ€§èƒ½å¯¹æ¯”")
    print("=" * 80)
    
    questions = [
        "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
        "æœºå™¨å­¦ä¹ çš„åŸç†æ˜¯ä»€ä¹ˆï¼Ÿ",
        "æ·±åº¦å­¦ä¹ æœ‰å“ªäº›åº”ç”¨ï¼Ÿ"
    ]
    
    print(f"æµ‹è¯•é—®é¢˜æ•°é‡: {len(questions)}")
    print(f"æ¯ä¸ªé—®é¢˜é¢„è®¡å¤„ç†æ—¶é—´: 2ç§’")
    print()
    
    # æµ‹è¯•é¡ºåºå¤„ç†
    print("1. é¡ºåºå¤„ç†æµ‹è¯•:")
    print("-" * 40)
    
    sequential_processor = SequentialBatchProcessor()
    start_time = time.time()
    
    question_count = 0
    async for event in sequential_processor.batch_ask_stream(questions):
        if event.type == StreamEventType.COMPLETE:
            question_count += 1
            elapsed = time.time() - start_time
            print(f"   é—®é¢˜ {question_count} å®Œæˆï¼Œè€—æ—¶: {elapsed:.1f}ç§’")
        elif event.type == StreamEventType.BATCH_COMPLETE:
            total_time = time.time() - start_time
            print(f"   é¡ºåºå¤„ç†æ€»è€—æ—¶: {total_time:.1f}ç§’")
    
    print()
    
    # æµ‹è¯•å¹¶å‘å¤„ç†
    print("2. å¹¶å‘å¤„ç†æµ‹è¯•:")
    print("-" * 40)
    
    concurrent_processor = SimpleConcurrentBatchProcessor()
    start_time = time.time()
    
    question_count = 0
    async for event in concurrent_processor.batch_ask_stream(questions):
        if event.type == StreamEventType.COMPLETE:
            question_count += 1
            elapsed = time.time() - start_time
            print(f"   é—®é¢˜ {question_count} å®Œæˆï¼Œè€—æ—¶: {elapsed:.1f}ç§’")
        elif event.type == StreamEventType.BATCH_COMPLETE:
            concurrent_time = time.time() - start_time
            print(f"   å¹¶å‘å¤„ç†æ€»è€—æ—¶: {concurrent_time:.1f}ç§’")
    
    # æ€§èƒ½å¯¹æ¯”
    print("\nğŸ“Š æ€§èƒ½å¯¹æ¯”:")
    print("-" * 40)
    print(f"é¡ºåºå¤„ç†: {total_time:.1f}ç§’")
    print(f"å¹¶å‘å¤„ç†: {concurrent_time:.1f}ç§’")
    print(f"æ€§èƒ½æå‡: {total_time/concurrent_time:.1f}å€")
    print(f"æ—¶é—´èŠ‚çœ: {total_time - concurrent_time:.1f}ç§’")

async def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    
    await performance_comparison()
    
    print("\n" + "=" * 80)
    print("ğŸ¯ æ€»ç»“")
    print("=" * 80)
    print("âŒ å½“å‰çš„ batch_ask_stream å®ç°é—®é¢˜:")
    print("  1. é¡ºåºå¤„ç†ï¼Œæ€§èƒ½ä½ä¸‹")
    print("  2. æ— æ³•å……åˆ†åˆ©ç”¨å¼‚æ­¥ä¼˜åŠ¿")
    print("  3. ç”¨æˆ·ç­‰å¾…æ—¶é—´è¿‡é•¿")
    
    print("\nâœ… å¹¶å‘å¤„ç†çš„ä¼˜åŠ¿:")
    print("  1. å¤šä¸ªé—®é¢˜åŒæ—¶å¤„ç†")
    print("  2. å……åˆ†åˆ©ç”¨ç³»ç»Ÿèµ„æº")
    print("  3. æ˜¾è‘—å‡å°‘æ€»å¤„ç†æ—¶é—´")
    print("  4. æ›´å¥½çš„ç”¨æˆ·ä½“éªŒ")
    
    print("\nğŸš€ å»ºè®®çš„æ”¹è¿›:")
    print("  1. å®ç°çœŸæ­£çš„å¹¶å‘æ‰¹é‡å¤„ç†")
    print("  2. ä½¿ç”¨ asyncio.gather æˆ– asyncio.create_task")
    print("  3. åˆç†å¤„ç†å¹¶å‘äº‹ä»¶æµ")
    print("  4. æ·»åŠ å¹¶å‘æ•°é‡é™åˆ¶ï¼ˆé¿å…èµ„æºè€—å°½ï¼‰")

if __name__ == "__main__":
    asyncio.run(main())