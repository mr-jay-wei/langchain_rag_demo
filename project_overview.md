# é¡¹ç›®æ¦‚è§ˆ: rag_example

æœ¬æ–‡æ¡£ç”±`generate_project_overview.py`è‡ªåŠ¨ç”Ÿæˆï¼ŒåŒ…å«äº†é¡¹ç›®çš„ç»“æ„æ ‘å’Œæ‰€æœ‰å¯è¯»æ–‡ä»¶çš„å†…å®¹ã€‚

## é¡¹ç›®ç»“æ„

```
rag_example/
â”œâ”€â”€ rag
â”‚   â”œâ”€â”€ prompts
â”‚   â”‚   â”œâ”€â”€ prompt_README.md
â”‚   â”‚   â”œâ”€â”€ qa_prompt.txt
â”‚   â”‚   â””â”€â”€ query_rewrite_prompt.txt
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ async_pipeline.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ hot_reload_manager.py
â”‚   â”œâ”€â”€ memory_manager.py
â”‚   â”œâ”€â”€ pipeline.py
â”‚   â”œâ”€â”€ prompt_manager.py
â”‚   â””â”€â”€ streaming_pipeline.py
â”œâ”€â”€ .env_example
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .python-version
â”œâ”€â”€ async_main.py
â”œâ”€â”€ main.py
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ README.md
â”œâ”€â”€ sse_api_server.py
â”œâ”€â”€ streaming_main.py
â””â”€â”€ streaming_web_demo.py
```

---

# æ–‡ä»¶å†…å®¹

## `.env_example`

```
CLOUD_INFINI_API_KEY = ""
CLOUD_BASE_URL = ""
CLOUD_MODEL_NAME = ""
DeepSeek_api_key = ""
DeepSeek_base_url = ""
DeepSeek_model_name = ""
```

## `.gitignore`

```
# Python-generated files
__pycache__/
*.py[oc]
build/
dist/
wheels/
*.egg-info

# Virtual environments
.venv

```

## `.python-version`

```
3.12

```

## `async_main.py`

```python
# async_main.py - å¼‚æ­¥RAGç³»ç»Ÿä½¿ç”¨ç¤ºä¾‹

import asyncio
import time
from rag.async_pipeline import AsyncRagPipeline


async def main():
    """å¼‚æ­¥RAGç³»ç»Ÿçš„ä¸»è¦æ¼”ç¤ºå‡½æ•°ã€‚"""
    print("=" * 60)
    print("ğŸš€ å¼‚æ­¥RAGç³»ç»Ÿæ¼”ç¤º")
    print("=" * 60)
    
    # åˆå§‹åŒ–å¼‚æ­¥RAGæµç¨‹
    rag = AsyncRagPipeline()
    
    # å¼‚æ­¥åŒæ­¥æ•°æ®ç›®å½•
    print("\nğŸ“ å¼€å§‹å¼‚æ­¥åŒæ­¥æ•°æ®ç›®å½•...")
    start_time = time.time()
    await rag.sync_data_directory_async()
    sync_time = time.time() - start_time
    print(f"âœ… å¼‚æ­¥åŒæ­¥å®Œæˆï¼Œè€—æ—¶: {sync_time:.2f}ç§’")
    
    # æµ‹è¯•é—®é¢˜åˆ—è¡¨
    # test_questions = [
    #     "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
    # ]
    test_questions = [
        "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
        "Pythonæœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ",
        "å¦‚ä½•ä½¿ç”¨RAGç³»ç»Ÿï¼Ÿ",
        "ä»€ä¹ˆæ˜¯æ··åˆæ£€ç´¢ï¼Ÿ",
        "ä¼ä¸šçº§åŠŸèƒ½æœ‰å“ªäº›ï¼Ÿ"
    ]


    print("\nğŸ¤– å¼€å§‹å¼‚æ­¥é—®ç­”æµ‹è¯•...")
    print("-" * 50)
    
    # å¹¶å‘æ‰§è¡Œå¤šä¸ªé—®ç­”
    async def ask_question(question: str, index: int):
        print(f"\n[é—®é¢˜ {index + 1}] {question}")
        start_time = time.time()
        
        result = await rag.ask_async(question)
        
        end_time = time.time()
        print(f"â±ï¸  å“åº”æ—¶é—´: {end_time - start_time:.2f}ç§’")
        print(f"ğŸ“ å›ç­”: {result['result']}")
        
        if result['source_documents']:
            print(f"ğŸ“š å‚è€ƒæ–‡æ¡£æ•°é‡: {len(result['source_documents'])}")
            for i, doc in enumerate(result['source_documents'][:2]):  # åªæ˜¾ç¤ºå‰2ä¸ª
                source = doc.metadata.get('source', 'æœªçŸ¥æ¥æº')
                print(f"   [{i+1}] {source}")
        
        return result
    
    # å¹¶å‘æ‰§è¡Œæ‰€æœ‰é—®ç­”
    start_time = time.time()
    tasks = [ask_question(question, i) for i, question in enumerate(test_questions)]
    results = await asyncio.gather(*tasks)
    total_time = time.time() - start_time
    
    print(f"\nâœ… æ‰€æœ‰é—®ç­”å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f}ç§’")
    print(f"ğŸ“Š å¹³å‡æ¯ä¸ªé—®é¢˜è€—æ—¶: {total_time / len(test_questions):.2f}ç§’")
    
    # æµ‹è¯•åˆ†ç±»æ£€ç´¢åŠŸèƒ½
    print("\nğŸ·ï¸  æµ‹è¯•å¼‚æ­¥åˆ†ç±»æ£€ç´¢åŠŸèƒ½...")
    print("-" * 50)
    
    # è·å–å¯ç”¨ç±»åˆ«
    categories = rag.get_available_categories()
    print(f"ğŸ“‹ å¯ç”¨ç±»åˆ«: {list(categories.keys())}")
    
    if categories:
        # æµ‹è¯•åˆ†ç±»æ£€ç´¢
        category_list = list(categories.keys())[:2]  # å–å‰ä¸¤ä¸ªç±»åˆ«
        question = "è¿™ä¸ªç³»ç»Ÿæœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼Ÿ"
        
        print(f"\n[åˆ†ç±»æ£€ç´¢æµ‹è¯•] é—®é¢˜: {question}")
        print(f"ğŸ¯ é™å®šç±»åˆ«: {category_list}")
        
        start_time = time.time()
        result = await rag.ask_with_categories_async(question, category_list)
        end_time = time.time()
        
        print(f"â±ï¸  å“åº”æ—¶é—´: {end_time - start_time:.2f}ç§’")
        print(f"ğŸ“ å›ç­”: {result['result']}")
        
        if result['source_documents']:
            print(f"ğŸ“š å‚è€ƒæ–‡æ¡£æ•°é‡: {len(result['source_documents'])}")
            for i, doc in enumerate(result['source_documents']):
                source = doc.metadata.get('source', 'æœªçŸ¥æ¥æº')
                category = doc.metadata.get('category', 'æœªçŸ¥ç±»åˆ«')
                print(f"   [{i+1}] {source} (ç±»åˆ«: {category})")
    
    print("\n" + "=" * 60)
    print("ğŸ‰ å¼‚æ­¥RAGç³»ç»Ÿæ¼”ç¤ºå®Œæˆï¼")
    print("=" * 60)


async def performance_comparison():
    """æ€§èƒ½å¯¹æ¯”æµ‹è¯•ï¼šåŒæ­¥ vs å¼‚æ­¥"""
    print("\n" + "=" * 60)
    print("âš¡ æ€§èƒ½å¯¹æ¯”æµ‹è¯•ï¼šåŒæ­¥ vs å¼‚æ­¥")
    print("=" * 60)
    
    # å¯¼å…¥åŒæ­¥ç‰ˆæœ¬
    from rag.pipeline import RagPipeline
    
    # æµ‹è¯•é—®é¢˜
    test_questions = [
        "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
        "Pythonç¼–ç¨‹è¯­è¨€çš„ç‰¹ç‚¹",
        "å¦‚ä½•ä¼˜åŒ–ç³»ç»Ÿæ€§èƒ½ï¼Ÿ"
    ]
    
    # åŒæ­¥ç‰ˆæœ¬æµ‹è¯•
    print("\nğŸ”„ åŒæ­¥ç‰ˆæœ¬æµ‹è¯•...")
    sync_rag = RagPipeline()
    sync_rag.sync_data_directory()
    
    sync_start = time.time()
    sync_results = []
    for question in test_questions:
        result = sync_rag.ask(question)
        sync_results.append(result)
    sync_time = time.time() - sync_start
    
    print(f"âœ… åŒæ­¥ç‰ˆæœ¬å®Œæˆï¼Œè€—æ—¶: {sync_time:.2f}ç§’")
    
    # å¼‚æ­¥ç‰ˆæœ¬æµ‹è¯•
    print("\nâš¡ å¼‚æ­¥ç‰ˆæœ¬æµ‹è¯•...")
    async_rag = AsyncRagPipeline()
    await async_rag.sync_data_directory_async()
    
    async_start = time.time()
    async_tasks = [async_rag.ask_async(question) for question in test_questions]
    async_results = await asyncio.gather(*async_tasks)
    async_time = time.time() - async_start
    
    print(f"âœ… å¼‚æ­¥ç‰ˆæœ¬å®Œæˆï¼Œè€—æ—¶: {async_time:.2f}ç§’")
    
    # æ€§èƒ½å¯¹æ¯”
    print(f"\nğŸ“Š æ€§èƒ½å¯¹æ¯”ç»“æœ:")
    print(f"   åŒæ­¥ç‰ˆæœ¬: {sync_time:.2f}ç§’")
    print(f"   å¼‚æ­¥ç‰ˆæœ¬: {async_time:.2f}ç§’")
    if sync_time > async_time:
        improvement = ((sync_time - async_time) / sync_time) * 100
        print(f"   ğŸš€ å¼‚æ­¥ç‰ˆæœ¬æå‡: {improvement:.1f}%")
    else:
        print(f"   âš ï¸  åœ¨æ­¤æµ‹è¯•ä¸­åŒæ­¥ç‰ˆæœ¬æ›´å¿«ï¼ˆå¯èƒ½ç”±äºé—®é¢˜ç®€å•æˆ–å¹¶å‘å¼€é”€ï¼‰")


async def batch_processing_demo():
    """æ‰¹é‡å¤„ç†æ¼”ç¤º"""
    print("\n" + "=" * 60)
    print("ğŸ“¦ æ‰¹é‡å¤„ç†æ¼”ç¤º")
    print("=" * 60)
    
    rag = AsyncRagPipeline()
    await rag.sync_data_directory_async()
    
    # å¤§é‡é—®é¢˜æ‰¹é‡å¤„ç†
    batch_questions = [
        "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
        "æ·±åº¦å­¦ä¹ çš„åº”ç”¨é¢†åŸŸ",
        "Pythonçš„ä¼˜åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ",
        "å¦‚ä½•ä¼˜åŒ–ç®—æ³•æ€§èƒ½ï¼Ÿ",
        "æ•°æ®ç§‘å­¦çš„å·¥ä½œæµç¨‹",
        "äººå·¥æ™ºèƒ½çš„å‘å±•è¶‹åŠ¿",
        "ç¼–ç¨‹è¯­è¨€çš„é€‰æ‹©æ ‡å‡†",
        "ç³»ç»Ÿæ¶æ„è®¾è®¡åŸåˆ™",
        "æ•°æ®åº“ä¼˜åŒ–æŠ€å·§",
        "äº‘è®¡ç®—çš„ä¼˜åŠ¿"
    ]
    
    print(f"ğŸ“ å‡†å¤‡å¤„ç† {len(batch_questions)} ä¸ªé—®é¢˜...")
    
    # åˆ†æ‰¹å¤„ç†ï¼ˆæ¯æ‰¹5ä¸ªï¼‰
    batch_size = 5
    all_results = []
    all_response_times = []
    
    for i in range(0, len(batch_questions), batch_size):
        batch = batch_questions[i:i + batch_size]
        print(f"\nğŸ”„ å¤„ç†ç¬¬ {i//batch_size + 1} æ‰¹ ({len(batch)} ä¸ªé—®é¢˜)...")
        
        # ä¸ºæ¯ä¸ªé—®é¢˜å•ç‹¬è®¡æ—¶
        async def ask_with_timing(question):
            start_time = time.time()
            result = await rag.ask_async(question)
            end_time = time.time()
            response_time = end_time - start_time
            return result, response_time
        
        batch_start = time.time()
        batch_tasks = [ask_with_timing(question) for question in batch]
        batch_results_with_timing = await asyncio.gather(*batch_tasks)
        batch_time = time.time() - batch_start
        
        # åˆ†ç¦»ç»“æœå’Œå“åº”æ—¶é—´
        batch_results = [result for result, _ in batch_results_with_timing]
        batch_response_times = [response_time for _, response_time in batch_results_with_timing]
        
        all_results.extend(batch_results)
        all_response_times.extend(batch_response_times)
        
        avg_batch_response_time = sum(batch_response_times) / len(batch_response_times)
        print(f"âœ… ç¬¬ {i//batch_size + 1} æ‰¹å®Œæˆï¼Œæ€»è€—æ—¶: {batch_time:.2f}ç§’ï¼Œå¹³å‡å•é—®é¢˜: {avg_batch_response_time:.2f}ç§’")
    
    print(f"\nğŸ‰ æ‰¹é‡å¤„ç†å®Œæˆï¼")
    print(f"ğŸ“Š æ€»å…±å¤„ç†: {len(all_results)} ä¸ªé—®é¢˜")
    print(f"â±ï¸  å¹³å‡å“åº”æ—¶é—´: {sum(all_response_times) / len(all_response_times):.2f}ç§’")
    print(f"âš¡ æœ€å¿«å“åº”: {min(all_response_times):.2f}ç§’")
    print(f"ğŸŒ æœ€æ…¢å“åº”: {max(all_response_times):.2f}ç§’")


if __name__ == "__main__":
    # è¿è¡Œä¸»æ¼”ç¤º
    # asyncio.run(main())
    
    # å¯é€‰ï¼šè¿è¡Œæ€§èƒ½å¯¹æ¯”æµ‹è¯•
    # asyncio.run(performance_comparison())
    
    # å¯é€‰ï¼šè¿è¡Œæ‰¹é‡å¤„ç†æ¼”ç¤º
    asyncio.run(batch_processing_demo())
```

## `main.py`

```python
# main.py

import os
from rag.pipeline import RagPipeline
from rag.config import DATA_PATH # å¯¼å…¥æ•°æ®è·¯å¾„

def setup_data_directory():
    """æ£€æŸ¥å¹¶åˆ›å»º.dataç›®å½•å’Œç¤ºä¾‹æ–‡ä»¶ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰ã€‚"""
    if not os.path.exists(DATA_PATH):
        print(f"æ­£åœ¨åˆ›å»ºæ•°æ®ç›®å½•: {DATA_PATH}")
        os.makedirs(DATA_PATH)
    
    sample_file_path = os.path.join(DATA_PATH, "initial_doc.txt")
    if not os.path.exists(sample_file_path):
        print(f"æ­£åœ¨åˆ›å»ºç¤ºä¾‹æ–‡ä»¶: {sample_file_path}")
        with open(sample_file_path, "w", encoding="utf-8") as f:
            f.write("è¿™æ˜¯ç³»ç»Ÿåˆå§‹åŒ–çš„ç¤ºä¾‹æ–‡æ¡£ã€‚ä½ å¯ä»¥å‘.dataç›®å½•æ·»åŠ æ›´å¤š.txtæ–‡ä»¶ã€‚")

def run_demo():
    """è¿è¡Œä¸€ä¸ªå®Œæ•´çš„RAGæ¼”ç¤ºï¼Œå…·å¤‡æ™ºèƒ½æ•°æ®åŒæ­¥åŠŸèƒ½ã€‚"""
    print("=" * 50)
    print("          æ¬¢è¿ä½¿ç”¨æœ¬åœ°RAGé—®ç­”ç³»ç»Ÿ (V3.0)")
    print("=" * 50)

    # 0. å‡†å¤‡å·¥ä½œï¼šç¡®ä¿.dataç›®å½•å­˜åœ¨
    setup_data_directory()

    # 1. åˆå§‹åŒ–RAG Pipeline
    # å®ƒä¼šè‡ªåŠ¨å°è¯•åŠ è½½ç°æœ‰æ•°æ®åº“
    rag_pipeline = RagPipeline()

    # 2. æ ¸å¿ƒæ­¥éª¤ï¼šåŒæ­¥æ•°æ®æ–‡ä»¶å¤¹
    print("\n--- æ­£åœ¨æ£€æŸ¥å¹¶åŒæ­¥çŸ¥è¯†åº“ ---")
    rag_pipeline.sync_data_directory()
    
    # 3. å¼€å§‹äº¤äº’å¼é—®ç­”
    print("\n--- é—®ç­”ç¯èŠ‚ ---")
    print("çŸ¥è¯†åº“å·²å°±ç»ªã€‚æ‚¨å¯ä»¥å¼€å§‹æé—®äº†ã€‚è¾“å…¥ 'é€€å‡º' æˆ– 'exit' æˆ– 'quit' æ¥ç»“æŸç¨‹åºã€‚")
    
    while True:
        question = input("\n[æ‚¨]ï¼š")
        if question.lower() in ['é€€å‡º', 'exit', 'quit']:
            print("æ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§ï¼")
            break
            
        if not question.strip():
            print("[ç³»ç»Ÿ]ï¼šé—®é¢˜ä¸èƒ½ä¸ºç©ºï¼Œè¯·é‡æ–°è¾“å…¥ã€‚")
            continue
            
        answer_dict = rag_pipeline.ask(question)
        
        print("\n" + "-"*20 + " å›ç­” " + "-"*20)
        print(f"[æœºå™¨äºº]ï¼š{answer_dict.get('result', 'æœªèƒ½è·å–åˆ°ç­”æ¡ˆã€‚').strip()}")
        
        source_documents = answer_dict.get('source_documents', [])
        if source_documents:
            print("\n--- å‚è€ƒèµ„æ–™æ¥æº ---")
            for i, doc in enumerate(source_documents):
                source = doc.metadata.get('source', 'æœªçŸ¥æ¥æº')
                print(f"[{i+1}] æ¥æº: {os.path.basename(source)}")
                print(f"    å†…å®¹: {doc.page_content.replace('\n', ' ')}\n")
        print("-" * 46)

if __name__ == "__main__":
    run_demo()
```

## `pyproject.toml`

```
[project]
name = "rag-example"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
requires-python = ">=3.12"
dependencies = [
    "chromadb>=1.0.15",
    "dotenv>=0.9.9",
    "fastapi>=0.116.1",
    "jieba>=0.42.1",
    "langchain>=0.3.26",
    "langchain-chroma>=0.2.5",
    "langchain-community>=0.3.27",
    "langchain-huggingface>=0.3.1",
    "langchain-openai>=0.3.28",
    "openai>=1.97.0",
    "python-multipart>=0.0.20",
    "rank-bm25>=0.2.2",
    "sentence-transformers>=5.0.0",
    "sse-starlette>=3.0.2",
    "uvicorn>=0.35.0",
    "watchdog>=6.0.0",
    "websockets>=14.0",
]

```

## `rag/__init__.py`

```python
[æ–‡ä»¶ä¸ºç©º]
```

## `rag/async_pipeline.py`

```python
# rag/async_pipeline.py

import os
import hashlib
import asyncio
from typing import List, Dict, Any, Set, Optional
from concurrent.futures import ThreadPoolExecutor

# å¯¼å…¥åŒæ­¥ç‰ˆæœ¬çš„RagPipeline
from .pipeline import RagPipeline
from . import config
# å¯¼å…¥æç¤ºè¯ç®¡ç†å™¨
from .prompt_manager import get_qa_prompt_template, get_query_rewrite_prompt_template
# å¯¼å…¥çŸ­æœŸè®°å¿†ç®¡ç†å™¨
from .memory_manager import memory_manager

# å¯¼å…¥éœ€è¦çš„ç»„ä»¶
from langchain_community.document_loaders import TextLoader
from langchain_core.documents import Document


class AsyncRagPipeline(RagPipeline):
    """
    å¼‚æ­¥ç‰ˆæœ¬çš„RAGæµç¨‹ç±» (ç‰ˆæœ¬ 4.0 - å¼‚æ­¥å¢å¼ºç‰ˆ)ã€‚
    ç»§æ‰¿è‡ªRagPipelineï¼Œæ·»åŠ å¼‚æ­¥æ“ä½œæ”¯æŒã€‚
    ç‰¹æ€§:
    - æ”¯æŒå¼‚æ­¥æ“ä½œï¼Œæé«˜å¹¶å‘æ€§èƒ½
    - ç»§æ‰¿æ‰€æœ‰åŒæ­¥åŠŸèƒ½
    - å¼‚æ­¥é—®ç­”å’Œæ£€ç´¢åŠŸèƒ½
    - å¼‚æ­¥æ–‡æ¡£ç®¡ç†åŠŸèƒ½
    """
    def __init__(self):
        """åˆå§‹åŒ–å¼‚æ­¥RAGæµç¨‹ï¼Œç»§æ‰¿çˆ¶ç±»åŠŸèƒ½å¹¶æ·»åŠ çº¿ç¨‹æ± ã€‚"""
        print("æ­£åœ¨åˆå§‹åŒ–å¼‚æ­¥ RAG Pipeline...")
        # åˆå§‹åŒ–çº¿ç¨‹æ± ç”¨äºCPUå¯†é›†å‹ä»»åŠ¡
        self.executor = ThreadPoolExecutor(max_workers=4)
        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        super().__init__()
        print("å¼‚æ­¥ RAG Pipeline åˆå§‹åŒ–å®Œæˆã€‚")

    async def _run_in_executor(self, func, *args):
        """åœ¨çº¿ç¨‹æ± ä¸­è¿è¡ŒCPUå¯†é›†å‹ä»»åŠ¡ã€‚"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(self.executor, func, *args)

    async def get_processed_sources_async(self) -> Set[str]:
        """
        å¼‚æ­¥è·å–å‘é‡æ•°æ®åº“ä¸­æ‰€æœ‰å·²å¤„ç†è¿‡çš„æ–‡æ¡£æºè·¯å¾„ã€‚
        
        Returns:
            ä¸€ä¸ªåŒ…å«æ‰€æœ‰å”¯ä¸€æºæ–‡ä»¶è·¯å¾„çš„é›†åˆ(Set)ã€‚
        """
        if not self.vector_store:
            return set()
        
        try:
            # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œæ•°æ®åº“æŸ¥è¯¢
            all_entries = await self._run_in_executor(
                lambda: self.vector_store.get(include=["metadatas"])
            )
            
            sources = {
                metadata['source'] 
                for metadata in all_entries['metadatas'] 
                if metadata and 'source' in metadata
            }
            return sources
        except Exception as e:
            print(f"ä»æ•°æ®åº“è·å–æºæ–‡ä»¶åˆ—è¡¨æ—¶å‡ºé”™: {e}")
            return set()

    async def _get_file_info_async(self, file_path: str) -> Dict[str, Any]:
        """
        å¼‚æ­¥è·å–æ–‡ä»¶çš„è¯¦ç»†ä¿¡æ¯ï¼ŒåŒ…æ‹¬ä¿®æ”¹æ—¶é—´å’Œå†…å®¹å“ˆå¸Œã€‚
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            åŒ…å«æ–‡ä»¶ä¿¡æ¯çš„å­—å…¸
        """
        def _get_file_info_sync():
            try:
                stat = os.stat(file_path)
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                return {
                    'path': file_path,
                    'mtime': stat.st_mtime,
                    'size': stat.st_size,
                    'hash': hashlib.md5(content.encode('utf-8')).hexdigest()
                }
            except Exception as e:
                print(f"è·å–æ–‡ä»¶ä¿¡æ¯å¤±è´¥ {file_path}: {e}")
                return None
        
        return await self._run_in_executor(_get_file_info_sync)

    async def _get_file_metadata_from_db_async(self, file_path: str) -> Optional[Dict[str, Any]]:
        """
        å¼‚æ­¥ä»æ•°æ®åº“ä¸­è·å–æ–‡ä»¶çš„å…ƒæ•°æ®ä¿¡æ¯ã€‚
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            æ–‡ä»¶çš„å…ƒæ•°æ®ä¿¡æ¯ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å›None
        """
        if not self.vector_store:
            return None
        
        try:
            all_entries = await self._run_in_executor(
                lambda: self.vector_store.get(
                    where={"source": file_path},
                    include=["metadatas"]
                )
            )
            
            if all_entries['metadatas']:
                return all_entries['metadatas'][0]
            
            return None
        except Exception as e:
            print(f"ä»æ•°æ®åº“è·å–æ–‡ä»¶å…ƒæ•°æ®å¤±è´¥ {file_path}: {e}")
            return None

    async def _is_file_modified_async(self, file_path: str) -> bool:
        """
        å¼‚æ­¥æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²è¢«ä¿®æ”¹ã€‚
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            å¦‚æœæ–‡ä»¶å·²ä¿®æ”¹è¿”å›Trueï¼Œå¦åˆ™è¿”å›False
        """
        current_info = await self._get_file_info_async(file_path)
        if not current_info:
            return False
        
        db_metadata = await self._get_file_metadata_from_db_async(file_path)
        if not db_metadata:
            return True  # æ•°æ®åº“ä¸­æ²¡æœ‰è¯¥æ–‡ä»¶ï¼Œè§†ä¸ºæ–°æ–‡ä»¶
        
        # æ¯”è¾ƒæ–‡ä»¶å“ˆå¸Œå€¼
        db_hash = db_metadata.get('file_hash')
        return current_info['hash'] != db_hash

    async def delete_documents_by_source_async(self, source_path: str) -> bool:
        """
        å¼‚æ­¥æ ¹æ®æºæ–‡ä»¶è·¯å¾„åˆ é™¤å‘é‡æ•°æ®åº“ä¸­çš„ç›¸å…³æ–‡æ¡£ã€‚
        
        Args:
            source_path: æºæ–‡ä»¶è·¯å¾„
            
        Returns:
            åˆ é™¤æˆåŠŸè¿”å›Trueï¼Œå¦åˆ™è¿”å›False
        """
        if not self.vector_store:
            print("å‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–ï¼Œæ— æ³•åˆ é™¤æ–‡æ¡£ã€‚")
            return False
        
        try:
            # è·å–è¯¥æ–‡ä»¶çš„æ‰€æœ‰æ–‡æ¡£ID
            all_entries = await self._run_in_executor(
                lambda: self.vector_store.get(
                    where={"source": source_path},
                    include=["metadatas"]
                )
            )
            
            if not all_entries['ids']:
                print(f"æœªæ‰¾åˆ°æ¥æºä¸º '{source_path}' çš„æ–‡æ¡£ã€‚")
                return False
            
            # åˆ é™¤æ‰€æœ‰ç›¸å…³æ–‡æ¡£
            await self._run_in_executor(
                lambda: self.vector_store.delete(ids=all_entries['ids'])
            )
            print(f"å·²åˆ é™¤ {len(all_entries['ids'])} ä¸ªæ¥æºä¸º '{source_path}' çš„æ–‡æ¡£å—ã€‚")
            return True
            
        except Exception as e:
            print(f"åˆ é™¤æ–‡æ¡£æ—¶å‡ºé”™: {e}")
            return False

    async def update_document_async(self, file_path: str) -> bool:
        """
        å¼‚æ­¥æ›´æ–°å•ä¸ªæ–‡æ¡£ï¼šå…ˆåˆ é™¤æ—§ç‰ˆæœ¬ï¼Œå†æ·»åŠ æ–°ç‰ˆæœ¬ã€‚
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            æ›´æ–°æˆåŠŸè¿”å›Trueï¼Œå¦åˆ™è¿”å›False
        """
        try:
            print(f"æ­£åœ¨æ›´æ–°æ–‡æ¡£: {file_path}")
            
            # 1. åˆ é™¤æ—§ç‰ˆæœ¬
            if not await self.delete_documents_by_source_async(file_path):
                print(f"åˆ é™¤æ—§ç‰ˆæœ¬å¤±è´¥: {file_path}")
                return False
            
            # 2. å¼‚æ­¥åŠ è½½æ–°ç‰ˆæœ¬
            def load_document():
                loader = TextLoader(file_path, encoding='utf-8')
                return loader.load()
            
            new_docs = await self._run_in_executor(load_document)
            
            # 3. æ·»åŠ æ–‡ä»¶ä¿¡æ¯åˆ°å…ƒæ•°æ®
            file_info = await self._get_file_info_async(file_path)
            if file_info:
                for doc in new_docs:
                    doc.metadata.update({
                        'file_hash': file_info['hash'],
                        'file_mtime': file_info['mtime'],
                        'file_size': file_info['size']
                    })
            
            # 4. åˆ†å‰²æ–‡æ¡£
            chunks = await self._run_in_executor(
                self.text_splitter.split_documents, new_docs
            )
            
            # 5. ç”Ÿæˆå”¯ä¸€ID
            for i, chunk in enumerate(chunks):
                chunk_id = f"{config.DOCUMENT_ID_PREFIX}{hashlib.md5(file_path.encode()).hexdigest()}_{i}"
                chunk.metadata['chunk_id'] = chunk_id
            
            # 6. æ·»åŠ åˆ°æ•°æ®åº“
            await self._run_in_executor(
                self.vector_store.add_documents, chunks
            )
            print(f"  - å·²æ›´æ–°æ–‡æ¡£ï¼Œæ–°å¢ {len(chunks)} ä¸ªæ–‡æœ¬å—ã€‚")
            
            return True
            
        except Exception as e:
            print(f"æ›´æ–°æ–‡æ¡£æ—¶å‡ºé”™: {e}")
            return False

    async def sync_data_directory_async(self):
        """
        å¼‚æ­¥ç‰ˆæœ¬çš„æ™ºèƒ½åŒæ­¥æ•°æ®ç›®å½•ã€‚æ”¯æŒå¤šè·¯å¾„ã€åˆ†ç±»ç®¡ç†ã€‚
        """
        if config.ENABLE_ENTERPRISE_MODE:
            print("--- å¼€å§‹å¼‚æ­¥ä¼ä¸šçº§æ™ºèƒ½åŒæ­¥ ---")
            await self._sync_enterprise_data_sources_async()
        else:
            print("--- å¼€å§‹å¼‚æ­¥ä¼ ç»Ÿæ¨¡å¼åŒæ­¥ ---")
            await self._sync_legacy_data_directory_async()

    async def _sync_legacy_data_directory_async(self):
        """
        å¼‚æ­¥ç‰ˆæœ¬çš„ä¼ ç»Ÿå•ä¸€æ•°æ®ç›®å½•åŒæ­¥ï¼ˆå‘åå…¼å®¹ï¼‰ã€‚
        """
        data_path = config.DATA_PATH
        if not os.path.exists(data_path):
            print(f"è­¦å‘Š: æ•°æ®ç›®å½• '{data_path}' ä¸å­˜åœ¨ã€‚")
            return

        print("--- å¼€å§‹å¼‚æ­¥æ™ºèƒ½åŒæ­¥æ•°æ®ç›®å½• ---")
        
        # 1. å¼‚æ­¥è·å–å·²å¤„ç†çš„æ–‡ä»¶åˆ—è¡¨
        processed_sources = await self.get_processed_sources_async()
        print(f"æ•°æ®åº“ä¸­å·²å­˜åœ¨ {len(processed_sources)} ä¸ªæ¥æºçš„æ–‡ä»¶ã€‚")

        # 2. å¼‚æ­¥æ‰«ææ•°æ®ç›®å½•ï¼Œæ‰¾å‡ºæ‰€æœ‰ .txt æ–‡ä»¶
        def scan_files():
            current_files = []
            for root, _, files in os.walk(data_path):
                for file in files:
                    if file.endswith(".txt"):
                        current_files.append(os.path.join(root, file))
            return current_files
        
        current_files = await self._run_in_executor(scan_files)
        print(f"å½“å‰ç›®å½•ä¸­å‘ç° {len(current_files)} ä¸ª .txt æ–‡ä»¶ã€‚")
        
        # 3. å¹¶å‘åˆ†ç±»å¤„ç†æ–‡ä»¶
        new_files = []
        modified_files = []
        unchanged_files = []
        
        # å¹¶å‘æ£€æŸ¥æ–‡ä»¶ä¿®æ”¹çŠ¶æ€
        file_check_tasks = []
        for file_path in current_files:
            if file_path not in processed_sources:
                new_files.append(file_path)
            elif config.ENABLE_FILE_MONITORING:
                file_check_tasks.append(self._is_file_modified_async(file_path))
            else:
                unchanged_files.append(file_path)
        
        # ç­‰å¾…æ‰€æœ‰æ–‡ä»¶æ£€æŸ¥å®Œæˆ
        if file_check_tasks:
            modification_results = await asyncio.gather(*file_check_tasks)
            for i, (file_path, is_modified) in enumerate(zip(
                [f for f in current_files if f in processed_sources and config.ENABLE_FILE_MONITORING],
                modification_results
            )):
                if is_modified:
                    modified_files.append(file_path)
                else:
                    unchanged_files.append(file_path)
        
        # 4. å¤„ç†å·²åˆ é™¤çš„æ–‡ä»¶
        deleted_files = []
        if config.AUTO_DELETE_MISSING_FILES:
            for processed_file in processed_sources:
                if processed_file not in current_files:
                    deleted_files.append(processed_file)
        
        # 5. æŠ¥å‘Šåˆ†æç»“æœ
        print(f"æ–‡ä»¶åˆ†æç»“æœ:")
        print(f"  - æ–°å¢æ–‡ä»¶: {len(new_files)} ä¸ª")
        print(f"  - ä¿®æ”¹æ–‡ä»¶: {len(modified_files)} ä¸ª")
        print(f"  - åˆ é™¤æ–‡ä»¶: {len(deleted_files)} ä¸ª")
        print(f"  - æœªå˜åŒ–æ–‡ä»¶: {len(unchanged_files)} ä¸ª")
        
        # 6. å¹¶å‘å¤„ç†åˆ é™¤çš„æ–‡ä»¶
        if deleted_files:
            print("\n--- å¤„ç†å·²åˆ é™¤çš„æ–‡ä»¶ ---")
            delete_tasks = [self.delete_documents_by_source_async(file_path) for file_path in deleted_files]
            delete_results = await asyncio.gather(*delete_tasks)
            
            for file_path, success in zip(deleted_files, delete_results):
                if success:
                    print(f"  âœ“ å·²åˆ é™¤: {file_path}")
                else:
                    print(f"  âœ— åˆ é™¤å¤±è´¥: {file_path}")
        
        # 7. å¹¶å‘å¤„ç†ä¿®æ”¹çš„æ–‡ä»¶
        if modified_files:
            print("\n--- å¤„ç†å·²ä¿®æ”¹çš„æ–‡ä»¶ ---")
            update_tasks = [self.update_document_async(file_path) for file_path in modified_files]
            update_results = await asyncio.gather(*update_tasks)
            
            for file_path, success in zip(modified_files, update_results):
                if success:
                    print(f"  âœ“ å·²æ›´æ–°: {file_path}")
                else:
                    print(f"  âœ— æ›´æ–°å¤±è´¥: {file_path}")
        
        # 8. å¤„ç†æ–°å¢çš„æ–‡ä»¶
        if new_files:
            print(f"\n--- å¤„ç†æ–°å¢çš„æ–‡ä»¶ ---")
            await self._process_new_files_async(new_files)
        
        # 9. é‡æ–°æ„å»ºé—®ç­”é“¾ï¼ˆå¦‚æœæœ‰ä»»ä½•å˜åŒ–ï¼‰
        if new_files or modified_files or deleted_files:
            print("\n--- æ›´æ–°é—®ç­”é“¾ ---")
            await self._rebuild_qa_chain_async()
            print("é—®ç­”é“¾å·²æ›´æ–°ï¼ŒåŒ…å«æœ€æ–°çŸ¥è¯†ã€‚")
        else:
            print("\n--- æ— éœ€æ›´æ–° ---")
            print("æ‰€æœ‰æ–‡ä»¶éƒ½æ˜¯æœ€æ–°çš„ï¼Œæ— éœ€æ›´æ–°é—®ç­”é“¾ã€‚")
        
        print("--- å¼‚æ­¥æ™ºèƒ½åŒæ­¥å®Œæˆ ---")

    async def _process_new_files_async(self, new_files: List[str]):
        """
        å¼‚æ­¥å¤„ç†æ–°å¢çš„æ–‡ä»¶ã€‚
        
        Args:
            new_files: æ–°å¢æ–‡ä»¶åˆ—è¡¨
        """
        print(f"å‘ç° {len(new_files)} ä¸ªæ–°æ–‡æ¡£ï¼Œæ­£åœ¨å¤„ç†...")
        
        # å¹¶å‘åŠ è½½æ–°æ–‡æ¡£
        async def load_single_file(file_path: str):
            try:
                def load_doc():
                    loader = TextLoader(file_path, encoding='utf-8')
                    return loader.load()
                
                docs = await self._run_in_executor(load_doc)
                
                # æ·»åŠ æ–‡ä»¶ä¿¡æ¯åˆ°å…ƒæ•°æ®
                file_info = await self._get_file_info_async(file_path)
                if file_info:
                    for doc in docs:
                        doc.metadata.update({
                            'file_hash': file_info['hash'],
                            'file_mtime': file_info['mtime'],
                            'file_size': file_info['size']
                        })
                
                print(f"  âœ“ å·²åŠ è½½: {file_path}")
                return docs
            except Exception as e:
                print(f"  âœ— åŠ è½½å¤±è´¥: {file_path} - {e}")
                return []
        
        # å¹¶å‘åŠ è½½æ‰€æœ‰æ–°æ–‡ä»¶
        load_tasks = [load_single_file(file_path) for file_path in new_files]
        all_docs_lists = await asyncio.gather(*load_tasks)
        
        # åˆå¹¶æ‰€æœ‰æ–‡æ¡£
        new_docs = []
        for docs_list in all_docs_lists:
            new_docs.extend(docs_list)
        
        if new_docs:
            # åˆ†å‰²æ–‡æ¡£
            chunks = await self._run_in_executor(
                self.text_splitter.split_documents, new_docs
            )
            print(f"  - æ–°æ–‡æ¡£è¢«åˆ†å‰²æˆ {len(chunks)} ä¸ªæ–‡æœ¬å—ã€‚")

            # ç”Ÿæˆå”¯ä¸€ID
            for chunk in chunks:
                source_path = chunk.metadata.get('source', '')
                chunk_hash = hashlib.md5(f"{source_path}_{chunk.page_content}".encode()).hexdigest()
                chunk.metadata['chunk_id'] = f"{config.DOCUMENT_ID_PREFIX}{chunk_hash}"

            # æ·»åŠ åˆ°æ•°æ®åº“
            if self.vector_store is None:
                print("æ­£åœ¨åˆ›å»ºæ–°çš„å‘é‡æ•°æ®åº“...")
                
                def create_vector_store():
                    from langchain_chroma import Chroma
                    return Chroma.from_documents(
                        documents=chunks,
                        embedding=self.embeddings,
                        persist_directory=config.VECTOR_STORE_PATH
                    )
                
                self.vector_store = await self._run_in_executor(create_vector_store)
                print(f"  - æ–°çš„å‘é‡æ•°æ®åº“å·²åˆ›å»ºäº '{config.VECTOR_STORE_PATH}'ã€‚")
            else:
                await self._run_in_executor(self.vector_store.add_documents, chunks)
                print("  - æ–°çš„æ–‡æœ¬å—å·²æˆåŠŸæ·»åŠ åˆ°ç°æœ‰æ•°æ®åº“ã€‚")

    async def _rebuild_qa_chain_async(self):
        """
        å¼‚æ­¥é‡æ–°æ„å»ºé—®ç­”é“¾ã€‚
        """
        def rebuild_sync():
            self._load_all_documents()  # é‡æ–°åŠ è½½æ‰€æœ‰æ–‡æ¡£ç”¨äºå…³é”®å­—æ£€ç´¢
            self._build_qa_chain()
        
        await self._run_in_executor(rebuild_sync)

    async def ask_async(self, question: str, use_memory: bool = True) -> Dict[str, Any]:
        """
        å¼‚æ­¥ç‰ˆæœ¬çš„é—®ç­”åŠŸèƒ½ã€‚
        æ”¯æŒé—®é¢˜æ”¹å†™åŠŸèƒ½ï¼Œæé«˜æœç´¢è¦†ç›–é¢ã€‚

        Args:
            question: ç”¨æˆ·æå‡ºçš„é—®é¢˜å­—ç¬¦ä¸²ã€‚

        Returns:
            ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«'result' (ç­”æ¡ˆ) å’Œ 'source_documents' (å‚è€ƒçš„æ–‡æ¡£ç‰‡æ®µ)ã€‚
        """
        if not self.qa_chain:
            return {
                "result": "é”™è¯¯: é—®ç­”é“¾å°šæœªåˆå§‹åŒ–ã€‚è¯·å…ˆè°ƒç”¨ `sync_data_directory_async` æ–¹æ³•åŠ è½½æ–‡æ¡£ã€‚",
                "source_documents": []
            }
        
        print(f"\næ­£åœ¨å¼‚æ­¥å¤„ç†é—®é¢˜: '{question}'...")
        
        # è·å–çŸ­æœŸè®°å¿†ä¸Šä¸‹æ–‡ï¼ˆæ‰€æœ‰åˆ†æ”¯éƒ½éœ€è¦ï¼‰
        memory_context = ""
        if use_memory and config.ENABLE_SHORT_TERM_MEMORY:
            memory_context = memory_manager.get_conversation_context(include_count=None)  # ä½¿ç”¨æ‰€æœ‰å¯¹è¯è½®æ¬¡
            if memory_context:
                print("--- çŸ­æœŸè®°å¿†ä¸Šä¸‹æ–‡ ---")
                print(f"åŒ…å« {len(memory_manager.get_recent_conversations())} è½®å¯¹è¯ä½œä¸ºä¸Šä¸‹æ–‡")
        
        # å¦‚æœå¯ç”¨äº†é—®é¢˜æ”¹å†™åŠŸèƒ½
        if config.ENABLE_QUERY_REWRITING:
            print("--- å¼‚æ­¥é—®é¢˜æ”¹å†™é˜¶æ®µ ---")
            
            # 1. å¼‚æ­¥æ”¹å†™é—®é¢˜
            rewritten_queries = await self._rewrite_query_async(question)
            
            # 2. ä½¿ç”¨å¤šä¸ªé—®é¢˜è¿›è¡Œå¼‚æ­¥æ£€ç´¢
            print("--- å¼‚æ­¥å¤šæŸ¥è¯¢æ£€ç´¢é˜¶æ®µ ---")
            retrieved_docs = await self._retrieve_with_multiple_queries_async(rewritten_queries)
            
            # 3. å¼‚æ­¥é‡æ’åº
            print("--- å¼‚æ­¥é‡æ’åºé˜¶æ®µ ---")
            if retrieved_docs and self.reranker:
                try:
                    reranked_docs = await self._run_in_executor(
                        self.reranker.compress_documents, retrieved_docs, question
                    )
                    final_docs = reranked_docs[:config.RERANKER_TOP_N]
                    print(f"  - é‡æ’åºå®Œæˆï¼Œæœ€ç»ˆé€‰æ‹© {len(final_docs)} ä¸ªæœ€ç›¸å…³æ–‡æ¡£")
                except Exception as e:
                    print(f"  - é‡æ’åºå¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹æ£€ç´¢ç»“æœ")
                    final_docs = retrieved_docs[:config.RERANKER_TOP_N]
            else:
                final_docs = retrieved_docs[:config.RERANKER_TOP_N]
            
            # 4. å¼‚æ­¥ç”Ÿæˆç­”æ¡ˆ
            print("--- å¼‚æ­¥ç­”æ¡ˆç”Ÿæˆé˜¶æ®µ ---")
            if final_docs:
                # æ„å»ºä¸Šä¸‹æ–‡
                context = "\n\n".join([doc.page_content for doc in final_docs])
                
                # æ„å»ºå®Œæ•´çš„ä¸Šä¸‹æ–‡ï¼ˆåŒ…å«è®°å¿†ä¸Šä¸‹æ–‡å’Œæ£€ç´¢ä¸Šä¸‹æ–‡ï¼‰
                full_context = context
                if memory_context:
                    full_context = f"å¯¹è¯å†å²:\n{memory_context}\n\nå½“å‰æ£€ç´¢åˆ°çš„ç›¸å…³ä¿¡æ¯:\n{context}"
                
                # ä½¿ç”¨è‡ªå®šä¹‰æç¤ºæ¨¡æ¿ç”Ÿæˆç­”æ¡ˆ
                # ä½¿ç”¨æç¤ºè¯ç®¡ç†å™¨è·å–é—®ç­”æç¤ºæ¨¡æ¿
                qa_template = get_qa_prompt_template()
                prompt_template = qa_template.template
                
                prompt = prompt_template.format(context=full_context, question=question)
                response = await self._run_in_executor(self.llm.invoke, prompt)
                
                if hasattr(response, 'content'):
                    answer = response.content.strip()
                else:
                    answer = str(response).strip()
                
                # ä¿å­˜å¯¹è¯åˆ°çŸ­æœŸè®°å¿†
                if use_memory and config.ENABLE_SHORT_TERM_MEMORY:
                    memory_manager.add_conversation(
                        question=question,
                        answer=answer,
                        metadata={
                            "source_documents_count": len(final_docs),
                            "memory_context_included": bool(memory_context),
                            "async_mode": True
                        }
                    )
                
                return {
                    "result": answer,
                    "source_documents": final_docs
                }
            else:
                no_result_answer = "æ ¹æ®æä¾›çš„èµ„æ–™ï¼Œæˆ‘æ— æ³•å›ç­”è¯¥é—®é¢˜ã€‚"
                
                # ä¿å­˜å¯¹è¯åˆ°çŸ­æœŸè®°å¿†
                if use_memory and config.ENABLE_SHORT_TERM_MEMORY:
                    memory_manager.add_conversation(
                        question=question,
                        answer=no_result_answer,
                        metadata={
                            "source_documents_count": 0,
                            "memory_context_included": False,
                            "async_mode": True,
                            "no_result": True
                        }
                    )
                
                return {
                    "result": no_result_answer,
                    "source_documents": []
                }
        else:
            # ä½¿ç”¨åŸå§‹çš„é—®ç­”é“¾ï¼Œä½†éœ€è¦æ‰‹åŠ¨å¤„ç†è®°å¿†ä¸Šä¸‹æ–‡
            print("--- å¼‚æ­¥åŸå§‹é—®ç­”é“¾æ¨¡å¼ ---")
            
            # å…ˆè·å–ç›¸å…³æ–‡æ¡£
            retriever = self.qa_chain.retriever
            retrieved_docs = await self._run_in_executor(
                retriever.get_relevant_documents, question
            )
            
            if retrieved_docs:
                # æ„å»ºä¸Šä¸‹æ–‡
                context = "\n\n".join([doc.page_content for doc in retrieved_docs])
                
                # æ„å»ºå®Œæ•´çš„ä¸Šä¸‹æ–‡ï¼ˆåŒ…å«è®°å¿†ä¸Šä¸‹æ–‡å’Œæ£€ç´¢ä¸Šä¸‹æ–‡ï¼‰
                full_context = context
                if memory_context:
                    full_context = f"å¯¹è¯å†å²:\n{memory_context}\n\nå½“å‰æ£€ç´¢åˆ°çš„ç›¸å…³ä¿¡æ¯:\n{context}"
                
                # ä½¿ç”¨æç¤ºè¯ç®¡ç†å™¨è·å–é—®ç­”æç¤ºæ¨¡æ¿
                qa_template = get_qa_prompt_template()
                prompt = qa_template.format(context=full_context, question=question)
                response = await self._run_in_executor(self.llm.invoke, prompt)
                
                if hasattr(response, 'content'):
                    answer = response.content.strip()
                else:
                    answer = str(response).strip()
                
                result = {
                    "result": answer,
                    "source_documents": retrieved_docs
                }
            else:
                no_result_answer = "æ ¹æ®æä¾›çš„èµ„æ–™ï¼Œæˆ‘æ— æ³•å›ç­”è¯¥é—®é¢˜ã€‚"
                result = {
                    "result": no_result_answer,
                    "source_documents": []
                }
            
            # ä¿å­˜å¯¹è¯åˆ°çŸ­æœŸè®°å¿†ï¼ˆåŸå§‹é—®ç­”é“¾æ¨¡å¼ï¼‰
            if use_memory and config.ENABLE_SHORT_TERM_MEMORY:
                memory_manager.add_conversation(
                    question=question,
                    answer=result.get("result", ""),
                    metadata={
                        "source_documents_count": len(result.get("source_documents", [])),
                        "memory_context_included": bool(memory_context),
                        "async_mode": True,
                        "original_chain": True
                    }
                )
            
            return result

    async def ask_with_categories_async(self, question: str, categories: List[str] = None, use_memory: bool = True) -> Dict[str, Any]:
        """
        å¼‚æ­¥ç‰ˆæœ¬çš„æ”¯æŒåˆ†ç±»æ£€ç´¢çš„é—®ç­”åŠŸèƒ½ã€‚
        
        Args:
            question: ç”¨æˆ·æå‡ºçš„é—®é¢˜å­—ç¬¦ä¸²
            categories: æŒ‡å®šæ£€ç´¢çš„ç±»åˆ«åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºæ£€ç´¢æ‰€æœ‰ç±»åˆ«
            
        Returns:
            ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«'result' (ç­”æ¡ˆ) å’Œ 'source_documents' (å‚è€ƒçš„æ–‡æ¡£ç‰‡æ®µ)
        """
        if not self.qa_chain:
            return {
                "result": "é”™è¯¯: é—®ç­”é“¾å°šæœªåˆå§‹åŒ–ã€‚è¯·å…ˆè°ƒç”¨ `sync_data_directory_async` æ–¹æ³•åŠ è½½æ–‡æ¡£ã€‚",
                "source_documents": []
            }
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šç±»åˆ«ï¼Œä½¿ç”¨é»˜è®¤ç±»åˆ«æˆ–æ‰€æœ‰ç±»åˆ«
        if categories is None:
            categories = config.DEFAULT_SEARCH_CATEGORIES
        
        print(f"\næ­£åœ¨å¼‚æ­¥å¤„ç†é—®é¢˜: '{question}'...")
        if categories:
            print(f"é™å®šæ£€ç´¢ç±»åˆ«: {categories}")
        
        # è·å–çŸ­æœŸè®°å¿†ä¸Šä¸‹æ–‡ï¼ˆæ‰€æœ‰åˆ†æ”¯éƒ½éœ€è¦ï¼‰
        memory_context = ""
        if use_memory and config.ENABLE_SHORT_TERM_MEMORY:
            memory_context = memory_manager.get_conversation_context(include_count=None)  # ä½¿ç”¨æ‰€æœ‰å¯¹è¯è½®æ¬¡
            if memory_context:
                print("--- çŸ­æœŸè®°å¿†ä¸Šä¸‹æ–‡ ---")
                print(f"åŒ…å« {len(memory_manager.get_recent_conversations())} è½®å¯¹è¯ä½œä¸ºä¸Šä¸‹æ–‡")
        
        # å¦‚æœå¯ç”¨äº†é—®é¢˜æ”¹å†™åŠŸèƒ½
        if config.ENABLE_QUERY_REWRITING:
            print("--- å¼‚æ­¥é—®é¢˜æ”¹å†™é˜¶æ®µ ---")
            
            # 1. å¼‚æ­¥æ”¹å†™é—®é¢˜
            rewritten_queries = await self._rewrite_query_async(question)
            
            # 2. ä½¿ç”¨å¤šä¸ªé—®é¢˜è¿›è¡Œå¼‚æ­¥åˆ†ç±»æ£€ç´¢
            print("--- å¼‚æ­¥å¤šæŸ¥è¯¢åˆ†ç±»æ£€ç´¢é˜¶æ®µ ---")
            retrieved_docs = await self._retrieve_with_multiple_queries_and_categories_async(rewritten_queries, categories)
            
            # 3. å¼‚æ­¥é‡æ’åº
            print("--- å¼‚æ­¥é‡æ’åºé˜¶æ®µ ---")
            if retrieved_docs and self.reranker:
                try:
                    reranked_docs = await self._run_in_executor(
                        self.reranker.compress_documents, retrieved_docs, question
                    )
                    final_docs = reranked_docs[:config.RERANKER_TOP_N]
                    print(f"  - é‡æ’åºå®Œæˆï¼Œæœ€ç»ˆé€‰æ‹© {len(final_docs)} ä¸ªæœ€ç›¸å…³æ–‡æ¡£")
                except Exception as e:
                    print(f"  - é‡æ’åºå¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹æ£€ç´¢ç»“æœ")
                    final_docs = retrieved_docs[:config.RERANKER_TOP_N]
            else:
                final_docs = retrieved_docs[:config.RERANKER_TOP_N]
            
            # 4. å¼‚æ­¥ç”Ÿæˆç­”æ¡ˆ
            print("--- å¼‚æ­¥ç­”æ¡ˆç”Ÿæˆé˜¶æ®µ ---")
            if final_docs:
                # æ„å»ºä¸Šä¸‹æ–‡
                context = "\n\n".join([doc.page_content for doc in final_docs])
                
                # æ„å»ºå®Œæ•´çš„ä¸Šä¸‹æ–‡ï¼ˆåŒ…å«è®°å¿†ä¸Šä¸‹æ–‡å’Œæ£€ç´¢ä¸Šä¸‹æ–‡ï¼‰
                full_context = context
                if memory_context:
                    full_context = f"å¯¹è¯å†å²:\n{memory_context}\n\nå½“å‰æ£€ç´¢åˆ°çš„ç›¸å…³ä¿¡æ¯:\n{context}"
                
                # ä½¿ç”¨è‡ªå®šä¹‰æç¤ºæ¨¡æ¿ç”Ÿæˆç­”æ¡ˆ
                # ä½¿ç”¨æç¤ºè¯ç®¡ç†å™¨è·å–é—®ç­”æç¤ºæ¨¡æ¿
                qa_template = get_qa_prompt_template()
                prompt_template = qa_template.template
                
                prompt = prompt_template.format(context=full_context, question=question)
                response = await self._run_in_executor(self.llm.invoke, prompt)
                
                if hasattr(response, 'content'):
                    answer = response.content.strip()
                else:
                    answer = str(response).strip()
                
                # ä¿å­˜å¯¹è¯åˆ°çŸ­æœŸè®°å¿†
                if use_memory and config.ENABLE_SHORT_TERM_MEMORY:
                    memory_manager.add_conversation(
                        question=question,
                        answer=answer,
                        metadata={
                            "used_query_rewriting": True,
                            "used_categories": categories,
                            "memory_context_included": bool(memory_context),
                            "source_documents_count": len(final_docs),
                            "async_mode": True
                        }
                    )
                
                return {
                    "result": answer,
                    "source_documents": final_docs
                }
            else:
                no_result_answer = "æ ¹æ®æä¾›çš„èµ„æ–™ï¼Œæˆ‘æ— æ³•å›ç­”è¯¥é—®é¢˜ã€‚"
                
                # ä¿å­˜å¯¹è¯åˆ°çŸ­æœŸè®°å¿†
                if use_memory and config.ENABLE_SHORT_TERM_MEMORY:
                    memory_manager.add_conversation(
                        question=question,
                        answer=no_result_answer,
                        metadata={
                            "used_query_rewriting": True,
                            "used_categories": categories,
                            "memory_context_included": bool(memory_context),
                            "source_documents_count": 0,
                            "async_mode": True,
                            "no_result": True
                        }
                    )
                
                return {
                    "result": no_result_answer,
                    "source_documents": []
                }
        else:
            # ä½¿ç”¨åˆ†ç±»æ£€ç´¢çš„é—®ç­”é“¾ï¼Œä½†éœ€è¦æ‰‹åŠ¨å¤„ç†è®°å¿†ä¸Šä¸‹æ–‡
            if categories:
                print("--- å¼‚æ­¥åˆ†ç±»æ£€ç´¢æ¨¡å¼ ---")
                
                # åˆ›å»ºä¸´æ—¶çš„åˆ†ç±»æ£€ç´¢å™¨å¹¶å¼‚æ­¥æ‰§è¡Œ
                def get_category_docs():
                    category_retriever = self._build_category_retriever(categories)
                    compression_retriever = ContextualCompressionRetriever(
                        base_compressor=self.reranker,
                        base_retriever=category_retriever
                    )
                    return compression_retriever.get_relevant_documents(question)
                
                retrieved_docs = await self._run_in_executor(get_category_docs)
                
                if retrieved_docs:
                    # æ„å»ºä¸Šä¸‹æ–‡
                    context = "\n\n".join([doc.page_content for doc in retrieved_docs])
                    
                    # æ„å»ºå®Œæ•´çš„ä¸Šä¸‹æ–‡ï¼ˆåŒ…å«è®°å¿†ä¸Šä¸‹æ–‡å’Œæ£€ç´¢ä¸Šä¸‹æ–‡ï¼‰
                    full_context = context
                    if memory_context:
                        full_context = f"å¯¹è¯å†å²:\n{memory_context}\n\nå½“å‰æ£€ç´¢åˆ°çš„ç›¸å…³ä¿¡æ¯:\n{context}"
                    
                    # ä½¿ç”¨æç¤ºè¯ç®¡ç†å™¨è·å–é—®ç­”æç¤ºæ¨¡æ¿
                    qa_template = get_qa_prompt_template()
                    prompt = qa_template.format(context=full_context, question=question)
                    response = await self._run_in_executor(self.llm.invoke, prompt)
                    
                    if hasattr(response, 'content'):
                        answer = response.content.strip()
                    else:
                        answer = str(response).strip()
                    
                    result = {
                        "result": answer,
                        "source_documents": retrieved_docs
                    }
                else:
                    no_result_answer = "æ ¹æ®æä¾›çš„èµ„æ–™ï¼Œæˆ‘æ— æ³•å›ç­”è¯¥é—®é¢˜ã€‚"
                    result = {
                        "result": no_result_answer,
                        "source_documents": []
                    }
                
                # ä¿å­˜å¯¹è¯åˆ°çŸ­æœŸè®°å¿†
                if use_memory and config.ENABLE_SHORT_TERM_MEMORY:
                    memory_manager.add_conversation(
                        question=question,
                        answer=result.get("result", ""),
                        metadata={
                            "used_query_rewriting": False,
                            "used_categories": categories,
                            "memory_context_included": bool(memory_context),
                            "source_documents_count": len(result.get("source_documents", [])),
                            "async_mode": True
                        }
                    )
                
                return result
            else:
                # ä½¿ç”¨åŸå§‹çš„é—®ç­”é“¾ï¼Œä½†éœ€è¦æ‰‹åŠ¨å¤„ç†è®°å¿†ä¸Šä¸‹æ–‡
                print("--- å¼‚æ­¥åŸå§‹é—®ç­”é“¾æ¨¡å¼ (ask_with_categories) ---")
                
                # å…ˆè·å–ç›¸å…³æ–‡æ¡£
                retriever = self.qa_chain.retriever
                retrieved_docs = await self._run_in_executor(
                    retriever.get_relevant_documents, question
                )
                
                if retrieved_docs:
                    # æ„å»ºä¸Šä¸‹æ–‡
                    context = "\n\n".join([doc.page_content for doc in retrieved_docs])
                    
                    # æ„å»ºå®Œæ•´çš„ä¸Šä¸‹æ–‡ï¼ˆåŒ…å«è®°å¿†ä¸Šä¸‹æ–‡å’Œæ£€ç´¢ä¸Šä¸‹æ–‡ï¼‰
                    full_context = context
                    if memory_context:
                        full_context = f"å¯¹è¯å†å²:\n{memory_context}\n\nå½“å‰æ£€ç´¢åˆ°çš„ç›¸å…³ä¿¡æ¯:\n{context}"
                    
                    # ä½¿ç”¨æç¤ºè¯ç®¡ç†å™¨è·å–é—®ç­”æç¤ºæ¨¡æ¿
                    qa_template = get_qa_prompt_template()
                    prompt = qa_template.format(context=full_context, question=question)
                    response = await self._run_in_executor(self.llm.invoke, prompt)
                    
                    if hasattr(response, 'content'):
                        answer = response.content.strip()
                    else:
                        answer = str(response).strip()
                    
                    result = {
                        "result": answer,
                        "source_documents": retrieved_docs
                    }
                else:
                    no_result_answer = "æ ¹æ®æä¾›çš„èµ„æ–™ï¼Œæˆ‘æ— æ³•å›ç­”è¯¥é—®é¢˜ã€‚"
                    result = {
                        "result": no_result_answer,
                        "source_documents": []
                    }
                
                # ä¿å­˜å¯¹è¯åˆ°çŸ­æœŸè®°å¿†
                if use_memory and config.ENABLE_SHORT_TERM_MEMORY:
                    memory_manager.add_conversation(
                        question=question,
                        answer=result.get("result", ""),
                        metadata={
                            "used_query_rewriting": False,
                            "used_categories": None,
                            "memory_context_included": bool(memory_context),
                            "source_documents_count": len(result.get("source_documents", [])),
                            "async_mode": True
                        }
                    )
                
                return result

    async def _rewrite_query_async(self, original_query: str) -> List[str]:
        """
        å¼‚æ­¥ç‰ˆæœ¬çš„é—®é¢˜æ”¹å†™åŠŸèƒ½ã€‚
        å°†åŸå§‹é—®é¢˜æ”¹å†™æˆå¤šä¸ªç›¸å…³é—®é¢˜ï¼Œæé«˜æœç´¢è¦†ç›–é¢ã€‚
        
        Args:
            original_query: ç”¨æˆ·çš„åŸå§‹é—®é¢˜
            
        Returns:
            åŒ…å«åŸå§‹é—®é¢˜å’Œæ”¹å†™é—®é¢˜çš„åˆ—è¡¨
        """
        if not config.ENABLE_QUERY_REWRITING:
            return [original_query]
        
        try:
            # æ„å»ºé—®é¢˜æ”¹å†™çš„æç¤ºæ¨¡æ¿
            from langchain_core.prompts import PromptTemplate
            
            # ä½¿ç”¨æç¤ºè¯ç®¡ç†å™¨è·å–é—®é¢˜æ”¹å†™æç¤ºæ¨¡æ¿
            rewrite_prompt = get_query_rewrite_prompt_template()
            
            # å¼‚æ­¥è°ƒç”¨LLMè¿›è¡Œé—®é¢˜æ”¹å†™
            prompt = rewrite_prompt.format(
                original_query=original_query,
                count=config.QUERY_REWRITE_COUNT
            )
            
            response = await self._run_in_executor(self.llm.invoke, prompt)
            
            # è§£ææ”¹å†™ç»“æœ
            rewritten_queries = []
            if hasattr(response, 'content'):
                content = response.content.strip()
            else:
                content = str(response).strip()
            
            # åˆ†å‰²å¹¶æ¸…ç†æ”¹å†™çš„é—®é¢˜
            lines = [line.strip() for line in content.split('\n') if line.strip()]
            for line in lines:
                # ç§»é™¤å¯èƒ½çš„ç¼–å·æ ¼å¼
                cleaned_line = line
                if line and (line[0].isdigit() or line.startswith('-') or line.startswith('â€¢')):
                    # ç§»é™¤ç¼–å·å’Œæ ‡ç‚¹
                    cleaned_line = line.split('.', 1)[-1].strip()
                    cleaned_line = cleaned_line.lstrip('- â€¢').strip()
                
                if cleaned_line and cleaned_line not in rewritten_queries:
                    rewritten_queries.append(cleaned_line)
            
            # ç¡®ä¿åŒ…å«åŸå§‹é—®é¢˜
            all_queries = [original_query]
            all_queries.extend(rewritten_queries[:config.QUERY_REWRITE_COUNT])
            
            print(f"  - é—®é¢˜æ”¹å†™å®Œæˆï¼Œç”Ÿæˆäº† {len(all_queries)} ä¸ªæŸ¥è¯¢é—®é¢˜")
            for i, query in enumerate(all_queries):
                print(f"    [{i+1}] {query}")
            
            return all_queries
            
        except Exception as e:
            print(f"é—®é¢˜æ”¹å†™å¤±è´¥: {e}")
            return [original_query]

    async def _retrieve_with_multiple_queries_async(self, queries: List[str]) -> List[Document]:
        """
        å¼‚æ­¥ç‰ˆæœ¬çš„å¤šæŸ¥è¯¢æ£€ç´¢åŠŸèƒ½ã€‚
        ä½¿ç”¨å¤šä¸ªæŸ¥è¯¢é—®é¢˜è¿›è¡Œæ£€ç´¢ï¼Œå¹¶åˆå¹¶ç»“æœã€‚
        
        Args:
            queries: æŸ¥è¯¢é—®é¢˜åˆ—è¡¨
            
        Returns:
            åˆå¹¶åçš„æ–‡æ¡£åˆ—è¡¨
        """
        all_documents = []
        seen_contents = set()  # ç”¨äºå»é‡
        
        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰æŸ¥è¯¢
        async def single_query_retrieve(query: str, index: int):
            print(f"  - æ‰§è¡ŒæŸ¥è¯¢ {index+1}: {query}")
            
            try:
                # ä½¿ç”¨æ··åˆæ£€ç´¢å™¨è¿›è¡Œæ£€ç´¢
                def retrieve_sync():
                    hybrid_retriever = self._build_hybrid_retriever()
                    
                    # ä¸ºæ”¹å†™çš„æŸ¥è¯¢ä½¿ç”¨è¾ƒå°‘çš„æ£€ç´¢æ•°é‡
                    if index == 0:  # åŸå§‹æŸ¥è¯¢ä½¿ç”¨æ­£å¸¸æ•°é‡
                        k = config.RETRIEVER_TOP_K
                    else:  # æ”¹å†™æŸ¥è¯¢ä½¿ç”¨è¾ƒå°‘æ•°é‡
                        k = config.REWRITE_QUERY_TOP_K
                    
                    # ä¸´æ—¶è°ƒæ•´æ£€ç´¢å™¨çš„kå€¼
                    if hasattr(hybrid_retriever, 'retrievers'):
                        # EnsembleRetriever
                        for retriever in hybrid_retriever.retrievers:
                            if hasattr(retriever, 'search_kwargs'):
                                retriever.search_kwargs['k'] = k
                            elif hasattr(retriever, 'k'):
                                retriever.k = k
                    
                    return hybrid_retriever.invoke(query)
                
                docs = await self._run_in_executor(retrieve_sync)
                print(f"    æ£€ç´¢åˆ° {len(docs)} ä¸ªæ–‡æ¡£")
                return docs
                
            except Exception as e:
                print(f"    æŸ¥è¯¢æ‰§è¡Œå¤±è´¥: {e}")
                return []
        
        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰æŸ¥è¯¢
        query_tasks = [single_query_retrieve(query, i) for i, query in enumerate(queries)]
        all_docs_lists = await asyncio.gather(*query_tasks)
        
        # åˆå¹¶ç»“æœå¹¶å»é‡
        for docs_list in all_docs_lists:
            for doc in docs_list:
                content_hash = hash(doc.page_content)
                if config.ENABLE_DOCUMENT_DEDUPLICATION:
                    if content_hash not in seen_contents:
                        all_documents.append(doc)
                        seen_contents.add(content_hash)
                else:
                    all_documents.append(doc)
        
        print(f"  - å¼‚æ­¥å¤šæŸ¥è¯¢æ£€ç´¢å®Œæˆï¼Œå…±è·å¾— {len(all_documents)} ä¸ªæ–‡æ¡£")
        return all_documents

    async def _retrieve_with_multiple_queries_and_categories_async(self, queries: List[str], categories: List[str] = None) -> List[Document]:
        """
        å¼‚æ­¥ç‰ˆæœ¬çš„å¤šæŸ¥è¯¢åˆ†ç±»æ£€ç´¢åŠŸèƒ½ã€‚
        ä½¿ç”¨å¤šä¸ªæŸ¥è¯¢é—®é¢˜è¿›è¡Œåˆ†ç±»æ£€ç´¢ï¼Œå¹¶åˆå¹¶ç»“æœã€‚
        
        Args:
            queries: æŸ¥è¯¢é—®é¢˜åˆ—è¡¨
            categories: æŒ‡å®šæ£€ç´¢çš„ç±»åˆ«åˆ—è¡¨
            
        Returns:
            åˆå¹¶åçš„æ–‡æ¡£åˆ—è¡¨
        """
        all_documents = []
        seen_contents = set()
        
        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰æŸ¥è¯¢
        async def single_category_query_retrieve(query: str, index: int):
            print(f"  - æ‰§è¡ŒæŸ¥è¯¢ {index+1}: {query}")
            
            try:
                # ä½¿ç”¨åˆ†ç±»æ£€ç´¢å™¨è¿›è¡Œæ£€ç´¢
                def retrieve_sync():
                    if categories:
                        category_retriever = self._build_category_retriever(categories)
                    else:
                        category_retriever = self._build_hybrid_retriever()
                    
                    # ä¸ºæ”¹å†™çš„æŸ¥è¯¢ä½¿ç”¨è¾ƒå°‘çš„æ£€ç´¢æ•°é‡
                    if index == 0:
                        k = config.RETRIEVER_TOP_K
                    else:
                        k = config.REWRITE_QUERY_TOP_K
                    
                    # ä¸´æ—¶è°ƒæ•´æ£€ç´¢å™¨çš„kå€¼
                    if hasattr(category_retriever, 'retrievers'):
                        for retriever in category_retriever.retrievers:
                            if hasattr(retriever, 'search_kwargs'):
                                retriever.search_kwargs['k'] = k
                            elif hasattr(retriever, 'k'):
                                retriever.k = k
                    
                    return category_retriever.invoke(query)
                
                docs = await self._run_in_executor(retrieve_sync)
                print(f"    æ£€ç´¢åˆ° {len(docs)} ä¸ªæ–‡æ¡£")
                return docs
                
            except Exception as e:
                print(f"    æŸ¥è¯¢æ‰§è¡Œå¤±è´¥: {e}")
                return []
        
        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰æŸ¥è¯¢
        query_tasks = [single_category_query_retrieve(query, i) for i, query in enumerate(queries)]
        all_docs_lists = await asyncio.gather(*query_tasks)
        
        # åˆå¹¶ç»“æœå¹¶å»é‡
        for docs_list in all_docs_lists:
            for doc in docs_list:
                content_hash = hash(doc.page_content)
                if config.ENABLE_DOCUMENT_DEDUPLICATION:
                    if content_hash not in seen_contents:
                        all_documents.append(doc)
                        seen_contents.add(content_hash)
                else:
                    all_documents.append(doc)
        
        print(f"  - å¼‚æ­¥å¤šæŸ¥è¯¢åˆ†ç±»æ£€ç´¢å®Œæˆï¼Œå…±è·å¾— {len(all_documents)} ä¸ªæ–‡æ¡£")
        return all_documents

    async def _sync_enterprise_data_sources_async(self):
        """
        å¼‚æ­¥ç‰ˆæœ¬çš„ä¼ä¸šçº§å¤šæ•°æ®æºåŒæ­¥ã€‚
        """
        # 1. å¼‚æ­¥æ‰«ææ‰€æœ‰ä¼ä¸šçº§æ•°æ®æº
        def scan_enterprise_files():
            return self._scan_enterprise_files()
        
        all_files_by_source = await self._run_in_executor(scan_enterprise_files)
        
        # 2. å¼‚æ­¥è·å–å·²å¤„ç†çš„æ–‡ä»¶åˆ—è¡¨
        processed_sources = await self.get_processed_sources_async()
        print(f"æ•°æ®åº“ä¸­å·²å­˜åœ¨ {len(processed_sources)} ä¸ªæ¥æºçš„æ–‡ä»¶ã€‚")
        
        # 3. åˆå¹¶æ‰€æœ‰æ•°æ®æºçš„æ–‡ä»¶
        all_current_files = []
        for source_name, files in all_files_by_source.items():
            all_current_files.extend(files)
        
        print(f"æ‰€æœ‰æ•°æ®æºå…±å‘ç° {len(all_current_files)} ä¸ªæ–‡ä»¶ã€‚")
        
        # 4. å¹¶å‘åˆ†ç±»å¤„ç†æ–‡ä»¶
        new_files = []
        modified_files = []
        unchanged_files = []
        
        # å¹¶å‘æ£€æŸ¥æ–‡ä»¶ä¿®æ”¹çŠ¶æ€
        file_check_tasks = []
        for file_path in all_current_files:
            if file_path not in processed_sources:
                new_files.append(file_path)
            elif config.ENABLE_FILE_MONITORING:
                file_check_tasks.append(self._is_file_modified_async(file_path))
            else:
                unchanged_files.append(file_path)
        
        # ç­‰å¾…æ‰€æœ‰æ–‡ä»¶æ£€æŸ¥å®Œæˆ
        if file_check_tasks:
            modification_results = await asyncio.gather(*file_check_tasks)
            for i, (file_path, is_modified) in enumerate(zip(
                [f for f in all_current_files if f in processed_sources and config.ENABLE_FILE_MONITORING],
                modification_results
            )):
                if is_modified:
                    modified_files.append(file_path)
                else:
                    unchanged_files.append(file_path)
        
        # 5. å¤„ç†å·²åˆ é™¤çš„æ–‡ä»¶
        deleted_files = []
        if config.AUTO_DELETE_MISSING_FILES:
            for processed_file in processed_sources:
                if processed_file not in all_current_files:
                    deleted_files.append(processed_file)
        
        # 6. æŠ¥å‘Šåˆ†æç»“æœ
        print(f"ä¼ä¸šçº§æ–‡ä»¶åˆ†æç»“æœ:")
        print(f"  - æ–°å¢æ–‡ä»¶: {len(new_files)} ä¸ª")
        print(f"  - ä¿®æ”¹æ–‡ä»¶: {len(modified_files)} ä¸ª")
        print(f"  - åˆ é™¤æ–‡ä»¶: {len(deleted_files)} ä¸ª")
        print(f"  - æœªå˜åŒ–æ–‡ä»¶: {len(unchanged_files)} ä¸ª")
        
        # 7. å¹¶å‘å¤„ç†åˆ é™¤çš„æ–‡ä»¶
        if deleted_files:
            print("\n--- å¤„ç†å·²åˆ é™¤çš„æ–‡ä»¶ ---")
            delete_tasks = [self.delete_documents_by_source_async(file_path) for file_path in deleted_files]
            delete_results = await asyncio.gather(*delete_tasks)
            
            for file_path, success in zip(deleted_files, delete_results):
                if success:
                    print(f"  âœ“ å·²åˆ é™¤: {file_path}")
                else:
                    print(f"  âœ— åˆ é™¤å¤±è´¥: {file_path}")
        
        # 8. å¹¶å‘å¤„ç†ä¿®æ”¹çš„æ–‡ä»¶
        if modified_files:
            print("\n--- å¤„ç†å·²ä¿®æ”¹çš„æ–‡ä»¶ ---")
            update_tasks = [self._update_enterprise_document_async(file_path, all_files_by_source) for file_path in modified_files]
            update_results = await asyncio.gather(*update_tasks)
            
            for file_path, success in zip(modified_files, update_results):
                if success:
                    print(f"  âœ“ å·²æ›´æ–°: {file_path}")
                else:
                    print(f"  âœ— æ›´æ–°å¤±è´¥: {file_path}")
        
        # 9. å¤„ç†æ–°å¢çš„æ–‡ä»¶
        if new_files:
            print(f"\n--- å¤„ç†æ–°å¢çš„æ–‡ä»¶ ---")
            await self._process_new_enterprise_files_async(new_files, all_files_by_source)
        
        # 10. é‡æ–°æ„å»ºé—®ç­”é“¾ï¼ˆå¦‚æœæœ‰ä»»ä½•å˜åŒ–ï¼‰
        if new_files or modified_files or deleted_files:
            print("\n--- æ›´æ–°é—®ç­”é“¾ ---")
            await self._rebuild_qa_chain_async()
            print("é—®ç­”é“¾å·²æ›´æ–°ï¼ŒåŒ…å«æœ€æ–°çŸ¥è¯†ã€‚")
        else:
            print("\n--- æ— éœ€æ›´æ–° ---")
            print("æ‰€æœ‰æ–‡ä»¶éƒ½æ˜¯æœ€æ–°çš„ï¼Œæ— éœ€æ›´æ–°é—®ç­”é“¾ã€‚")
        
        print("--- å¼‚æ­¥ä¼ä¸šçº§æ™ºèƒ½åŒæ­¥å®Œæˆ ---")

    async def _update_enterprise_document_async(self, file_path: str, all_files_by_source: Dict[str, List[str]]) -> bool:
        """
        å¼‚æ­¥æ›´æ–°ä¼ä¸šçº§æ–‡æ¡£ï¼ŒåŒ…å«åˆ†ç±»ä¿¡æ¯ã€‚
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            all_files_by_source: æŒ‰æ•°æ®æºåˆ†ç»„çš„æ–‡ä»¶åˆ—è¡¨
            
        Returns:
            æ›´æ–°æˆåŠŸè¿”å›Trueï¼Œå¦åˆ™è¿”å›False
        """
        try:
            print(f"æ­£åœ¨æ›´æ–°ä¼ä¸šçº§æ–‡æ¡£: {file_path}")
            
            # 1. åˆ é™¤æ—§ç‰ˆæœ¬
            if not await self.delete_documents_by_source_async(file_path):
                print(f"åˆ é™¤æ—§ç‰ˆæœ¬å¤±è´¥: {file_path}")
                return False
            
            # 2. è·å–æ–‡ä»¶å¯¹åº”çš„æ•°æ®æºé…ç½®
            source_config = self._get_source_config_for_file(file_path, all_files_by_source)
            
            if not source_config:
                print(f"æœªæ‰¾åˆ°æ–‡ä»¶ {file_path} å¯¹åº”çš„æ•°æ®æºé…ç½®")
                return False
            
            # 3. å¼‚æ­¥åŠ è½½æ–°ç‰ˆæœ¬
            def load_document():
                loader = TextLoader(file_path, encoding='utf-8')
                return loader.load()
            
            new_docs = await self._run_in_executor(load_document)
            
            # 4. æ·»åŠ æ–‡ä»¶ä¿¡æ¯å’Œåˆ†ç±»ä¿¡æ¯åˆ°å…ƒæ•°æ®
            file_info = await self._get_file_info_async(file_path)
            if file_info:
                for doc in new_docs:
                    doc.metadata.update({
                        'file_hash': file_info['hash'],
                        'file_mtime': file_info['mtime'],
                        'file_size': file_info['size'],
                        'category': source_config['category'],
                        'data_source': source_config.get('description', ''),
                        'priority': source_config.get('priority', 999)
                    })
            
            # 5. åˆ†å‰²æ–‡æ¡£
            chunks = await self._run_in_executor(
                self.text_splitter.split_documents, new_docs
            )
            
            # 6. ç”Ÿæˆå”¯ä¸€ID
            for i, chunk in enumerate(chunks):
                chunk_id = f"{config.DOCUMENT_ID_PREFIX}{hashlib.md5(file_path.encode()).hexdigest()}_{i}"
                chunk.metadata['chunk_id'] = chunk_id
            
            # 7. æ·»åŠ åˆ°æ•°æ®åº“
            await self._run_in_executor(
                self.vector_store.add_documents, chunks
            )
            print(f"  - å·²æ›´æ–°æ–‡æ¡£ï¼Œæ–°å¢ {len(chunks)} ä¸ªæ–‡æœ¬å—ï¼Œç±»åˆ«: {source_config['category']}")
            
            return True
            
        except Exception as e:
            print(f"æ›´æ–°ä¼ä¸šçº§æ–‡æ¡£æ—¶å‡ºé”™: {e}")
            return False

    async def _process_new_enterprise_files_async(self, new_files: List[str], all_files_by_source: Dict[str, List[str]]):
        """
        å¼‚æ­¥å¤„ç†æ–°å¢çš„ä¼ä¸šçº§æ–‡ä»¶ã€‚
        
        Args:
            new_files: æ–°å¢æ–‡ä»¶åˆ—è¡¨
            all_files_by_source: æŒ‰æ•°æ®æºåˆ†ç»„çš„æ–‡ä»¶åˆ—è¡¨
        """
        print(f"å‘ç° {len(new_files)} ä¸ªæ–°æ–‡æ¡£ï¼Œæ­£åœ¨å¤„ç†...")
        
        # æŒ‰æ•°æ®æºåˆ†ç»„å¤„ç†æ–°æ–‡ä»¶
        data_sources = self._get_enterprise_data_sources()
        files_by_category = {}
        
        for file_path in new_files:
            source_config = self._get_source_config_for_file(file_path, all_files_by_source)
            if source_config:
                category = source_config['category']
                if category not in files_by_category:
                    files_by_category[category] = []
                files_by_category[category].append((file_path, source_config))
        
        # å¹¶å‘æŒ‰ç±»åˆ«å¤„ç†æ–‡ä»¶
        async def process_category_files(category: str, file_configs: List[tuple]):
            print(f"\nå¤„ç†ç±»åˆ« '{category}' çš„æ–‡ä»¶:")
            
            async def load_single_enterprise_file(file_path: str, source_config: Dict[str, Any]):
                try:
                    def load_doc():
                        loader = TextLoader(file_path, encoding='utf-8')
                        return loader.load()
                    
                    docs = await self._run_in_executor(load_doc)
                    
                    # æ·»åŠ æ–‡ä»¶ä¿¡æ¯å’Œåˆ†ç±»ä¿¡æ¯åˆ°å…ƒæ•°æ®
                    file_info = await self._get_file_info_async(file_path)
                    if file_info:
                        for doc in docs:
                            doc.metadata.update({
                                'file_hash': file_info['hash'],
                                'file_mtime': file_info['mtime'],
                                'file_size': file_info['size'],
                                'category': source_config['category'],
                                'data_source': source_config.get('description', ''),
                                'priority': source_config.get('priority', 999)
                            })
                    
                    print(f"  âœ“ å·²åŠ è½½: {file_path} (ç±»åˆ«: {category})")
                    return docs
                except Exception as e:
                    print(f"  âœ— åŠ è½½å¤±è´¥: {file_path} - {e}")
                    return []
            
            # å¹¶å‘åŠ è½½è¯¥ç±»åˆ«çš„æ‰€æœ‰æ–‡ä»¶
            load_tasks = [load_single_enterprise_file(file_path, source_config) for file_path, source_config in file_configs]
            all_docs_lists = await asyncio.gather(*load_tasks)
            
            # åˆå¹¶è¯¥ç±»åˆ«çš„æ‰€æœ‰æ–‡æ¡£
            category_docs = []
            for docs_list in all_docs_lists:
                category_docs.extend(docs_list)
            
            return category_docs
        
        # å¹¶å‘å¤„ç†æ‰€æœ‰ç±»åˆ«
        category_tasks = [process_category_files(category, file_configs) for category, file_configs in files_by_category.items()]
        all_category_docs = await asyncio.gather(*category_tasks)
        
        # åˆå¹¶æ‰€æœ‰ç±»åˆ«çš„æ–‡æ¡£
        all_new_docs = []
        for category_docs in all_category_docs:
            all_new_docs.extend(category_docs)
        
        if all_new_docs:
            chunks = await self._run_in_executor(
                self.text_splitter.split_documents, all_new_docs
            )
            print(f"\næ–°æ–‡æ¡£è¢«åˆ†å‰²æˆ {len(chunks)} ä¸ªæ–‡æœ¬å—ã€‚")

            # ç”Ÿæˆå”¯ä¸€IDå¹¶æ·»åŠ åˆ†ç±»ä¿¡æ¯
            for chunk in chunks:
                source_path = chunk.metadata.get('source', '')
                chunk_hash = hashlib.md5(f"{source_path}_{chunk.page_content}".encode()).hexdigest()
                chunk.metadata['chunk_id'] = f"{config.DOCUMENT_ID_PREFIX}{chunk_hash}"

            # æ·»åŠ åˆ°æ•°æ®åº“
            if self.vector_store is None:
                print("æ­£åœ¨åˆ›å»ºæ–°çš„å‘é‡æ•°æ®åº“...")
                
                def create_vector_store():
                    from langchain_chroma import Chroma
                    return Chroma.from_documents(
                        documents=chunks,
                        embedding=self.embeddings,
                        persist_directory=config.VECTOR_STORE_PATH
                    )
                
                self.vector_store = await self._run_in_executor(create_vector_store)
                print(f"  - æ–°çš„å‘é‡æ•°æ®åº“å·²åˆ›å»ºäº '{config.VECTOR_STORE_PATH}'ã€‚")
            else:
                await self._run_in_executor(self.vector_store.add_documents, chunks)
                print("  - æ–°çš„æ–‡æœ¬å—å·²æˆåŠŸæ·»åŠ åˆ°ç°æœ‰æ•°æ®åº“ã€‚")
            
            # æŒ‰ç±»åˆ«ç»Ÿè®¡
            category_stats = {}
            for chunk in chunks:
                category = chunk.metadata.get('category', 'unknown')
                category_stats[category] = category_stats.get(category, 0) + 1
            
            print("æŒ‰ç±»åˆ«ç»Ÿè®¡:")
            for category, count in category_stats.items():
                print(f"  - {category}: {count} ä¸ªæ–‡æœ¬å—")

    def __del__(self):
        """ææ„å‡½æ•°ï¼Œæ¸…ç†çº¿ç¨‹æ± èµ„æºã€‚"""
        if hasattr(self, 'executor'):
            self.executor.shutdown(wait=True)

```

## `rag/config.py`

```python
# rag/config.py

from typing import Dict, Any

# --- æ¨¡å‹é…ç½® ---

# æ”¹å›Hugging Faceä¸Šçš„æ ‡å‡†æ¨¡å‹åç§°
EMBEDDING_MODEL_NAME: str = r'../local_models/bge-small-zh-v1.5'
RERANKER_MODEL_NAME: str = r'../local_models/bge-reranker-base/snapshots/2cfc18c9415c912f9d8155881c133215df768a70'

# æ¨¡å‹è¿è¡Œå‚æ•°: å¼ºåˆ¶åœ¨CPUä¸Šè¿è¡Œï¼Œå¹¶è®¾ç½®ç¼“å­˜ç›®å½•
VECTOR_STORE_PATH: str = "my_chromadb_vector_store" 
MODEL_DEVICE: str = "cpu"
EMBEDDING_MODEL_KWARGS: Dict[str, Any] = {"device": MODEL_DEVICE}
RERANKER_MODEL_KWARGS: Dict[str, Any] = {"device": MODEL_DEVICE}

# --- ä¼ä¸šçº§å¤šè·¯å¾„æ•°æ®æºé…ç½® ---

# ä¼ ç»Ÿå•ä¸€æ•°æ®è·¯å¾„ï¼ˆå‘åå…¼å®¹ï¼‰
DATA_PATH: str = "./data"

# ä¼ä¸šçº§å¤šè·¯å¾„æ•°æ®æºé…ç½®
# æ”¯æŒå¤šä¸ªç¡¬ç›˜/ç›®å½•ï¼Œæ¯ä¸ªè·¯å¾„å¯ä»¥æŒ‡å®šç±»åˆ«
ENTERPRISE_DATA_SOURCES: Dict[str, Dict[str, Any]] = {
    # ä¸»æ•°æ®ç›®å½•
    "main": {
        "path": "./data",
        "category": "general",
        "description": "é€šç”¨çŸ¥è¯†åº“",
        "enabled": True,
        "file_patterns": ["*.txt", "*.md", "*.pdf"],
        "priority": 1 # é¢„ç•™å­—æ®µï¼Œç”¨äºæœªæ¥çš„ä¼˜å…ˆçº§åŠŸèƒ½
    },
    
    # æŠ€æœ¯æ–‡æ¡£ç›®å½•
    "technical": {
        "path": "./data/technical",
        "category": "technical",
        "description": "æŠ€æœ¯æ–‡æ¡£åº“",
        "enabled": True,
        "file_patterns": ["*.txt", "*.md"],
        "priority": 2 # é¢„ç•™å­—æ®µï¼Œç”¨äºæœªæ¥çš„ä¼˜å…ˆçº§åŠŸèƒ½
    },
    
    # äº§å“æ–‡æ¡£ç›®å½•
    "product": {
        "path": "./data/product",
        "category": "product",
        "description": "äº§å“æ–‡æ¡£åº“",
        "enabled": True,
        "file_patterns": ["*.txt", "*.md"],
        "priority": 3 # é¢„ç•™å­—æ®µï¼Œç”¨äºæœªæ¥çš„ä¼˜å…ˆçº§åŠŸèƒ½
    },
    
    # å¯ä»¥æ·»åŠ æ›´å¤šæ•°æ®æºï¼Œæ¯”å¦‚ä¸åŒç¡¬ç›˜çš„è·¯å¾„
    # "disk_d": {
    #     "path": "D:/enterprise_docs",
    #     "category": "enterprise",
    #     "description": "ä¼ä¸šæ–‡æ¡£åº“(Dç›˜)",
    #     "enabled": True,
    #     "file_patterns": ["*.txt", "*.md", "*.pdf"],
    #     "priority": 4 # é¢„ç•™å­—æ®µï¼Œç”¨äºæœªæ¥çš„ä¼˜å…ˆçº§åŠŸèƒ½
    # },
    
    # "disk_e": {
    #     "path": "E:/research_docs",
    #     "category": "research",
    #     "description": "ç ”ç©¶æ–‡æ¡£åº“(Eç›˜)",
    #     "enabled": True,
    #     "file_patterns": ["*.txt", "*.md"],
    #     "priority": 5 # é¢„ç•™å­—æ®µï¼Œç”¨äºæœªæ¥çš„ä¼˜å…ˆçº§åŠŸèƒ½
    # }
}

# æ˜¯å¦å¯ç”¨ä¼ä¸šçº§å¤šè·¯å¾„æ¨¡å¼
ENABLE_ENTERPRISE_MODE: bool = True

# é»˜è®¤æ£€ç´¢çš„ç±»åˆ«ï¼ˆç©ºåˆ—è¡¨è¡¨ç¤ºæ£€ç´¢æ‰€æœ‰ç±»åˆ«ï¼‰
DEFAULT_SEARCH_CATEGORIES: list = []  # ä¾‹å¦‚: ["technical", "product"]

# ç±»åˆ«ä¼˜å…ˆçº§ï¼ˆæ•°å­—è¶Šå°ä¼˜å…ˆçº§è¶Šé«˜ï¼‰
CATEGORY_PRIORITIES: Dict[str, int] = {
    "general": 1,
    "technical": 2,
    "product": 3,
    "enterprise": 4,
    "research": 5
}


# --- æ–‡æ¡£å¤„ç†é…ç½® ---

# æ–‡æœ¬åˆ†å‰²å—å¤§å°: å°†æ–‡æ¡£åˆ†å‰²æˆçš„å°å—çš„æœ€å¤§å­—ç¬¦æ•°
CHUNK_SIZE: int = 500
# æ–‡æœ¬åˆ†å‰²å—é‡å : ç›¸é‚»å—ä¹‹é—´çš„é‡å å­—ç¬¦æ•°ï¼Œä»¥ä¿è¯è¯­ä¹‰è¿ç»­æ€§
CHUNK_OVERLAP: int = 150


# --- çŸ­æœŸè®°å¿†é…ç½® ---

# æ˜¯å¦å¯ç”¨çŸ­æœŸè®°å¿†åŠŸèƒ½
ENABLE_SHORT_TERM_MEMORY: bool = True

# çŸ­æœŸè®°å¿†æœ€å¤§å­—ç¬¦é•¿åº¦ï¼ˆ100kå­—ç¬¦ï¼‰
SHORT_TERM_MEMORY_MAX_LENGTH: int = 100_000

# å•æ¡å¯¹è¯è®°å½•çš„æœ€å¤§å­—ç¬¦é•¿åº¦ï¼ˆé˜²æ­¢å•æ¡è®°å½•è¿‡é•¿ï¼‰
# è®¾ç½®ä¸ºæ€»è®°å¿†çš„1/5ï¼Œç¡®ä¿è‡³å°‘èƒ½å®¹çº³5è½®å¯¹è¯
SINGLE_CONVERSATION_MAX_LENGTH: int = 20_000

# è®°å¿†ä¿ç•™çš„æœ€å°å¯¹è¯è½®æ•°ï¼ˆå³ä½¿è¶…è¿‡é•¿åº¦é™åˆ¶ä¹Ÿä¿ç•™æœ€è¿‘çš„Nè½®å¯¹è¯ï¼‰
MIN_CONVERSATION_ROUNDS: int = 1

# è®°å¿†æ¸…ç†ç­–ç•¥
# "auto" - è‡ªåŠ¨æ¸…ç†æœ€æ—§çš„è®°å½•
# "manual" - æ‰‹åŠ¨æ¸…ç†
# "sliding_window" - æ»‘åŠ¨çª—å£ï¼Œä¿æŒå›ºå®šæ•°é‡çš„å¯¹è¯
MEMORY_CLEANUP_STRATEGY: str = "auto"

# æ»‘åŠ¨çª—å£å¤§å°ï¼ˆå½“ç­–ç•¥ä¸ºsliding_windowæ—¶ä½¿ç”¨ï¼‰
SLIDING_WINDOW_SIZE: int = 20

# --- çƒ­é‡è½½é…ç½® ---

# æ˜¯å¦å¯ç”¨æç¤ºè¯çƒ­é‡è½½åŠŸèƒ½
ENABLE_HOT_RELOAD: bool = True

# çƒ­é‡è½½é˜²æŠ–æ—¶é—´ï¼ˆç§’ï¼‰
HOT_RELOAD_DEBOUNCE_TIME: float = 0.5

# --- æ£€ç´¢ä¸é‡æ’åºé…ç½® ---

# å‘é‡æ£€ç´¢Top K: ä»å‘é‡æ•°æ®åº“ä¸­åˆæ­¥æ£€ç´¢å‡ºçš„æœ€ç›¸ä¼¼æ–‡æ¡£æ•°é‡
RETRIEVER_TOP_K: int = 10

# å…³é”®å­—æ£€ç´¢Top K: ä»å…³é”®å­—æ£€ç´¢ä¸­è·å–çš„æ–‡æ¡£æ•°é‡
KEYWORD_RETRIEVER_TOP_K: int = 5

# æ··åˆæ£€ç´¢æƒé‡é…ç½®
VECTOR_SEARCH_WEIGHT: float = 0.7  # å‘é‡æ£€ç´¢æƒé‡
KEYWORD_SEARCH_WEIGHT: float = 0.3  # å…³é”®å­—æ£€ç´¢æƒé‡

# é‡æ’åºTop N: ç»è¿‡é‡æ’åºåï¼Œæœ€ç»ˆé€‰é€ç»™å¤§è¯­è¨€æ¨¡å‹çš„æ–‡æ¡£æ•°é‡
RERANKER_TOP_N: int = 3

# æ˜¯å¦å¯ç”¨æ··åˆæ£€ç´¢ (True: æ··åˆæ£€ç´¢, False: ä»…å‘é‡æ£€ç´¢)
ENABLE_HYBRID_SEARCH: bool = True

# --- é—®é¢˜æ”¹å†™é…ç½® ---

# æ˜¯å¦å¯ç”¨é—®é¢˜æ”¹å†™åŠŸèƒ½
ENABLE_QUERY_REWRITING: bool = True

# é—®é¢˜æ”¹å†™æ•°é‡: å°†åŸé—®é¢˜æ”¹å†™æˆå¤šå°‘ä¸ªç›¸å…³é—®é¢˜
QUERY_REWRITE_COUNT: int = 3

# é—®é¢˜æ”¹å†™æ—¶æ¯ä¸ªæ”¹å†™é—®é¢˜çš„æ£€ç´¢æ•°é‡
REWRITE_QUERY_TOP_K: int = 5

# æ˜¯å¦åœ¨æœ€ç»ˆç»“æœä¸­å»é‡ç›¸ä¼¼æ–‡æ¡£
ENABLE_DOCUMENT_DEDUPLICATION: bool = True

# --- çŸ¥è¯†åº“ç®¡ç†é…ç½® ---

# æ˜¯å¦å¯ç”¨æ™ºèƒ½æ–‡ä»¶ç›‘æ§å’Œæ›´æ–°
ENABLE_FILE_MONITORING: bool = True

# æ˜¯å¦åœ¨åŒæ­¥æ—¶è‡ªåŠ¨åˆ é™¤ä¸å­˜åœ¨çš„æ–‡ä»¶å¯¹åº”çš„æ–‡æ¡£
AUTO_DELETE_MISSING_FILES: bool = True

# æ–‡æ¡£IDå‰ç¼€ï¼Œç”¨äºæ ‡è¯†æ–‡æ¡£å—çš„æ¥æºæ–‡ä»¶
DOCUMENT_ID_PREFIX: str = "doc_"
```

## `rag/hot_reload_manager.py`

```python
# rag/hot_reload_manager.py

import os
import time
import threading
from pathlib import Path
from typing import Dict, Set, Optional, Callable
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler, FileModifiedEvent, FileCreatedEvent, FileDeletedEvent

from .prompt_manager import prompt_manager
from . import config


class PromptFileHandler(FileSystemEventHandler):
    """æç¤ºè¯æ–‡ä»¶å˜åŒ–å¤„ç†å™¨"""
    
    def __init__(self, callback: Optional[Callable[[str, str], None]] = None):
        """
        åˆå§‹åŒ–æ–‡ä»¶å¤„ç†å™¨
        
        Args:
            callback: æ–‡ä»¶å˜åŒ–æ—¶çš„å›è°ƒå‡½æ•°ï¼Œå‚æ•°ä¸º(event_type, prompt_name)
        """
        super().__init__()
        self.callback = callback
        self.last_modified: Dict[str, float] = {}
        self.debounce_time = config.HOT_RELOAD_DEBOUNCE_TIME  # é˜²æŠ–æ—¶é—´ï¼ˆç§’ï¼‰
        
    def _should_process_event(self, file_path: str) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥å¤„ç†è¯¥äº‹ä»¶ï¼ˆé˜²æŠ–å¤„ç†ï¼‰
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            æ˜¯å¦åº”è¯¥å¤„ç†
        """
        current_time = time.time()
        last_time = self.last_modified.get(file_path, 0)
        
        if current_time - last_time < self.debounce_time:
            return False
        
        self.last_modified[file_path] = current_time
        return True
    
    def _get_prompt_name(self, file_path: str) -> Optional[str]:
        """
        ä»æ–‡ä»¶è·¯å¾„è·å–æç¤ºè¯åç§°
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            æç¤ºè¯åç§°ï¼Œå¦‚æœä¸æ˜¯æç¤ºè¯æ–‡ä»¶åˆ™è¿”å›None
        """
        path = Path(file_path)
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯æç¤ºè¯æ–‡ä»¶
        if (path.suffix == '.txt' and 
            'prompts' in str(path) and 
            path.parent.name == 'prompts'):
            return path.stem
        
        return None
    
    def on_modified(self, event):
        """æ–‡ä»¶ä¿®æ”¹äº‹ä»¶å¤„ç†"""
        if event.is_directory:
            return
        
        prompt_name = self._get_prompt_name(event.src_path)
        if not prompt_name:
            return
        
        if not self._should_process_event(event.src_path):
            return
        
        try:
            print(f"ğŸ”„ æ£€æµ‹åˆ°æç¤ºè¯æ–‡ä»¶ä¿®æ”¹: {prompt_name}")
            
            # æ¸…é™¤æ‰€æœ‰ç›¸å…³ç¼“å­˜
            prompt_manager._prompt_cache.pop(prompt_name, None)
            prompt_manager._template_cache.pop(prompt_name, None)
            
            # é‡æ–°åŠ è½½æç¤ºè¯ï¼ˆè¿™ä¼šé‡æ–°å¡«å……ç¼“å­˜ï¼‰
            prompt_manager.load_prompt(prompt_name)
            print(f"âœ… è‡ªåŠ¨é‡è½½å®Œæˆ: {prompt_name}")
            
            # è°ƒç”¨å›è°ƒå‡½æ•°
            if self.callback:
                self.callback("modified", prompt_name)
                
        except Exception as e:
            print(f"âŒ è‡ªåŠ¨é‡è½½å¤±è´¥ {prompt_name}: {e}")
    
    def on_created(self, event):
        """æ–‡ä»¶åˆ›å»ºäº‹ä»¶å¤„ç†"""
        if event.is_directory:
            return
        
        prompt_name = self._get_prompt_name(event.src_path)
        if not prompt_name:
            return
        
        try:
            print(f"â• æ£€æµ‹åˆ°æ–°æç¤ºè¯æ–‡ä»¶: {prompt_name}")
            
            # åŠ è½½æ–°æç¤ºè¯
            prompt_manager.load_prompt(prompt_name)
            print(f"âœ… è‡ªåŠ¨åŠ è½½å®Œæˆ: {prompt_name}")
            
            # è°ƒç”¨å›è°ƒå‡½æ•°
            if self.callback:
                self.callback("created", prompt_name)
                
        except Exception as e:
            print(f"âŒ è‡ªåŠ¨åŠ è½½å¤±è´¥ {prompt_name}: {e}")
    
    def on_deleted(self, event):
        """æ–‡ä»¶åˆ é™¤äº‹ä»¶å¤„ç†"""
        if event.is_directory:
            return
        
        prompt_name = self._get_prompt_name(event.src_path)
        if not prompt_name:
            return
        
        try:
            print(f"ğŸ—‘ï¸ æ£€æµ‹åˆ°æç¤ºè¯æ–‡ä»¶åˆ é™¤: {prompt_name}")
            
            # ä»ç¼“å­˜ä¸­ç§»é™¤
            prompt_manager._prompt_cache.pop(prompt_name, None)
            prompt_manager._template_cache.pop(prompt_name, None)
            print(f"âœ… ç¼“å­˜æ¸…ç†å®Œæˆ: {prompt_name}")
            
            # è°ƒç”¨å›è°ƒå‡½æ•°
            if self.callback:
                self.callback("deleted", prompt_name)
                
        except Exception as e:
            print(f"âŒ ç¼“å­˜æ¸…ç†å¤±è´¥ {prompt_name}: {e}")


class HotReloadManager:
    """çƒ­é‡è½½ç®¡ç†å™¨"""
    
    def __init__(self, enable_hot_reload: bool = True):
        """
        åˆå§‹åŒ–çƒ­é‡è½½ç®¡ç†å™¨
        
        Args:
            enable_hot_reload: æ˜¯å¦å¯ç”¨çƒ­é‡è½½åŠŸèƒ½
        """
        self.enable_hot_reload = enable_hot_reload
        self.observer: Optional[Observer] = None
        self.event_handler: Optional[PromptFileHandler] = None
        self.is_running = False
        self.callbacks: Set[Callable[[str, str], None]] = set()
        
        # ç›‘æ§çš„ç›®å½•
        self.watch_directory = prompt_manager.prompts_dir
        
        if self.enable_hot_reload:
            self._setup_file_watcher()
    
    def _setup_file_watcher(self):
        """è®¾ç½®æ–‡ä»¶ç›‘æ§å™¨"""
        try:
            # ç¡®ä¿ç›‘æ§ç›®å½•å­˜åœ¨
            self.watch_directory.mkdir(exist_ok=True)
            
            # åˆ›å»ºäº‹ä»¶å¤„ç†å™¨
            self.event_handler = PromptFileHandler(callback=self._on_file_change)
            
            # åˆ›å»ºè§‚å¯Ÿè€…
            self.observer = Observer()
            self.observer.schedule(
                self.event_handler,
                str(self.watch_directory),
                recursive=False
            )
            
            print(f"ğŸ” çƒ­é‡è½½ç›‘æ§å·²è®¾ç½®ï¼Œç›‘æ§ç›®å½•: {self.watch_directory}")
            
        except Exception as e:
            print(f"âŒ è®¾ç½®æ–‡ä»¶ç›‘æ§å™¨å¤±è´¥: {e}")
            self.enable_hot_reload = False
    
    def _on_file_change(self, event_type: str, prompt_name: str):
        """æ–‡ä»¶å˜åŒ–å›è°ƒå¤„ç†"""
        # é€šçŸ¥æ‰€æœ‰æ³¨å†Œçš„å›è°ƒå‡½æ•°
        for callback in self.callbacks:
            try:
                callback(event_type, prompt_name)
            except Exception as e:
                print(f"âŒ å›è°ƒå‡½æ•°æ‰§è¡Œå¤±è´¥: {e}")
    
    def start(self):
        """å¯åŠ¨çƒ­é‡è½½ç›‘æ§"""
        if not self.enable_hot_reload:
            print("âš ï¸ çƒ­é‡è½½åŠŸèƒ½æœªå¯ç”¨")
            return False
        
        if self.is_running:
            print("âš ï¸ çƒ­é‡è½½ç›‘æ§å·²åœ¨è¿è¡Œä¸­")
            return True
        
        # å¦‚æœobserverå·²ç»åœæ­¢ï¼Œéœ€è¦é‡æ–°åˆ›å»º
        if self.observer and not self.observer.is_alive():
            self._setup_file_watcher()
        
        if not self.observer:
            print("âŒ æ–‡ä»¶ç›‘æ§å™¨åˆå§‹åŒ–å¤±è´¥")
            return False
        
        try:
            self.observer.start()
            self.is_running = True
            print(f"ğŸ”¥ çƒ­é‡è½½ç›‘æ§å·²å¯åŠ¨ï¼Œæ­£åœ¨ç›‘æ§: {self.watch_directory}")
            return True
            
        except Exception as e:
            print(f"âŒ å¯åŠ¨çƒ­é‡è½½ç›‘æ§å¤±è´¥: {e}")
            # å°è¯•é‡æ–°åˆ›å»ºobserver
            self._setup_file_watcher()
            if self.observer:
                try:
                    self.observer.start()
                    self.is_running = True
                    print(f"ğŸ”¥ çƒ­é‡è½½ç›‘æ§å·²é‡æ–°å¯åŠ¨ï¼Œæ­£åœ¨ç›‘æ§: {self.watch_directory}")
                    return True
                except Exception as e2:
                    print(f"âŒ é‡æ–°å¯åŠ¨ä¹Ÿå¤±è´¥: {e2}")
            return False
    
    def stop(self):
        """åœæ­¢çƒ­é‡è½½ç›‘æ§"""
        if not self.observer or not self.is_running:
            return
        
        try:
            self.observer.stop()
            self.observer.join(timeout=5)  # ç­‰å¾…æœ€å¤š5ç§’
            self.is_running = False
            print("ğŸ›‘ çƒ­é‡è½½ç›‘æ§å·²åœæ­¢")
            
        except Exception as e:
            print(f"âŒ åœæ­¢çƒ­é‡è½½ç›‘æ§å¤±è´¥: {e}")
    
    def add_callback(self, callback: Callable[[str, str], None]):
        """
        æ·»åŠ æ–‡ä»¶å˜åŒ–å›è°ƒå‡½æ•°
        
        Args:
            callback: å›è°ƒå‡½æ•°ï¼Œå‚æ•°ä¸º(event_type, prompt_name)
        """
        self.callbacks.add(callback)
        print(f"ğŸ“ å·²æ·»åŠ çƒ­é‡è½½å›è°ƒå‡½æ•°")
    
    def remove_callback(self, callback: Callable[[str, str], None]):
        """
        ç§»é™¤æ–‡ä»¶å˜åŒ–å›è°ƒå‡½æ•°
        
        Args:
            callback: è¦ç§»é™¤çš„å›è°ƒå‡½æ•°
        """
        self.callbacks.discard(callback)
        print(f"ğŸ—‘ï¸ å·²ç§»é™¤çƒ­é‡è½½å›è°ƒå‡½æ•°")
    
    def get_status(self) -> Dict[str, any]:
        """
        è·å–çƒ­é‡è½½çŠ¶æ€ä¿¡æ¯
        
        Returns:
            çŠ¶æ€ä¿¡æ¯å­—å…¸
        """
        return {
            "enabled": self.enable_hot_reload,
            "running": self.is_running,
            "watch_directory": str(self.watch_directory),
            "callbacks_count": len(self.callbacks),
            "observer_alive": self.observer.is_alive() if self.observer else False
        }
    
    def __enter__(self):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        self.start()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        self.stop()


# æ£€æŸ¥æ˜¯å¦å®‰è£…äº†watchdogåº“
try:
    import watchdog
    WATCHDOG_AVAILABLE = True
except ImportError:
    WATCHDOG_AVAILABLE = False
    print("âš ï¸ æœªå®‰è£…watchdogåº“ï¼Œçƒ­é‡è½½åŠŸèƒ½ä¸å¯ç”¨")
    print("   å®‰è£…å‘½ä»¤: uv add watchdog")


# åˆ›å»ºå…¨å±€çƒ­é‡è½½ç®¡ç†å™¨å®ä¾‹
hot_reload_manager = HotReloadManager(
    enable_hot_reload=WATCHDOG_AVAILABLE and getattr(config, 'ENABLE_HOT_RELOAD', True)
) if WATCHDOG_AVAILABLE else None


def enable_hot_reload():
    """å¯ç”¨çƒ­é‡è½½åŠŸèƒ½"""
    if not WATCHDOG_AVAILABLE:
        print("âŒ watchdogåº“æœªå®‰è£…ï¼Œæ— æ³•å¯ç”¨çƒ­é‡è½½åŠŸèƒ½")
        print("   å®‰è£…å‘½ä»¤: uv add watchdog")
        return False
    
    if hot_reload_manager:
        return hot_reload_manager.start()
    return False


def disable_hot_reload():
    """ç¦ç”¨çƒ­é‡è½½åŠŸèƒ½"""
    if hot_reload_manager:
        hot_reload_manager.stop()


def is_hot_reload_enabled() -> bool:
    """æ£€æŸ¥çƒ­é‡è½½æ˜¯å¦å¯ç”¨"""
    return (hot_reload_manager is not None and 
            hot_reload_manager.is_running if hot_reload_manager else False)


def get_hot_reload_status() -> Dict[str, any]:
    """è·å–çƒ­é‡è½½çŠ¶æ€"""
    if hot_reload_manager:
        return hot_reload_manager.get_status()
    else:
        return {
            "enabled": False,
            "running": False,
            "error": "watchdogåº“æœªå®‰è£…" if not WATCHDOG_AVAILABLE else "çƒ­é‡è½½ç®¡ç†å™¨æœªåˆå§‹åŒ–"
        }
```

## `rag/memory_manager.py`

```python
# rag/memory_manager.py

import time
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime
import json

from . import config


@dataclass
class ConversationTurn:
    """å•è½®å¯¹è¯è®°å½•"""
    question: str
    answer: str
    timestamp: float
    metadata: Optional[Dict[str, Any]] = None
    
    def __post_init__(self):
        """åˆå§‹åŒ–åå¤„ç†"""
        if self.metadata is None:
            self.metadata = {}
        
        # è®¡ç®—å­—ç¬¦é•¿åº¦
        self.char_length = len(self.question) + len(self.answer)
        
        # æˆªæ–­è¿‡é•¿çš„å†…å®¹
        if self.char_length > config.SINGLE_CONVERSATION_MAX_LENGTH:
            max_q_len = config.SINGLE_CONVERSATION_MAX_LENGTH // 2
            max_a_len = config.SINGLE_CONVERSATION_MAX_LENGTH - max_q_len
            
            if len(self.question) > max_q_len:
                self.question = self.question[:max_q_len-3] + "..."
            
            if len(self.answer) > max_a_len:
                self.answer = self.answer[:max_a_len-3] + "..."
            
            # é‡æ–°è®¡ç®—é•¿åº¦
            self.char_length = len(self.question) + len(self.answer)
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return asdict(self)
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'ConversationTurn':
        """ä»å­—å…¸åˆ›å»ºå¯¹è±¡"""
        return cls(**data)
    
    def get_formatted_time(self) -> str:
        """è·å–æ ¼å¼åŒ–çš„æ—¶é—´å­—ç¬¦ä¸²"""
        return datetime.fromtimestamp(self.timestamp).strftime("%Y-%m-%d %H:%M:%S")


class ShortTermMemoryManager:
    """çŸ­æœŸè®°å¿†ç®¡ç†å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–è®°å¿†ç®¡ç†å™¨"""
        self.conversations: List[ConversationTurn] = []
        self.total_char_length = 0
        self.max_length = config.SHORT_TERM_MEMORY_MAX_LENGTH
        self.min_rounds = config.MIN_CONVERSATION_ROUNDS
        self.cleanup_strategy = config.MEMORY_CLEANUP_STRATEGY
        self.sliding_window_size = config.SLIDING_WINDOW_SIZE
        
        print(f"çŸ­æœŸè®°å¿†ç®¡ç†å™¨å·²åˆå§‹åŒ– (æœ€å¤§é•¿åº¦: {self.max_length:,} å­—ç¬¦)")
    
    def add_conversation(self, question: str, answer: str, metadata: Optional[Dict[str, Any]] = None) -> None:
        """
        æ·»åŠ ä¸€è½®å¯¹è¯åˆ°è®°å¿†ä¸­
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            answer: AIå›ç­”
            metadata: é¢å¤–çš„å…ƒæ•°æ®
        """
        if not config.ENABLE_SHORT_TERM_MEMORY:
            return
        
        # åˆ›å»ºå¯¹è¯è®°å½•
        conversation = ConversationTurn(
            question=question,
            answer=answer,
            timestamp=time.time(),
            metadata=metadata or {}
        )
        
        # æ·»åŠ åˆ°åˆ—è¡¨
        self.conversations.append(conversation)
        self.total_char_length += conversation.char_length
        
        print(f"ğŸ“ æ·»åŠ å¯¹è¯è®°å½• (é•¿åº¦: {conversation.char_length} å­—ç¬¦, æ€»é•¿åº¦: {self.total_char_length:,} å­—ç¬¦)")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ¸…ç†
        self._cleanup_if_needed()
    
    def _cleanup_if_needed(self) -> None:
        """æ ¹æ®ç­–ç•¥æ¸…ç†è®°å¿†"""
        if self.total_char_length <= self.max_length:
            return
        
        if self.cleanup_strategy == "auto":
            self._auto_cleanup()
        elif self.cleanup_strategy == "sliding_window":
            self._sliding_window_cleanup()
        # manualç­–ç•¥ä¸è‡ªåŠ¨æ¸…ç†
    
    def _auto_cleanup(self) -> None:
        """
        è‡ªåŠ¨æ¸…ç†ç­–ç•¥ï¼šä¸¥æ ¼æ§åˆ¶æ€»é•¿åº¦ä¸è¶…è¿‡max_length
        ä¼˜å…ˆçº§ï¼šé•¿åº¦é™åˆ¶ > è½®æ•°ä¿ç•™
        å¦‚æœå•è½®å¯¹è¯è¶…é•¿ï¼Œä¼šæˆªå–è¯¥è½®å¯¹è¯çš„å†…å®¹
        """
        removed_count = 0
        truncated_count = 0
        
        # ç¬¬ä¸€é˜¶æ®µï¼šç§»é™¤æ•´è½®å¯¹è¯ç›´åˆ°æ»¡è¶³é•¿åº¦è¦æ±‚æˆ–åªå‰©ä¸€è½®
        while (self.total_char_length > self.max_length and len(self.conversations) > 1):
            removed_conversation = self.conversations.pop(0)
            self.total_char_length -= removed_conversation.char_length
            removed_count += 1
        
        # ç¬¬äºŒé˜¶æ®µï¼šå¦‚æœè¿˜æ˜¯è¶…é•¿ä¸”åªå‰©ä¸€è½®å¯¹è¯ï¼Œæˆªå–è¯¥è½®å¯¹è¯
        if self.total_char_length > self.max_length and len(self.conversations) == 1:
            last_conversation = self.conversations[0]
            
            # è®¡ç®—éœ€è¦æˆªå–å¤šå°‘å­—ç¬¦
            excess_chars = self.total_char_length - self.max_length
            target_length = last_conversation.char_length - excess_chars
            
            if target_length > 0:
                # æŒ‰æ¯”ä¾‹æˆªå–é—®é¢˜å’Œç­”æ¡ˆ
                total_original_length = len(last_conversation.question) + len(last_conversation.answer)
                question_ratio = len(last_conversation.question) / total_original_length
                answer_ratio = len(last_conversation.answer) / total_original_length
                
                target_question_length = int(target_length * question_ratio)
                target_answer_length = target_length - target_question_length
                
                # æˆªå–é—®é¢˜å’Œç­”æ¡ˆ
                if target_question_length > 3:  # ä¿ç•™è‡³å°‘3ä¸ªå­—ç¬¦ç”¨äº"..."
                    truncated_question = last_conversation.question[:target_question_length-3] + "..."
                else:
                    truncated_question = "..."
                
                if target_answer_length > 3:  # ä¿ç•™è‡³å°‘3ä¸ªå­—ç¬¦ç”¨äº"..."
                    truncated_answer = last_conversation.answer[:target_answer_length-3] + "..."
                else:
                    truncated_answer = "..."
                
                # æ›´æ–°å¯¹è¯å†…å®¹
                old_length = last_conversation.char_length
                last_conversation.question = truncated_question
                last_conversation.answer = truncated_answer
                last_conversation.char_length = len(truncated_question) + len(truncated_answer)
                
                # æ›´æ–°æ€»é•¿åº¦
                self.total_char_length = self.total_char_length - old_length + last_conversation.char_length
                truncated_count = 1
                
                print(f"âš ï¸  æœ€åä¸€è½®å¯¹è¯è¿‡é•¿ï¼Œå·²æˆªå– {old_length - last_conversation.char_length} å­—ç¬¦")
            else:
                # å¦‚æœç›®æ ‡é•¿åº¦å¤ªå°ï¼Œç›´æ¥æ¸…ç©ºè¯¥è½®å¯¹è¯
                self.conversations.clear()
                self.total_char_length = 0
                removed_count += 1
                print(f"âš ï¸  å•è½®å¯¹è¯è¶…å‡ºé™åˆ¶å¤ªå¤šï¼Œå·²æ¸…ç©ºæ‰€æœ‰è®°å¿†")
        
        # ç¬¬ä¸‰é˜¶æ®µï¼šå¦‚æœè¿˜æœ‰å¤šè½®å¯¹è¯ä½†ä»è¶…é•¿ï¼Œç»§ç»­ç§»é™¤ï¼ˆç†è®ºä¸Šä¸åº”è¯¥å‘ç”Ÿï¼‰
        while self.total_char_length > self.max_length and len(self.conversations) > 0:
            removed_conversation = self.conversations.pop(0)
            self.total_char_length -= removed_conversation.char_length
            removed_count += 1
        
        # è¾“å‡ºæ¸…ç†ç»“æœ
        if removed_count > 0 or truncated_count > 0:
            messages = []
            if removed_count > 0:
                messages.append(f"ç§»é™¤äº† {removed_count} è½®æ—§å¯¹è¯")
            if truncated_count > 0:
                messages.append(f"æˆªå–äº† {truncated_count} è½®å¯¹è¯å†…å®¹")
            
            print(f"ğŸ§¹ è‡ªåŠ¨æ¸…ç†å®Œæˆï¼š{', '.join(messages)} (å½“å‰æ€»é•¿åº¦: {self.total_char_length:,} å­—ç¬¦)")
        
        # æœ€ç»ˆéªŒè¯ï¼šç¡®ä¿ç»å¯¹ä¸è¶…è¿‡é™åˆ¶
        if self.total_char_length > self.max_length:
            print(f"âŒ è­¦å‘Šï¼šæ¸…ç†åä»è¶…å‡ºé™åˆ¶ ({self.total_char_length:,} > {self.max_length:,})")
            # ç´§æ€¥å¤„ç†ï¼šç›´æ¥æ¸…ç©º
            self.conversations.clear()
            self.total_char_length = 0
            print(f"ğŸš¨ ç´§æ€¥æ¸…ç©ºæ‰€æœ‰è®°å¿†ä»¥é¿å…è¶…å‡ºé™åˆ¶")
    
    def _sliding_window_cleanup(self) -> None:
        """æ»‘åŠ¨çª—å£æ¸…ç†ç­–ç•¥ï¼šä¿æŒå›ºå®šæ•°é‡çš„å¯¹è¯"""
        if len(self.conversations) <= self.sliding_window_size:
            return
        
        # è®¡ç®—éœ€è¦ç§»é™¤çš„å¯¹è¯æ•°é‡
        excess_count = len(self.conversations) - self.sliding_window_size
        
        # ç§»é™¤æœ€æ—§çš„å¯¹è¯
        for _ in range(excess_count):
            removed_conversation = self.conversations.pop(0)
            self.total_char_length -= removed_conversation.char_length
        
        print(f"ğŸªŸ æ»‘åŠ¨çª—å£æ¸…ç†äº† {excess_count} è½®æ—§å¯¹è¯ (ä¿ç•™æœ€è¿‘ {self.sliding_window_size} è½®)")
    
    def get_recent_conversations(self, count: Optional[int] = None) -> List[ConversationTurn]:
        """
        è·å–æœ€è¿‘çš„å¯¹è¯è®°å½•
        
        Args:
            count: è·å–çš„å¯¹è¯è½®æ•°ï¼ŒNoneè¡¨ç¤ºè·å–æ‰€æœ‰
            
        Returns:
            å¯¹è¯è®°å½•åˆ—è¡¨
        """
        if count is None:
            return self.conversations.copy()
        
        return self.conversations[-count:] if count > 0 else []
    
    def get_conversation_context(self, include_count: Optional[int] = None) -> str:
        """
        è·å–å¯¹è¯ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²ï¼Œç”¨äºæä¾›ç»™LLM
        
        Args:
            include_count: åŒ…å«çš„å¯¹è¯è½®æ•°ï¼ŒNoneè¡¨ç¤ºåŒ…å«æ‰€æœ‰
            
        Returns:
            æ ¼å¼åŒ–çš„å¯¹è¯ä¸Šä¸‹æ–‡
        """
        conversations = self.get_recent_conversations(include_count)
        
        if not conversations:
            return ""
        
        context_parts = []
        for i, conv in enumerate(conversations, 1):
            context_parts.append(f"ç¬¬{i}è½®å¯¹è¯:")
            context_parts.append(f"ç”¨æˆ·: {conv.question}")
            context_parts.append(f"åŠ©æ‰‹: {conv.answer}")
            context_parts.append("")  # ç©ºè¡Œåˆ†éš”
        
        return "\n".join(context_parts).strip()
    
    def get_memory_stats(self) -> Dict[str, Any]:
        """
        è·å–è®°å¿†ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        if not self.conversations:
            return {
                "total_conversations": 0,
                "total_char_length": 0,
                "memory_usage_percent": 0.0,
                "oldest_conversation": None,
                "newest_conversation": None,
                "average_conversation_length": 0
            }
        
        return {
            "total_conversations": len(self.conversations),
            "total_char_length": self.total_char_length,
            "memory_usage_percent": (self.total_char_length / self.max_length) * 100,
            "oldest_conversation": self.conversations[0].get_formatted_time(),
            "newest_conversation": self.conversations[-1].get_formatted_time(),
            "average_conversation_length": self.total_char_length // len(self.conversations)
        }
    
    def clear_memory(self) -> int:
        """
        æ¸…ç©ºæ‰€æœ‰è®°å¿†
        
        Returns:
            æ¸…é™¤çš„å¯¹è¯è½®æ•°
        """
        cleared_count = len(self.conversations)
        self.conversations.clear()
        self.total_char_length = 0
        
        print(f"ğŸ—‘ï¸ å·²æ¸…ç©ºæ‰€æœ‰è®°å¿† (æ¸…é™¤äº† {cleared_count} è½®å¯¹è¯)")
        return cleared_count
    
    def remove_old_conversations(self, keep_count: int) -> int:
        """
        æ‰‹åŠ¨ç§»é™¤æ—§å¯¹è¯ï¼Œä¿ç•™æŒ‡å®šæ•°é‡çš„æœ€æ–°å¯¹è¯
        
        Args:
            keep_count: ä¿ç•™çš„å¯¹è¯è½®æ•°
            
        Returns:
            ç§»é™¤çš„å¯¹è¯è½®æ•°
        """
        if keep_count >= len(self.conversations):
            return 0
        
        # è®¡ç®—éœ€è¦ç§»é™¤çš„æ•°é‡
        remove_count = len(self.conversations) - keep_count
        
        # ç§»é™¤æœ€æ—§çš„å¯¹è¯
        removed_conversations = self.conversations[:remove_count]
        self.conversations = self.conversations[remove_count:]
        
        # æ›´æ–°æ€»é•¿åº¦
        removed_length = sum(conv.char_length for conv in removed_conversations)
        self.total_char_length -= removed_length
        
        print(f"ğŸ§¹ æ‰‹åŠ¨ç§»é™¤äº† {remove_count} è½®æ—§å¯¹è¯ (å½“å‰æ€»é•¿åº¦: {self.total_char_length:,} å­—ç¬¦)")
        return remove_count
    
    def search_conversations(self, keyword: str, limit: int = 10) -> List[Tuple[int, ConversationTurn]]:
        """
        åœ¨å¯¹è¯å†å²ä¸­æœç´¢å…³é”®è¯
        
        Args:
            keyword: æœç´¢å…³é”®è¯
            limit: è¿”å›ç»“æœæ•°é‡é™åˆ¶
            
        Returns:
            åŒ¹é…çš„å¯¹è¯è®°å½•åˆ—è¡¨ï¼ŒåŒ…å«ç´¢å¼•å’Œå¯¹è¯å¯¹è±¡
        """
        results = []
        keyword_lower = keyword.lower()
        
        for i, conv in enumerate(self.conversations):
            if (keyword_lower in conv.question.lower() or 
                keyword_lower in conv.answer.lower()):
                results.append((i, conv))
                
                if len(results) >= limit:
                    break
        
        return results
    
    def export_conversations(self, file_path: str) -> bool:
        """
        å¯¼å‡ºå¯¹è¯è®°å½•åˆ°JSONæ–‡ä»¶
        
        Args:
            file_path: å¯¼å‡ºæ–‡ä»¶è·¯å¾„
            
        Returns:
            å¯¼å‡ºæ˜¯å¦æˆåŠŸ
        """
        try:
            export_data = {
                "export_time": datetime.now().isoformat(),
                "total_conversations": len(self.conversations),
                "total_char_length": self.total_char_length,
                "conversations": [conv.to_dict() for conv in self.conversations]
            }
            
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ“¤ å¯¹è¯è®°å½•å·²å¯¼å‡ºåˆ°: {file_path}")
            return True
            
        except Exception as e:
            print(f"âŒ å¯¼å‡ºå¯¹è¯è®°å½•å¤±è´¥: {e}")
            return False
    
    def import_conversations(self, file_path: str, append: bool = False) -> bool:
        """
        ä»JSONæ–‡ä»¶å¯¼å…¥å¯¹è¯è®°å½•
        
        Args:
            file_path: å¯¼å…¥æ–‡ä»¶è·¯å¾„
            append: æ˜¯å¦è¿½åŠ åˆ°ç°æœ‰è®°å½•ï¼ˆFalseè¡¨ç¤ºæ›¿æ¢ï¼‰
            
        Returns:
            å¯¼å…¥æ˜¯å¦æˆåŠŸ
        """
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                import_data = json.load(f)
            
            imported_conversations = [
                ConversationTurn.from_dict(conv_data) 
                for conv_data in import_data['conversations']
            ]
            
            if not append:
                self.clear_memory()
            
            # æ·»åŠ å¯¼å…¥çš„å¯¹è¯
            for conv in imported_conversations:
                self.conversations.append(conv)
                self.total_char_length += conv.char_length
            
            # æ¸…ç†å¦‚æœéœ€è¦
            self._cleanup_if_needed()
            
            print(f"ğŸ“¥ å·²å¯¼å…¥ {len(imported_conversations)} è½®å¯¹è¯è®°å½•")
            return True
            
        except Exception as e:
            print(f"âŒ å¯¼å…¥å¯¹è¯è®°å½•å¤±è´¥: {e}")
            return False


# åˆ›å»ºå…¨å±€è®°å¿†ç®¡ç†å™¨å®ä¾‹
memory_manager = ShortTermMemoryManager()
```

## `rag/pipeline.py`

```python
# rag/pipeline.py

import os
import hashlib
import time
import glob
from pathlib import Path
from typing import List, Dict, Any, Set, Optional

# ä» .env æ–‡ä»¶åŠ è½½ç¯å¢ƒå˜é‡ï¼Œå¿…é¡»åœ¨è®¿é—® os.getenv ä¹‹å‰è°ƒç”¨
from dotenv import load_dotenv
load_dotenv()

# LangChain æ ¸å¿ƒç»„ä»¶
from langchain_core.documents import Document
from langchain_core.prompts import PromptTemplate

# æ–‡æ¡£åŠ è½½å™¨å’Œåˆ†å‰²å™¨
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import DirectoryLoader, TextLoader

# å‘é‡å­˜å‚¨ä¸åµŒå…¥
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
# LLM ä¸ RAG é“¾
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA

# é‡æ’åºç›¸å…³ç»„ä»¶
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CrossEncoderReranker
from langchain_community.cross_encoders import HuggingFaceCrossEncoder # <-- å¯¼å…¥è¿™ä¸ªæ–°ç±»

# æ··åˆæ£€ç´¢ç›¸å…³ç»„ä»¶
from langchain.retrievers import EnsembleRetriever
from langchain_community.retrievers import BM25Retriever

# æŠ‘åˆ¶ jieba çš„ pkg_resources å¼ƒç”¨è­¦å‘Š
import warnings
warnings.filterwarnings("ignore", message="pkg_resources is deprecated", category=UserWarning)
import jieba  # ä¸­æ–‡åˆ†è¯åº“

# å¯¼å…¥é¡¹ç›®é…ç½®
from . import config
# å¯¼å…¥æç¤ºè¯ç®¡ç†å™¨
from .prompt_manager import get_qa_prompt_template, get_query_rewrite_prompt_template
# å¯¼å…¥çŸ­æœŸè®°å¿†ç®¡ç†å™¨
from .memory_manager import memory_manager


class RagPipeline:
    """
    ä¸€ä¸ªå°è£…äº†å®Œæ•´RAGæµç¨‹çš„ç±» (ç‰ˆæœ¬ 3.1 - ä¿®æ­£ç‰ˆ)ã€‚
    ç‰¹æ€§:
    - ä»æœ¬åœ°åŠ è½½åµŒå…¥å’Œé‡æ’åºæ¨¡å‹ã€‚
    - ä½¿ç”¨ChromaDBè¿›è¡ŒæŒä¹…åŒ–å‘é‡å­˜å‚¨ã€‚
    - æ™ºèƒ½åŒæ­¥æ•°æ®ç›®å½•ï¼Œè‡ªåŠ¨åŠ è½½æ–°æ–‡ä»¶ã€‚
    """
    def __init__(self):
        """åˆå§‹åŒ–RAGæµç¨‹æ‰€éœ€çš„æ‰€æœ‰ç»„ä»¶ã€‚"""
        print("æ­£åœ¨åˆå§‹åŒ– RAG Pipeline...")
        self._setup_models()
        self.vector_store = self._load_vector_store()
        self.all_documents = []  # å­˜å‚¨æ‰€æœ‰æ–‡æ¡£ï¼Œç”¨äºå…³é”®å­—æ£€ç´¢
        self.bm25_retriever = None  # BM25æ£€ç´¢å™¨
        
        # === ã€å·²ä¿®æ­£ã€‘å…³é”®æ”¹åŠ¨ï¼šåªæœ‰åœ¨æˆåŠŸåŠ è½½æ•°æ®åº“åæ‰æ„å»ºé—®ç­”é“¾ ===
        if self.vector_store:
            print("å·²æˆåŠŸåŠ è½½ç°æœ‰æ•°æ®åº“ï¼Œæ­£åœ¨æ„å»ºé—®ç­”é“¾...")
            self._load_all_documents()  # åŠ è½½æ‰€æœ‰æ–‡æ¡£ç”¨äºå…³é”®å­—æ£€ç´¢
            self._build_qa_chain()
        else:
            print("æœªå‘ç°ç°æœ‰æ•°æ®åº“ã€‚é—®ç­”é“¾å°†åœ¨æ•°æ®åŒæ­¥åæ„å»ºã€‚")
            
        print("RAG Pipeline åˆå§‹åŒ–å®Œæˆã€‚")

    def _setup_models(self):
        """ç§æœ‰æ–¹æ³•ï¼Œç”¨äºè®¾ç½®æ‰€æœ‰æ¨¡å‹å’Œåˆ†å‰²å™¨ã€‚"""
        print(f"  - åŠ è½½åµŒå…¥æ¨¡å‹: {config.EMBEDDING_MODEL_NAME}")
        self.embeddings = HuggingFaceEmbeddings(
            model_name=config.EMBEDDING_MODEL_NAME,
            model_kwargs=config.EMBEDDING_MODEL_KWARGS
        )
        print(f"  - åŠ è½½é‡æ’åºæ¨¡å‹: {config.RERANKER_MODEL_NAME}")
        reranker_model = HuggingFaceCrossEncoder(
            model_name=config.RERANKER_MODEL_NAME,
            model_kwargs=config.RERANKER_MODEL_KWARGS
        )
        self.reranker = CrossEncoderReranker(model=reranker_model, top_n=config.RERANKER_TOP_N)
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=config.CHUNK_SIZE,
            chunk_overlap=config.CHUNK_OVERLAP
        )
        self._setup_llm()
        self.qa_chain = None

    def _setup_llm(self):
        """åŠ è½½å¤§è¯­è¨€æ¨¡å‹é…ç½®ã€‚"""
        api_key = os.getenv("CLOUD_INFINI_API_KEY")
        base_url = os.getenv("CLOUD_BASE_URL")
        model_name = os.getenv("CLOUD_MODEL_NAME")
        # api_key = os.getenv("DeepSeek_api_key")
        # base_url = os.getenv("DeepSeek_base_url")
        # model_name = os.getenv("DeepSeek_model_name")
        if not all([api_key, base_url, model_name]):
            raise ValueError(
                "APIå¯†é’¥æˆ–æ¨¡å‹é…ç½®æœªæ‰¾åˆ°ã€‚è¯·æ£€æŸ¥æ‚¨çš„ .env æ–‡ä»¶æ˜¯å¦åŒ…å« "
                "LLM_API_KEY, LLM_BASE_URL, å’Œ LLM_MODEL_NAMEã€‚"
            )
        
        print(f"  - é…ç½®å¤§è¯­è¨€æ¨¡å‹: {model_name}")
        self.llm = ChatOpenAI(
            model=model_name,  # æ¨¡å‹åç§°
            openai_api_key=api_key,  # åœ¨å¹³å°æ³¨å†Œè´¦å·åè·å–
            openai_api_base=base_url,  # å¹³å° API åœ°å€
            temperature=0,
            seed=42
            )

    def _load_vector_store(self) -> Chroma:
        """åŠ è½½å‘é‡æ•°æ®åº“ã€‚å¦‚æœä¸å­˜åœ¨ï¼Œåˆ™è¿”å›Noneã€‚"""
        persist_directory = config.VECTOR_STORE_PATH
        if os.path.exists(persist_directory) and os.listdir(persist_directory):
            print(f"å‘ç°å·²å­˜åœ¨çš„å‘é‡æ•°æ®åº“ï¼Œæ­£åœ¨ä» '{persist_directory}' åŠ è½½...")
            return Chroma(
                persist_directory=persist_directory,
                embedding_function=self.embeddings
            )
        return None

    def _get_processed_sources(self) -> Set[str]:
        """
        ä»å‘é‡æ•°æ®åº“ä¸­è·å–æ‰€æœ‰å·²å¤„ç†è¿‡çš„æ–‡æ¡£æºè·¯å¾„ã€‚

        è¿™æ˜¯å®ç°å¢é‡æ›´æ–°çš„å…³é”®ï¼Œé€šè¿‡å®ƒæˆ‘ä»¬å¯ä»¥çŸ¥é“å“ªäº›æ–‡ä»¶å·²ç»æ˜¯â€œæ—§â€æ–‡ä»¶ã€‚
        
        Returns:
            ä¸€ä¸ªåŒ…å«æ‰€æœ‰å”¯ä¸€æºæ–‡ä»¶è·¯å¾„çš„é›†åˆ(Set)ã€‚
        """
        if not self.vector_store:
            return set()
        
        try:
            # ChromaDBçš„ .get() æ–¹æ³•å¯ä»¥è·å–æ•°æ®åº“ä¸­çš„æ¡ç›®ã€‚
            # æˆ‘ä»¬åªéœ€è¦å…ƒæ•°æ®(metadatas)éƒ¨åˆ†ã€‚
            all_entries = self.vector_store.get(include=["metadatas"])
            
            # ä½¿ç”¨é›†åˆæ¨å¯¼å¼é«˜æ•ˆåœ°æå–æ‰€æœ‰'source'å…ƒæ•°æ®
            # 'source'æ˜¯åœ¨åŠ è½½æ–‡æ¡£æ—¶ç”±DirectoryLoaderè‡ªåŠ¨æ·»åŠ çš„å…ƒæ•°æ®ï¼Œå€¼ä¸ºæ–‡ä»¶è·¯å¾„ã€‚
            sources = {
                metadata['source'] 
                for metadata in all_entries['metadatas'] 
                if metadata and 'source' in metadata
            }
            return sources
        except Exception as e:
            # å¢åŠ é”™è¯¯å¤„ç†ï¼Œæé«˜ä»£ç å¥å£®æ€§
            print(f"ä»æ•°æ®åº“è·å–æºæ–‡ä»¶åˆ—è¡¨æ—¶å‡ºé”™: {e}")
            return set()

    def _get_file_info(self, file_path: str) -> Dict[str, Any]:
        """
        è·å–æ–‡ä»¶çš„è¯¦ç»†ä¿¡æ¯ï¼ŒåŒ…æ‹¬ä¿®æ”¹æ—¶é—´å’Œå†…å®¹å“ˆå¸Œã€‚
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            åŒ…å«æ–‡ä»¶ä¿¡æ¯çš„å­—å…¸
        """
        try:
            stat = os.stat(file_path)
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            return {
                'path': file_path,
                'mtime': stat.st_mtime,
                'size': stat.st_size,
                'hash': hashlib.md5(content.encode('utf-8')).hexdigest()
            }
        except Exception as e:
            print(f"è·å–æ–‡ä»¶ä¿¡æ¯å¤±è´¥ {file_path}: {e}")
            return None

    def _get_file_metadata_from_db(self, file_path: str) -> Optional[Dict[str, Any]]:
        """
        ä»æ•°æ®åº“ä¸­è·å–æ–‡ä»¶çš„å…ƒæ•°æ®ä¿¡æ¯ã€‚
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            æ–‡ä»¶çš„å…ƒæ•°æ®ä¿¡æ¯ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å›None
        """
        if not self.vector_store:
            return None
        
        try:
            # è·å–è¯¥æ–‡ä»¶çš„æ‰€æœ‰æ–‡æ¡£å—
            all_entries = self.vector_store.get(
                where={"source": file_path},
                include=["metadatas"]
            )
            
            if all_entries['metadatas']:
                # è¿”å›ç¬¬ä¸€ä¸ªæ–‡æ¡£å—çš„å…ƒæ•°æ®ï¼ˆæ‰€æœ‰å—çš„æ–‡ä»¶ä¿¡æ¯åº”è¯¥ç›¸åŒï¼‰
                return all_entries['metadatas'][0]
            
            return None
        except Exception as e:
            print(f"ä»æ•°æ®åº“è·å–æ–‡ä»¶å…ƒæ•°æ®å¤±è´¥ {file_path}: {e}")
            return None

    def _is_file_modified(self, file_path: str) -> bool:
        """
        æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²è¢«ä¿®æ”¹ã€‚
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            å¦‚æœæ–‡ä»¶å·²ä¿®æ”¹è¿”å›Trueï¼Œå¦åˆ™è¿”å›False
        """
        current_info = self._get_file_info(file_path)
        if not current_info:
            return False
        
        db_metadata = self._get_file_metadata_from_db(file_path)
        if not db_metadata:
            return True  # æ•°æ®åº“ä¸­æ²¡æœ‰è¯¥æ–‡ä»¶ï¼Œè§†ä¸ºæ–°æ–‡ä»¶
        
        # æ¯”è¾ƒæ–‡ä»¶å“ˆå¸Œå€¼
        db_hash = db_metadata.get('file_hash')
        return current_info['hash'] != db_hash

    def delete_documents_by_source(self, source_path: str) -> bool:
        """
        æ ¹æ®æºæ–‡ä»¶è·¯å¾„åˆ é™¤å‘é‡æ•°æ®åº“ä¸­çš„ç›¸å…³æ–‡æ¡£ã€‚
        
        Args:
            source_path: æºæ–‡ä»¶è·¯å¾„
            
        Returns:
            åˆ é™¤æˆåŠŸè¿”å›Trueï¼Œå¦åˆ™è¿”å›False
        """
        if not self.vector_store:
            print("å‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–ï¼Œæ— æ³•åˆ é™¤æ–‡æ¡£ã€‚")
            return False
        
        try:
            # è·å–è¯¥æ–‡ä»¶çš„æ‰€æœ‰æ–‡æ¡£ID
            all_entries = self.vector_store.get(
                where={"source": source_path},
                include=["metadatas"]
            )
            
            if not all_entries['ids']:
                print(f"æœªæ‰¾åˆ°æ¥æºä¸º '{source_path}' çš„æ–‡æ¡£ã€‚")
                return False
            
            # åˆ é™¤æ‰€æœ‰ç›¸å…³æ–‡æ¡£
            self.vector_store.delete(ids=all_entries['ids'])
            print(f"å·²åˆ é™¤ {len(all_entries['ids'])} ä¸ªæ¥æºä¸º '{source_path}' çš„æ–‡æ¡£å—ã€‚")
            return True
            
        except Exception as e:
            print(f"åˆ é™¤æ–‡æ¡£æ—¶å‡ºé”™: {e}")
            return False

    def update_document(self, file_path: str) -> bool:
        """
        æ›´æ–°å•ä¸ªæ–‡æ¡£ï¼šå…ˆåˆ é™¤æ—§ç‰ˆæœ¬ï¼Œå†æ·»åŠ æ–°ç‰ˆæœ¬ã€‚
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            æ›´æ–°æˆåŠŸè¿”å›Trueï¼Œå¦åˆ™è¿”å›False
        """
        try:
            print(f"æ­£åœ¨æ›´æ–°æ–‡æ¡£: {file_path}")
            
            # 1. åˆ é™¤æ—§ç‰ˆæœ¬
            if not self.delete_documents_by_source(file_path):
                print(f"åˆ é™¤æ—§ç‰ˆæœ¬å¤±è´¥: {file_path}")
                return False
            
            # 2. åŠ è½½æ–°ç‰ˆæœ¬
            loader = TextLoader(file_path, encoding='utf-8')
            new_docs = loader.load()
            
            # 3. æ·»åŠ æ–‡ä»¶ä¿¡æ¯åˆ°å…ƒæ•°æ®
            file_info = self._get_file_info(file_path)
            if file_info:
                for doc in new_docs:
                    doc.metadata.update({
                        'file_hash': file_info['hash'],
                        'file_mtime': file_info['mtime'],
                        'file_size': file_info['size']
                    })
            
            # 4. åˆ†å‰²æ–‡æ¡£
            chunks = self.text_splitter.split_documents(new_docs)
            
            # 5. ç”Ÿæˆå”¯ä¸€ID
            for i, chunk in enumerate(chunks):
                chunk_id = f"{config.DOCUMENT_ID_PREFIX}{hashlib.md5(file_path.encode()).hexdigest()}_{i}"
                chunk.metadata['chunk_id'] = chunk_id
            
            # 6. æ·»åŠ åˆ°æ•°æ®åº“
            self.vector_store.add_documents(chunks)
            print(f"  - å·²æ›´æ–°æ–‡æ¡£ï¼Œæ–°å¢ {len(chunks)} ä¸ªæ–‡æœ¬å—ã€‚")
            
            return True
            
        except Exception as e:
            print(f"æ›´æ–°æ–‡æ¡£æ—¶å‡ºé”™: {e}")
            return False

    def _get_enterprise_data_sources(self) -> Dict[str, Dict[str, Any]]:
        """
        è·å–ä¼ä¸šçº§æ•°æ®æºé…ç½®ã€‚
        
        Returns:
            æ•°æ®æºé…ç½®å­—å…¸
        """
        if config.ENABLE_ENTERPRISE_MODE:
            return {k: v for k, v in config.ENTERPRISE_DATA_SOURCES.items() if v.get('enabled', True)}
        else:
            # å‘åå…¼å®¹æ¨¡å¼
            return {
                "legacy": {
                    "path": config.DATA_PATH,
                    "category": "general",
                    "description": "ä¼ ç»Ÿæ•°æ®ç›®å½•",
                    "enabled": True,
                    "file_patterns": ["*.txt"],
                    "priority": 1
                }
            }

    def _scan_enterprise_files(self) -> Dict[str, List[str]]:
        """
        æ‰«æä¼ä¸šçº§æ•°æ®æºä¸­çš„æ‰€æœ‰æ–‡ä»¶ã€‚
        
        Returns:
            æŒ‰æ•°æ®æºåˆ†ç»„çš„æ–‡ä»¶åˆ—è¡¨
        """
        data_sources = self._get_enterprise_data_sources()
        all_files_by_source = {}
        
        for source_name, source_config in data_sources.items():
            source_path = source_config['path']
            file_patterns = source_config.get('file_patterns', ['*.txt'])
            
            if not os.path.exists(source_path):
                print(f"è­¦å‘Š: æ•°æ®æº '{source_name}' çš„è·¯å¾„ '{source_path}' ä¸å­˜åœ¨ã€‚")
                all_files_by_source[source_name] = []
                continue
            
            source_files = []
            for pattern in file_patterns:
                # ä½¿ç”¨globé€’å½’æœç´¢æ–‡ä»¶
                pattern_path = os.path.join(source_path, "**", pattern)
                matched_files = glob.glob(pattern_path, recursive=True)
                source_files.extend(matched_files)
            
            # å»é‡å¹¶è§„èŒƒåŒ–è·¯å¾„
            source_files = list(set(os.path.normpath(f) for f in source_files))
            all_files_by_source[source_name] = source_files
            
            print(f"æ•°æ®æº '{source_name}' ({source_config['description']}): å‘ç° {len(source_files)} ä¸ªæ–‡ä»¶")
        
        return all_files_by_source

    def _add_category_metadata(self, docs: List[Document], source_name: str, source_config: Dict[str, Any]) -> List[Document]:
        """
        ä¸ºæ–‡æ¡£æ·»åŠ åˆ†ç±»å…ƒæ•°æ®ã€‚
        
        Args:
            docs: æ–‡æ¡£åˆ—è¡¨
            source_name: æ•°æ®æºåç§°
            source_config: æ•°æ®æºé…ç½®
            
        Returns:
            æ·»åŠ äº†åˆ†ç±»å…ƒæ•°æ®çš„æ–‡æ¡£åˆ—è¡¨
        """
        for doc in docs:
            doc.metadata.update({
                'data_source': source_name,
                'category': source_config['category'],
                'description': source_config['description'],
                'priority': source_config.get('priority', 999)
            })
        return docs

    def sync_data_directory(self):
        """
        ä¼ä¸šçº§æ™ºèƒ½åŒæ­¥æ•°æ®ç›®å½•ã€‚æ”¯æŒå¤šè·¯å¾„ã€åˆ†ç±»ç®¡ç†ã€‚
        """
        if config.ENABLE_ENTERPRISE_MODE:
            print("--- å¼€å§‹ä¼ä¸šçº§æ™ºèƒ½åŒæ­¥ ---")
            self._sync_enterprise_data_sources()
        else:
            print("--- å¼€å§‹ä¼ ç»Ÿæ¨¡å¼åŒæ­¥ ---")
            self._sync_legacy_data_directory()

    def _sync_legacy_data_directory(self):
        """
        ä¼ ç»Ÿå•ä¸€æ•°æ®ç›®å½•åŒæ­¥ï¼ˆå‘åå…¼å®¹ï¼‰ã€‚
        """
        data_path = config.DATA_PATH
        if not os.path.exists(data_path):
            print(f"è­¦å‘Š: æ•°æ®ç›®å½• '{data_path}' ä¸å­˜åœ¨ã€‚")
            return

        print("--- å¼€å§‹æ™ºèƒ½åŒæ­¥æ•°æ®ç›®å½• ---")
        
        # 1. è·å–å·²å¤„ç†çš„æ–‡ä»¶åˆ—è¡¨
        processed_sources = self._get_processed_sources()
        print(f"æ•°æ®åº“ä¸­å·²å­˜åœ¨ {len(processed_sources)} ä¸ªæ¥æºçš„æ–‡ä»¶ã€‚")

        # 2. æ‰«ææ•°æ®ç›®å½•ï¼Œæ‰¾å‡ºæ‰€æœ‰ .txt æ–‡ä»¶
        current_files = []
        for root, _, files in os.walk(data_path):
            for file in files:
                if file.endswith(".txt"):
                    current_files.append(os.path.join(root, file))
        
        print(f"å½“å‰ç›®å½•ä¸­å‘ç° {len(current_files)} ä¸ª .txt æ–‡ä»¶ã€‚")
        
        # 3. åˆ†ç±»å¤„ç†æ–‡ä»¶
        new_files = []
        modified_files = []
        unchanged_files = []
        
        for file_path in current_files:
            if file_path not in processed_sources:
                new_files.append(file_path)
            elif config.ENABLE_FILE_MONITORING and self._is_file_modified(file_path):
                modified_files.append(file_path)
            else:
                unchanged_files.append(file_path)
        
        # 4. å¤„ç†å·²åˆ é™¤çš„æ–‡ä»¶
        deleted_files = []
        if config.AUTO_DELETE_MISSING_FILES:
            for processed_file in processed_sources:
                if processed_file not in current_files:
                    deleted_files.append(processed_file)
        
        # 5. æŠ¥å‘Šåˆ†æç»“æœ
        print(f"æ–‡ä»¶åˆ†æç»“æœ:")
        print(f"  - æ–°å¢æ–‡ä»¶: {len(new_files)} ä¸ª")
        print(f"  - ä¿®æ”¹æ–‡ä»¶: {len(modified_files)} ä¸ª")
        print(f"  - åˆ é™¤æ–‡ä»¶: {len(deleted_files)} ä¸ª")
        print(f"  - æœªå˜åŒ–æ–‡ä»¶: {len(unchanged_files)} ä¸ª")
        
        # 6. å¤„ç†åˆ é™¤çš„æ–‡ä»¶
        if deleted_files:
            print("\n--- å¤„ç†å·²åˆ é™¤çš„æ–‡ä»¶ ---")
            for file_path in deleted_files:
                if self.delete_documents_by_source(file_path):
                    print(f"  âœ“ å·²åˆ é™¤: {file_path}")
                else:
                    print(f"  âœ— åˆ é™¤å¤±è´¥: {file_path}")
        
        # 7. å¤„ç†ä¿®æ”¹çš„æ–‡ä»¶
        if modified_files:
            print("\n--- å¤„ç†å·²ä¿®æ”¹çš„æ–‡ä»¶ ---")
            for file_path in modified_files:
                if self.update_document(file_path):
                    print(f"  âœ“ å·²æ›´æ–°: {file_path}")
                else:
                    print(f"  âœ— æ›´æ–°å¤±è´¥: {file_path}")
        
        # 8. å¤„ç†æ–°å¢çš„æ–‡ä»¶
        if new_files:
            print(f"\n--- å¤„ç†æ–°å¢çš„æ–‡ä»¶ ---")
            print(f"å‘ç° {len(new_files)} ä¸ªæ–°æ–‡æ¡£ï¼Œæ­£åœ¨å¤„ç†...")
            
            # åŠ è½½æ–°æ–‡æ¡£
            new_docs = []
            for file_path in new_files:
                try:
                    loader = TextLoader(file_path, encoding='utf-8')
                    docs = loader.load()
                    
                    # æ·»åŠ æ–‡ä»¶ä¿¡æ¯åˆ°å…ƒæ•°æ®
                    file_info = self._get_file_info(file_path)
                    if file_info:
                        for doc in docs:
                            doc.metadata.update({
                                'file_hash': file_info['hash'],
                                'file_mtime': file_info['mtime'],
                                'file_size': file_info['size']
                            })
                    
                    new_docs.extend(docs)
                    print(f"  âœ“ å·²åŠ è½½: {file_path}")
                except Exception as e:
                    print(f"  âœ— åŠ è½½å¤±è´¥: {file_path} - {e}")
            
            if new_docs:
                chunks = self.text_splitter.split_documents(new_docs)
                '''chunks demo
                chunks = [
                    Document(
                        page_content="æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿåœ¨æ²¡æœ‰æ˜ç¡®ç¼–ç¨‹çš„æƒ…å†µä¸‹å­¦ä¹ ã€‚",
                        metadata={
                            # åŸå§‹æ–‡ä»¶ä¿¡æ¯
                            'source': 'data/æœºå™¨å­¦ä¹ ä»‹ç».txt',
                            
                            # æ–‡ä»¶å…ƒæ•°æ®ï¼ˆç”± _get_file_info æ·»åŠ ï¼‰
                            'file_hash': 'a1b2c3d4e5f6...',
                            'file_mtime': 1704067200.123,
                            'file_size': 256,
                            
                            # åˆ†å—ä¿¡æ¯ï¼ˆç”±ä»£ç æ·»åŠ ï¼‰
                            'chunk_id': 'doc_5d41402abc4b2a76b9719d911017c592_0',
                            
                            # å¯èƒ½çš„å…¶ä»–å…ƒæ•°æ®
                            'chunk_index': 0,
                            'total_chunks': 3
                        }
                    ),
                    
                    Document(
                        page_content="æœºå™¨å­¦ä¹ ç®—æ³•é€šè¿‡è®­ç»ƒæ•°æ®æ¥æ„å»ºæ•°å­¦æ¨¡å‹ï¼Œä»¥ä¾¿å¯¹æ–°æ•°æ®è¿›è¡Œé¢„æµ‹æˆ–å†³ç­–ã€‚",
                        metadata={
                            'source': 'data/æœºå™¨å­¦ä¹ ä»‹ç».txt',
                            'file_hash': 'a1b2c3d4e5f6...',
                            'file_mtime': 1704067200.123,
                            'file_size': 256,
                            'chunk_id': 'doc_5d41402abc4b2a76b9719d911017c592_1',
                            'chunk_index': 1,
                            'total_chunks': 3
                        }
                    ),
                    
                    Document(
                        page_content="å¸¸è§çš„æœºå™¨å­¦ä¹ ç±»å‹åŒ…æ‹¬ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ã€‚",
                        metadata={
                            'source': 'data/æœºå™¨å­¦ä¹ ä»‹ç».txt',
                            'file_hash': 'a1b2c3d4e5f6...',
                            'file_mtime': 1704067200.123,
                            'file_size': 256,
                            'chunk_id': 'doc_5d41402abc4b2a76b9719d911017c592_2',
                            'chunk_index': 2,
                            'total_chunks': 3
                        }
                    )
                ]
                '''
                print(f"  - æ–°æ–‡æ¡£è¢«åˆ†å‰²æˆ {len(chunks)} ä¸ªæ–‡æœ¬å—ã€‚")

                # ç”Ÿæˆå”¯ä¸€ID
                for chunk in chunks:
                    source_path = chunk.metadata.get('source', '')
                    chunk_hash = hashlib.md5(f"{source_path}_{chunk.page_content}".encode()).hexdigest()
                    chunk.metadata['chunk_id'] = f"{config.DOCUMENT_ID_PREFIX}{chunk_hash}"

                # æ·»åŠ åˆ°æ•°æ®åº“
                if self.vector_store is None:
                    # å¦‚æœæ•°æ®åº“æ˜¯ç©ºçš„ï¼Œç›´æ¥åŸºäºæ–°æ–‡æ¡£åˆ›å»º
                    print("æ­£åœ¨åˆ›å»ºæ–°çš„å‘é‡æ•°æ®åº“...")
                    self.vector_store = Chroma.from_documents(
                        documents=chunks,
                        embedding=self.embeddings,
                        persist_directory=config.VECTOR_STORE_PATH
                    )
                    print(f"  - æ–°çš„å‘é‡æ•°æ®åº“å·²åˆ›å»ºäº '{config.VECTOR_STORE_PATH}'ã€‚")
                else:
                    # å¦åˆ™ï¼Œå¢é‡æ·»åŠ 
                    self.vector_store.add_documents(chunks)
                    print("  - æ–°çš„æ–‡æœ¬å—å·²æˆåŠŸæ·»åŠ åˆ°ç°æœ‰æ•°æ®åº“ã€‚")

        # 9. é‡æ–°æ„å»ºé—®ç­”é“¾ï¼ˆå¦‚æœæœ‰ä»»ä½•å˜åŒ–ï¼‰
        if new_files or modified_files or deleted_files:
            print("\n--- æ›´æ–°é—®ç­”é“¾ ---")
            self._load_all_documents()  # é‡æ–°åŠ è½½æ‰€æœ‰æ–‡æ¡£ç”¨äºå…³é”®å­—æ£€ç´¢
            self._build_qa_chain()
            print("é—®ç­”é“¾å·²æ›´æ–°ï¼ŒåŒ…å«æœ€æ–°çŸ¥è¯†ã€‚")
        else:
            print("\n--- æ— éœ€æ›´æ–° ---")
            print("æ‰€æœ‰æ–‡ä»¶éƒ½æ˜¯æœ€æ–°çš„ï¼Œæ— éœ€æ›´æ–°é—®ç­”é“¾ã€‚")
        
        print("--- æ™ºèƒ½åŒæ­¥å®Œæˆ ---")

    def _sync_enterprise_data_sources(self):
        """
        ä¼ä¸šçº§å¤šæ•°æ®æºåŒæ­¥ã€‚
        """
        # 1. æ‰«ææ‰€æœ‰ä¼ä¸šçº§æ•°æ®æº
        all_files_by_source = self._scan_enterprise_files()
        
        # 2. è·å–å·²å¤„ç†çš„æ–‡ä»¶åˆ—è¡¨
        processed_sources = self._get_processed_sources()
        print(f"æ•°æ®åº“ä¸­å·²å­˜åœ¨ {len(processed_sources)} ä¸ªæ¥æºçš„æ–‡ä»¶ã€‚")
        
        # 3. åˆå¹¶æ‰€æœ‰æ•°æ®æºçš„æ–‡ä»¶
        all_current_files = []
        for source_name, files in all_files_by_source.items():
            all_current_files.extend(files)
        
        print(f"æ‰€æœ‰æ•°æ®æºå…±å‘ç° {len(all_current_files)} ä¸ªæ–‡ä»¶ã€‚")
        
        # 4. åˆ†ç±»å¤„ç†æ–‡ä»¶
        new_files = []
        modified_files = []
        unchanged_files = []
        
        for file_path in all_current_files:
            if file_path not in processed_sources:
                new_files.append(file_path)
            elif config.ENABLE_FILE_MONITORING and self._is_file_modified(file_path):
                modified_files.append(file_path)
            else:
                unchanged_files.append(file_path)
        
        # 5. å¤„ç†å·²åˆ é™¤çš„æ–‡ä»¶
        deleted_files = []
        if config.AUTO_DELETE_MISSING_FILES:
            for processed_file in processed_sources:
                if processed_file not in all_current_files:
                    deleted_files.append(processed_file)
        
        # 6. æŠ¥å‘Šåˆ†æç»“æœ
        print(f"ä¼ä¸šçº§æ–‡ä»¶åˆ†æç»“æœ:")
        print(f"  - æ–°å¢æ–‡ä»¶: {len(new_files)} ä¸ª")
        print(f"  - ä¿®æ”¹æ–‡ä»¶: {len(modified_files)} ä¸ª")
        print(f"  - åˆ é™¤æ–‡ä»¶: {len(deleted_files)} ä¸ª")
        print(f"  - æœªå˜åŒ–æ–‡ä»¶: {len(unchanged_files)} ä¸ª")
        
        # 7. å¤„ç†åˆ é™¤çš„æ–‡ä»¶
        if deleted_files:
            print("\n--- å¤„ç†å·²åˆ é™¤çš„æ–‡ä»¶ ---")
            for file_path in deleted_files:
                if self.delete_documents_by_source(file_path):
                    print(f"  âœ“ å·²åˆ é™¤: {file_path}")
                else:
                    print(f"  âœ— åˆ é™¤å¤±è´¥: {file_path}")
        
        # 8. å¤„ç†ä¿®æ”¹çš„æ–‡ä»¶
        if modified_files:
            print("\n--- å¤„ç†å·²ä¿®æ”¹çš„æ–‡ä»¶ ---")
            for file_path in modified_files:
                if self._update_enterprise_document(file_path):
                    print(f"  âœ“ å·²æ›´æ–°: {file_path}")
                else:
                    print(f"  âœ— æ›´æ–°å¤±è´¥: {file_path}")
        
        # 9. å¤„ç†æ–°å¢çš„æ–‡ä»¶
        if new_files:
            print(f"\n--- å¤„ç†æ–°å¢çš„æ–‡ä»¶ ---")
            self._process_new_enterprise_files(new_files, all_files_by_source)
        
        # 10. é‡æ–°æ„å»ºé—®ç­”é“¾ï¼ˆå¦‚æœæœ‰ä»»ä½•å˜åŒ–ï¼‰
        if new_files or modified_files or deleted_files:
            print("\n--- æ›´æ–°é—®ç­”é“¾ ---")
            self._load_all_documents()
            self._build_qa_chain()
            print("é—®ç­”é“¾å·²æ›´æ–°ï¼ŒåŒ…å«æœ€æ–°çŸ¥è¯†ã€‚")
        else:
            print("\n--- æ— éœ€æ›´æ–° ---")
            print("æ‰€æœ‰æ–‡ä»¶éƒ½æ˜¯æœ€æ–°çš„ï¼Œæ— éœ€æ›´æ–°é—®ç­”é“¾ã€‚")
        
        print("--- ä¼ä¸šçº§æ™ºèƒ½åŒæ­¥å®Œæˆ ---")

    def _get_source_config_for_file(self, file_path: str, all_files_by_source: Dict[str, List[str]]) -> Optional[Dict[str, Any]]:
        """
        æ ¹æ®æ–‡ä»¶è·¯å¾„è·å–å¯¹åº”çš„æ•°æ®æºé…ç½®ã€‚
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            all_files_by_source: æŒ‰æ•°æ®æºåˆ†ç»„çš„æ–‡ä»¶åˆ—è¡¨
            
        Returns:
            æ•°æ®æºé…ç½®ï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å›None
        """
        data_sources = self._get_enterprise_data_sources()
        
        for source_name, files in all_files_by_source.items():
            if file_path in files:
                return data_sources.get(source_name)
        
        return None

    def _update_enterprise_document(self, file_path: str) -> bool:
        """
        æ›´æ–°ä¼ä¸šçº§æ–‡æ¡£ï¼ŒåŒ…å«åˆ†ç±»ä¿¡æ¯ã€‚
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            æ›´æ–°æˆåŠŸè¿”å›Trueï¼Œå¦åˆ™è¿”å›False
        """
        try:
            print(f"æ­£åœ¨æ›´æ–°ä¼ä¸šçº§æ–‡æ¡£: {file_path}")
            
            # 1. åˆ é™¤æ—§ç‰ˆæœ¬
            if not self.delete_documents_by_source(file_path):
                print(f"åˆ é™¤æ—§ç‰ˆæœ¬å¤±è´¥: {file_path}")
                return False
            
            # 2. è·å–æ–‡ä»¶å¯¹åº”çš„æ•°æ®æºé…ç½®
            all_files_by_source = self._scan_enterprise_files()
            source_config = self._get_source_config_for_file(file_path, all_files_by_source)
            
            if not source_config:
                print(f"æœªæ‰¾åˆ°æ–‡ä»¶ {file_path} å¯¹åº”çš„æ•°æ®æºé…ç½®")
                return False
            
            # 3. åŠ è½½æ–°ç‰ˆæœ¬
            loader = TextLoader(file_path, encoding='utf-8')
            new_docs = loader.load()
            
            # 4. æ·»åŠ æ–‡ä»¶ä¿¡æ¯å’Œåˆ†ç±»ä¿¡æ¯åˆ°å…ƒæ•°æ®
            file_info = self._get_file_info(file_path)
            if file_info:
                for doc in new_docs:
                    doc.metadata.update({
                        'file_hash': file_info['hash'],
                        'file_mtime': file_info['mtime'],
                        'file_size': file_info['size'],
                        'category': source_config['category'],
                        'data_source': source_config.get('description', ''),
                        'priority': source_config.get('priority', 999)
                    })
            
            # 5. åˆ†å‰²æ–‡æ¡£
            chunks = self.text_splitter.split_documents(new_docs)
            
            # 6. ç”Ÿæˆå”¯ä¸€ID
            for i, chunk in enumerate(chunks):
                chunk_id = f"{config.DOCUMENT_ID_PREFIX}{hashlib.md5(file_path.encode()).hexdigest()}_{i}"
                chunk.metadata['chunk_id'] = chunk_id
            
            # 7. æ·»åŠ åˆ°æ•°æ®åº“
            self.vector_store.add_documents(chunks)
            print(f"  - å·²æ›´æ–°æ–‡æ¡£ï¼Œæ–°å¢ {len(chunks)} ä¸ªæ–‡æœ¬å—ï¼Œç±»åˆ«: {source_config['category']}")
            
            return True
            
        except Exception as e:
            print(f"æ›´æ–°ä¼ä¸šçº§æ–‡æ¡£æ—¶å‡ºé”™: {e}")
            return False

    def _process_new_enterprise_files(self, new_files: List[str], all_files_by_source: Dict[str, List[str]]):
        """
        å¤„ç†æ–°å¢çš„ä¼ä¸šçº§æ–‡ä»¶ã€‚
        
        Args:
            new_files: æ–°å¢æ–‡ä»¶åˆ—è¡¨
            all_files_by_source: æŒ‰æ•°æ®æºåˆ†ç»„çš„æ–‡ä»¶åˆ—è¡¨
        """
        print(f"å‘ç° {len(new_files)} ä¸ªæ–°æ–‡æ¡£ï¼Œæ­£åœ¨å¤„ç†...")
        
        # æŒ‰æ•°æ®æºåˆ†ç»„å¤„ç†æ–°æ–‡ä»¶
        data_sources = self._get_enterprise_data_sources()
        files_by_category = {}
        
        for file_path in new_files:
            source_config = self._get_source_config_for_file(file_path, all_files_by_source)
            if source_config:
                category = source_config['category']
                if category not in files_by_category:
                    files_by_category[category] = []
                files_by_category[category].append((file_path, source_config))
        
        # æŒ‰ç±»åˆ«å¤„ç†æ–‡ä»¶
        all_new_docs = []
        for category, file_configs in files_by_category.items():
            print(f"\nå¤„ç†ç±»åˆ« '{category}' çš„æ–‡ä»¶:")
            
            for file_path, source_config in file_configs:
                try:
                    loader = TextLoader(file_path, encoding='utf-8')
                    docs = loader.load()
                    
                    # æ·»åŠ æ–‡ä»¶ä¿¡æ¯å’Œåˆ†ç±»ä¿¡æ¯åˆ°å…ƒæ•°æ®
                    file_info = self._get_file_info(file_path)
                    if file_info:
                        for doc in docs:
                            doc.metadata.update({
                                'file_hash': file_info['hash'],
                                'file_mtime': file_info['mtime'],
                                'file_size': file_info['size'],
                                'category': source_config['category'],
                                'data_source': source_config.get('description', ''),
                                'priority': source_config.get('priority', 999)
                            })
                    
                    all_new_docs.extend(docs)
                    print(f"  âœ“ å·²åŠ è½½: {file_path} (ç±»åˆ«: {category})")
                except Exception as e:
                    print(f"  âœ— åŠ è½½å¤±è´¥: {file_path} - {e}")
        
        if all_new_docs:
            chunks = self.text_splitter.split_documents(all_new_docs)
            print(f"\næ–°æ–‡æ¡£è¢«åˆ†å‰²æˆ {len(chunks)} ä¸ªæ–‡æœ¬å—ã€‚")

            # ç”Ÿæˆå”¯ä¸€IDå¹¶æ·»åŠ åˆ†ç±»ä¿¡æ¯
            for chunk in chunks:
                source_path = chunk.metadata.get('source', '')
                chunk_hash = hashlib.md5(f"{source_path}_{chunk.page_content}".encode()).hexdigest()
                chunk.metadata['chunk_id'] = f"{config.DOCUMENT_ID_PREFIX}{chunk_hash}"

            # æ·»åŠ åˆ°æ•°æ®åº“
            if self.vector_store is None:
                print("æ­£åœ¨åˆ›å»ºæ–°çš„å‘é‡æ•°æ®åº“...")
                self.vector_store = Chroma.from_documents(
                    documents=chunks,
                    embedding=self.embeddings,
                    persist_directory=config.VECTOR_STORE_PATH
                )
                print(f"  - æ–°çš„å‘é‡æ•°æ®åº“å·²åˆ›å»ºäº '{config.VECTOR_STORE_PATH}'ã€‚")
            else:
                self.vector_store.add_documents(chunks)
                print("  - æ–°çš„æ–‡æœ¬å—å·²æˆåŠŸæ·»åŠ åˆ°ç°æœ‰æ•°æ®åº“ã€‚")
            
            # æŒ‰ç±»åˆ«ç»Ÿè®¡
            category_stats = {}
            for chunk in chunks:
                category = chunk.metadata.get('category', 'unknown')
                category_stats[category] = category_stats.get(category, 0) + 1
            
            print("æŒ‰ç±»åˆ«ç»Ÿè®¡:")
            for category, count in category_stats.items():
                print(f"  - {category}: {count} ä¸ªæ–‡æœ¬å—")

    def _load_all_documents(self):
        """
        ä»å‘é‡æ•°æ®åº“ä¸­åŠ è½½æ‰€æœ‰æ–‡æ¡£ï¼Œç”¨äºæ„å»ºå…³é”®å­—æ£€ç´¢å™¨ã€‚
        """
        if not self.vector_store:
            print("è­¦å‘Š: å‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–ï¼Œæ— æ³•åŠ è½½æ–‡æ¡£ç”¨äºå…³é”®å­—æ£€ç´¢ã€‚")
            return
        
        try:
            # ä»ChromaDBè·å–æ‰€æœ‰æ–‡æ¡£å†…å®¹å’Œå…ƒæ•°æ®
            all_entries = self.vector_store.get(include=["documents", "metadatas"])
            
            # é‡æ„Documentå¯¹è±¡
            self.all_documents = []
            for i, (doc_content, metadata) in enumerate(zip(all_entries['documents'], all_entries['metadatas'])):
                doc = Document(
                    page_content=doc_content,
                    metadata=metadata or {}
                )
                self.all_documents.append(doc)
            
            print(f"  - å·²åŠ è½½ {len(self.all_documents)} ä¸ªæ–‡æ¡£å—ç”¨äºå…³é”®å­—æ£€ç´¢")
            
            # æ„å»ºBM25æ£€ç´¢å™¨
            self._build_bm25_retriever()
            
        except Exception as e:
            print(f"åŠ è½½æ–‡æ¡£ç”¨äºå…³é”®å­—æ£€ç´¢æ—¶å‡ºé”™: {e}")
            self.all_documents = []

    def _build_bm25_retriever(self):
        """
        æ„å»ºBM25å…³é”®å­—æ£€ç´¢å™¨ã€‚
        """
        if not self.all_documents:
            print("è­¦å‘Š: æ²¡æœ‰æ–‡æ¡£å¯ç”¨äºæ„å»ºBM25æ£€ç´¢å™¨ã€‚")
            return
        
        try:
            # ä½¿ç”¨jiebaè¿›è¡Œä¸­æ–‡åˆ†è¯çš„é¢„å¤„ç†å‡½æ•°
            def preprocess_func(text: str) -> List[str]:
                # å¯¹ä¸­æ–‡æ–‡æœ¬è¿›è¡Œåˆ†è¯
                return list(jieba.cut(text))
            
            # æ„å»ºBM25æ£€ç´¢å™¨
            self.bm25_retriever = BM25Retriever.from_documents(
                self.all_documents,
                preprocess_func=preprocess_func
            )
            self.bm25_retriever.k = config.KEYWORD_RETRIEVER_TOP_K
            
            print(f"  - BM25å…³é”®å­—æ£€ç´¢å™¨æ„å»ºå®Œæˆï¼ŒTop-K: {config.KEYWORD_RETRIEVER_TOP_K}")
            
        except Exception as e:
            print(f"æ„å»ºBM25æ£€ç´¢å™¨æ—¶å‡ºé”™: {e}")
            self.bm25_retriever = None

    def _build_qa_chain(self):
        """
        æ„å»ºåŒ…å«æ£€ç´¢å™¨ã€é‡æ’åºå™¨å’ŒLLMçš„é—®ç­”é“¾ã€‚
        """
        # æ„å»ºæ··åˆæ£€ç´¢å™¨
        hybrid_retriever = self._build_hybrid_retriever()
        
        # å‹ç¼©æ£€ç´¢å™¨ï¼Œé›†æˆäº†é‡æ’åºé€»è¾‘
        compression_retriever = ContextualCompressionRetriever(
            base_compressor=self.reranker,
            base_retriever=hybrid_retriever
        )
        
        # ä½¿ç”¨æç¤ºè¯ç®¡ç†å™¨è·å–é—®ç­”æç¤ºæ¨¡æ¿
        QA_CHAIN_PROMPT = get_qa_prompt_template()

        self.qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff", # "stuff"æ¨¡å¼ä¼šå°†æ‰€æœ‰æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹â€œå¡â€è¿›ä¸€ä¸ªPromptä¸­
            retriever=compression_retriever,
            chain_type_kwargs={"prompt": QA_CHAIN_PROMPT},
            return_source_documents=True # è¿”å›å¼•ç”¨çš„æºæ–‡æ¡£ï¼Œä¾¿äºæº¯æº
        )

    def _build_hybrid_retriever(self):
        """
        æ„å»ºæ··åˆæ£€ç´¢å™¨ï¼Œç»“åˆå‘é‡æ£€ç´¢å’Œå…³é”®å­—æ£€ç´¢ã€‚
        """
        # å‘é‡æ£€ç´¢å™¨
        vector_retriever = self.vector_store.as_retriever(
            search_type="similarity",
            search_kwargs={"k": config.RETRIEVER_TOP_K}
        )
        
        # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦å¯ç”¨æ··åˆæ£€ç´¢
        if config.ENABLE_HYBRID_SEARCH and self.bm25_retriever is not None:
            print(f"  - å¯ç”¨æ··åˆæ£€ç´¢æ¨¡å¼ (å‘é‡æƒé‡: {config.VECTOR_SEARCH_WEIGHT}, å…³é”®å­—æƒé‡: {config.KEYWORD_SEARCH_WEIGHT})")
            
            # åˆ›å»ºæ··åˆæ£€ç´¢å™¨
            ensemble_retriever = EnsembleRetriever(
                retrievers=[vector_retriever, self.bm25_retriever],
                weights=[config.VECTOR_SEARCH_WEIGHT, config.KEYWORD_SEARCH_WEIGHT]
            )
            return ensemble_retriever
        else:
            print("  - ä½¿ç”¨çº¯å‘é‡æ£€ç´¢æ¨¡å¼")
            return vector_retriever

    def ask_with_categories(self, question: str, categories: List[str] = None, use_memory: bool = True) -> Dict[str, Any]:
        """
        æ”¯æŒåˆ†ç±»æ£€ç´¢çš„é—®ç­”åŠŸèƒ½ã€‚
        
        Args:
            question: ç”¨æˆ·æå‡ºçš„é—®é¢˜å­—ç¬¦ä¸²
            categories: æŒ‡å®šæ£€ç´¢çš„ç±»åˆ«åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºæ£€ç´¢æ‰€æœ‰ç±»åˆ«
            
        Returns:
            ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«'result' (ç­”æ¡ˆ) å’Œ 'source_documents' (å‚è€ƒçš„æ–‡æ¡£ç‰‡æ®µ)
        """
        if not self.qa_chain:
            return {
                "result": "é”™è¯¯: é—®ç­”é“¾å°šæœªåˆå§‹åŒ–ã€‚è¯·å…ˆè°ƒç”¨ `sync_data_directory` æ–¹æ³•åŠ è½½æ–‡æ¡£ã€‚",
                "source_documents": []
            }
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šç±»åˆ«ï¼Œä½¿ç”¨é»˜è®¤ç±»åˆ«æˆ–æ‰€æœ‰ç±»åˆ«
        if categories is None:
            categories = config.DEFAULT_SEARCH_CATEGORIES
        
        print(f"\næ­£åœ¨å¤„ç†é—®é¢˜: '{question}'...")
        if categories:
            print(f"é™å®šæ£€ç´¢ç±»åˆ«: {categories}")
        
        # è·å–çŸ­æœŸè®°å¿†ä¸Šä¸‹æ–‡
        memory_context = ""
        if use_memory and config.ENABLE_SHORT_TERM_MEMORY:
            memory_context = memory_manager.get_conversation_context(include_count=None)  # ä½¿ç”¨æ‰€æœ‰å¯¹è¯è½®æ¬¡
            if memory_context:
                print("--- çŸ­æœŸè®°å¿†ä¸Šä¸‹æ–‡ ---")
                print(f"åŒ…å« {len(memory_manager.get_recent_conversations())} è½®å¯¹è¯ä½œä¸ºä¸Šä¸‹æ–‡")
        
        # å¦‚æœå¯ç”¨äº†é—®é¢˜æ”¹å†™åŠŸèƒ½
        if config.ENABLE_QUERY_REWRITING:
            print("--- é—®é¢˜æ”¹å†™é˜¶æ®µ ---")
            
            # 1. æ”¹å†™é—®é¢˜
            rewritten_queries = self._rewrite_query(question)
            
            # 2. ä½¿ç”¨å¤šä¸ªé—®é¢˜è¿›è¡Œåˆ†ç±»æ£€ç´¢
            print("--- å¤šæŸ¥è¯¢åˆ†ç±»æ£€ç´¢é˜¶æ®µ ---")
            retrieved_docs = self._retrieve_with_multiple_queries_and_categories(rewritten_queries, categories)
            
            # 3. é‡æ’åº
            print("--- é‡æ’åºé˜¶æ®µ ---")
            if retrieved_docs and self.reranker:
                try:
                    reranked_docs = self.reranker.compress_documents(retrieved_docs, question)
                    final_docs = reranked_docs[:config.RERANKER_TOP_N]
                    print(f"  - é‡æ’åºå®Œæˆï¼Œæœ€ç»ˆé€‰æ‹© {len(final_docs)} ä¸ªæœ€ç›¸å…³æ–‡æ¡£")
                except Exception as e:
                    print(f"  - é‡æ’åºå¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹æ£€ç´¢ç»“æœ")
                    final_docs = retrieved_docs[:config.RERANKER_TOP_N]
            else:
                final_docs = retrieved_docs[:config.RERANKER_TOP_N]
            
            # 4. ç”Ÿæˆç­”æ¡ˆ
            print("--- ç­”æ¡ˆç”Ÿæˆé˜¶æ®µ ---")
            if final_docs:
                # æ„å»ºä¸Šä¸‹æ–‡
                context = "\n\n".join([doc.page_content for doc in final_docs])
                
                # æ„å»ºå®Œæ•´çš„ä¸Šä¸‹æ–‡ï¼ˆåŒ…å«è®°å¿†ä¸Šä¸‹æ–‡å’Œæ£€ç´¢ä¸Šä¸‹æ–‡ï¼‰
                full_context = context
                if memory_context:
                    full_context = f"å¯¹è¯å†å²:\n{memory_context}\n\nå½“å‰æ£€ç´¢åˆ°çš„ç›¸å…³ä¿¡æ¯:\n{context}"
                
                # ä½¿ç”¨æç¤ºè¯ç®¡ç†å™¨è·å–é—®ç­”æç¤ºæ¨¡æ¿
                qa_template = get_qa_prompt_template()
                prompt = qa_template.format(context=full_context, question=question)
                response = self.llm.invoke(prompt)
                
                if hasattr(response, 'content'):
                    answer = response.content.strip()
                else:
                    answer = str(response).strip()
                
                # ä¿å­˜å¯¹è¯åˆ°çŸ­æœŸè®°å¿†
                if use_memory and config.ENABLE_SHORT_TERM_MEMORY:
                    memory_manager.add_conversation(
                        question=question,
                        answer=answer,
                        metadata={
                            "used_query_rewriting": True,
                            "used_categories": categories,
                            "memory_context_included": bool(memory_context),
                            "source_documents_count": len(final_docs)
                        }
                    )
                
                return {
                    "result": answer,
                    "source_documents": final_docs
                }
            else:
                no_result_answer = "æ ¹æ®æä¾›çš„èµ„æ–™ï¼Œæˆ‘æ— æ³•å›ç­”è¯¥é—®é¢˜ã€‚"
                
                # ä¿å­˜å¯¹è¯åˆ°çŸ­æœŸè®°å¿†
                if use_memory and config.ENABLE_SHORT_TERM_MEMORY:
                    memory_manager.add_conversation(
                        question=question,
                        answer=no_result_answer,
                        metadata={
                            "used_query_rewriting": True,
                            "used_categories": categories,
                            "memory_context_included": bool(memory_context),
                            "source_documents_count": 0,
                            "no_result": True
                        }
                    )
                
                return {
                    "result": no_result_answer,
                    "source_documents": []
                }
        else:
            # ä½¿ç”¨åˆ†ç±»æ£€ç´¢çš„é—®ç­”é“¾ï¼Œä½†éœ€è¦æ‰‹åŠ¨å¤„ç†è®°å¿†ä¸Šä¸‹æ–‡
            if categories:
                print("--- åˆ†ç±»æ£€ç´¢æ¨¡å¼ ---")
                
                # åˆ›å»ºä¸´æ—¶çš„åˆ†ç±»æ£€ç´¢å™¨
                category_retriever = self._build_category_retriever(categories)
                compression_retriever = ContextualCompressionRetriever(
                    base_compressor=self.reranker,
                    base_retriever=category_retriever
                )
                
                # è·å–ç›¸å…³æ–‡æ¡£
                retrieved_docs = compression_retriever.get_relevant_documents(question)
                
                if retrieved_docs:
                    # æ„å»ºä¸Šä¸‹æ–‡
                    context = "\n\n".join([doc.page_content for doc in retrieved_docs])
                    
                    # æ„å»ºå®Œæ•´çš„ä¸Šä¸‹æ–‡ï¼ˆåŒ…å«è®°å¿†ä¸Šä¸‹æ–‡å’Œæ£€ç´¢ä¸Šä¸‹æ–‡ï¼‰
                    full_context = context
                    if memory_context:
                        full_context = f"å¯¹è¯å†å²:\n{memory_context}\n\nå½“å‰æ£€ç´¢åˆ°çš„ç›¸å…³ä¿¡æ¯:\n{context}"
                    
                    # ä½¿ç”¨æç¤ºè¯ç®¡ç†å™¨è·å–é—®ç­”æç¤ºæ¨¡æ¿
                    qa_template = get_qa_prompt_template()
                    prompt = qa_template.format(context=full_context, question=question)
                    response = self.llm.invoke(prompt)
                    
                    if hasattr(response, 'content'):
                        answer = response.content.strip()
                    else:
                        answer = str(response).strip()
                    
                    result = {
                        "result": answer,
                        "source_documents": retrieved_docs
                    }
                else:
                    no_result_answer = "æ ¹æ®æä¾›çš„èµ„æ–™ï¼Œæˆ‘æ— æ³•å›ç­”è¯¥é—®é¢˜ã€‚"
                    result = {
                        "result": no_result_answer,
                        "source_documents": []
                    }
                
                # ä¿å­˜å¯¹è¯åˆ°çŸ­æœŸè®°å¿†
                if use_memory and config.ENABLE_SHORT_TERM_MEMORY:
                    memory_manager.add_conversation(
                        question=question,
                        answer=result.get("result", ""),
                        metadata={
                            "used_query_rewriting": False,
                            "used_categories": categories,
                            "memory_context_included": bool(memory_context),
                            "source_documents_count": len(result.get("source_documents", []))
                        }
                    )
                
                return result
            else:
                # ä½¿ç”¨åŸå§‹çš„é—®ç­”é“¾ï¼Œä½†éœ€è¦æ‰‹åŠ¨å¤„ç†è®°å¿†ä¸Šä¸‹æ–‡
                print("--- åŸå§‹é—®ç­”é“¾æ¨¡å¼ (ask_with_categories) ---")
                
                # å…ˆè·å–ç›¸å…³æ–‡æ¡£
                retriever = self.qa_chain.retriever
                retrieved_docs = retriever.get_relevant_documents(question)
                
                if retrieved_docs:
                    # æ„å»ºä¸Šä¸‹æ–‡
                    context = "\n\n".join([doc.page_content for doc in retrieved_docs])
                    
                    # æ„å»ºå®Œæ•´çš„ä¸Šä¸‹æ–‡ï¼ˆåŒ…å«è®°å¿†ä¸Šä¸‹æ–‡å’Œæ£€ç´¢ä¸Šä¸‹æ–‡ï¼‰
                    full_context = context
                    if memory_context:
                        full_context = f"å¯¹è¯å†å²:\n{memory_context}\n\nå½“å‰æ£€ç´¢åˆ°çš„ç›¸å…³ä¿¡æ¯:\n{context}"
                    
                    # ä½¿ç”¨æç¤ºè¯ç®¡ç†å™¨è·å–é—®ç­”æç¤ºæ¨¡æ¿
                    qa_template = get_qa_prompt_template()
                    prompt = qa_template.format(context=full_context, question=question)
                    response = self.llm.invoke(prompt)
                    
                    if hasattr(response, 'content'):
                        answer = response.content.strip()
                    else:
                        answer = str(response).strip()
                    
                    result = {
                        "result": answer,
                        "source_documents": retrieved_docs
                    }
                else:
                    no_result_answer = "æ ¹æ®æä¾›çš„èµ„æ–™ï¼Œæˆ‘æ— æ³•å›ç­”è¯¥é—®é¢˜ã€‚"
                    result = {
                        "result": no_result_answer,
                        "source_documents": []
                    }
                
                # ä¿å­˜å¯¹è¯åˆ°çŸ­æœŸè®°å¿†
                if use_memory and config.ENABLE_SHORT_TERM_MEMORY:
                    memory_manager.add_conversation(
                        question=question,
                        answer=result.get("result", ""),
                        metadata={
                            "used_query_rewriting": False,
                            "used_categories": None,
                            "memory_context_included": bool(memory_context),
                            "source_documents_count": len(result.get("source_documents", []))
                        }
                    )
                
                return result

    def _retrieve_with_multiple_queries_and_categories(self, queries: List[str], categories: List[str] = None) -> List[Document]:
        """
        ä½¿ç”¨å¤šä¸ªæŸ¥è¯¢é—®é¢˜è¿›è¡Œåˆ†ç±»æ£€ç´¢ï¼Œå¹¶åˆå¹¶ç»“æœã€‚
        
        Args:
            queries: æŸ¥è¯¢é—®é¢˜åˆ—è¡¨
            categories: æŒ‡å®šæ£€ç´¢çš„ç±»åˆ«åˆ—è¡¨
            
        Returns:
            åˆå¹¶åçš„æ–‡æ¡£åˆ—è¡¨
        """
        all_documents = []
        seen_contents = set()
        
        for i, query in enumerate(queries):
            print(f"  - æ‰§è¡ŒæŸ¥è¯¢ {i+1}: {query}")
            
            try:
                # ä½¿ç”¨åˆ†ç±»æ£€ç´¢å™¨è¿›è¡Œæ£€ç´¢
                if categories:
                    category_retriever = self._build_category_retriever(categories)
                else:
                    category_retriever = self._build_hybrid_retriever()
                
                # ä¸ºæ”¹å†™çš„æŸ¥è¯¢ä½¿ç”¨è¾ƒå°‘çš„æ£€ç´¢æ•°é‡
                if i == 0:
                    k = config.RETRIEVER_TOP_K
                else:
                    k = config.REWRITE_QUERY_TOP_K
                
                # ä¸´æ—¶è°ƒæ•´æ£€ç´¢å™¨çš„kå€¼
                if hasattr(category_retriever, 'retrievers'):
                    for retriever in category_retriever.retrievers:
                        if hasattr(retriever, 'search_kwargs'):
                            retriever.search_kwargs['k'] = k
                        elif hasattr(retriever, 'k'):
                            retriever.k = k
                
                docs = category_retriever.invoke(query)
                
                # å»é‡å¤„ç†
                for doc in docs:
                    content_hash = hash(doc.page_content)
                    if config.ENABLE_DOCUMENT_DEDUPLICATION:
                        if content_hash not in seen_contents:
                            all_documents.append(doc)
                            seen_contents.add(content_hash)
                    else:
                        all_documents.append(doc)
                        
                print(f"    æ£€ç´¢åˆ° {len(docs)} ä¸ªæ–‡æ¡£")
                
            except Exception as e:
                print(f"    æŸ¥è¯¢æ‰§è¡Œå¤±è´¥: {e}")
                continue
        
        print(f"  - å¤šæŸ¥è¯¢åˆ†ç±»æ£€ç´¢å®Œæˆï¼Œå…±è·å¾— {len(all_documents)} ä¸ªæ–‡æ¡£")
        return all_documents

    def _build_category_retriever(self, categories: List[str]):
        """
        æ„å»ºåˆ†ç±»æ£€ç´¢å™¨ï¼Œåªæ£€ç´¢æŒ‡å®šç±»åˆ«çš„æ–‡æ¡£ã€‚
        
        Args:
            categories: ç±»åˆ«åˆ—è¡¨
            
        Returns:
            åˆ†ç±»æ£€ç´¢å™¨
        """
        if not categories:
            return self._build_hybrid_retriever()
        
        # è¿‡æ»¤æŒ‡å®šç±»åˆ«çš„æ–‡æ¡£
        category_documents = []
        for doc in self.all_documents:
            doc_category = doc.metadata.get('category', 'general')
            if doc_category in categories:
                category_documents.append(doc)
        
        print(f"  - åˆ†ç±»è¿‡æ»¤: ä» {len(self.all_documents)} ä¸ªæ–‡æ¡£ä¸­ç­›é€‰å‡º {len(category_documents)} ä¸ªæŒ‡å®šç±»åˆ«çš„æ–‡æ¡£")
        
        if not category_documents:
            print("  - è­¦å‘Š: æŒ‡å®šç±»åˆ«ä¸­æ²¡æœ‰æ‰¾åˆ°æ–‡æ¡£")
            return self._build_hybrid_retriever()
        
        # åˆ›å»ºä¸´æ—¶å‘é‡å­˜å‚¨ï¼ˆä»…åŒ…å«æŒ‡å®šç±»åˆ«çš„æ–‡æ¡£ï¼‰
        try:
            temp_vector_store = Chroma.from_documents(
                documents=category_documents,
                embedding=self.embeddings
            )
            
            vector_retriever = temp_vector_store.as_retriever(
                search_type="similarity",
                search_kwargs={"k": config.RETRIEVER_TOP_K}
            )
            
            # å¦‚æœå¯ç”¨æ··åˆæ£€ç´¢ï¼Œè¿˜éœ€è¦åˆ›å»ºåˆ†ç±»BM25æ£€ç´¢å™¨
            if config.ENABLE_HYBRID_SEARCH:
                try:
                    def preprocess_func(text: str) -> List[str]:
                        return list(jieba.cut(text))
                    
                    category_bm25_retriever = BM25Retriever.from_documents(
                        category_documents,
                        preprocess_func=preprocess_func
                    )
                    category_bm25_retriever.k = config.KEYWORD_RETRIEVER_TOP_K
                    
                    # åˆ›å»ºæ··åˆæ£€ç´¢å™¨
                    ensemble_retriever = EnsembleRetriever(
                        retrievers=[vector_retriever, category_bm25_retriever],
                        weights=[config.VECTOR_SEARCH_WEIGHT, config.KEYWORD_SEARCH_WEIGHT]
                    )
                    
                    print(f"  - åˆ†ç±»æ··åˆæ£€ç´¢å™¨æ„å»ºå®Œæˆ")
                    return ensemble_retriever
                    
                except Exception as e:
                    print(f"  - åˆ†ç±»BM25æ£€ç´¢å™¨æ„å»ºå¤±è´¥: {e}ï¼Œä½¿ç”¨çº¯å‘é‡æ£€ç´¢")
                    return vector_retriever
            else:
                return vector_retriever
                
        except Exception as e:
            print(f"  - åˆ†ç±»æ£€ç´¢å™¨æ„å»ºå¤±è´¥: {e}ï¼Œä½¿ç”¨å…¨å±€æ£€ç´¢å™¨")
            return self._build_hybrid_retriever()

    def get_available_categories(self) -> Dict[str, int]:
        """
        è·å–çŸ¥è¯†åº“ä¸­å¯ç”¨çš„ç±»åˆ«åŠå…¶æ–‡æ¡£æ•°é‡ã€‚
        
        Returns:
            ç±»åˆ«åç§°åˆ°æ–‡æ¡£æ•°é‡çš„æ˜ å°„
        """
        if not self.all_documents:
            return {}
        
        category_counts = {}
        for doc in self.all_documents:
            category = doc.metadata.get('category', 'general')
            category_counts[category] = category_counts.get(category, 0) + 1
        
        return category_counts

    def get_data_source_info(self) -> Dict[str, Dict[str, Any]]:
        """
        è·å–æ•°æ®æºä¿¡æ¯ç»Ÿè®¡ã€‚
        
        Returns:
            æ•°æ®æºä¿¡æ¯å­—å…¸
        """
        if not self.all_documents:
            return {}
        
        source_info = {}
        for doc in self.all_documents:
            category = doc.metadata.get('category', 'general')
            data_source = doc.metadata.get('data_source', 'unknown')
            
            if category not in source_info:
                source_info[category] = {
                    'count': 0,
                    'sources': set(),
                    'description': doc.metadata.get('description', ''),
                    'priority': doc.metadata.get('priority', 999)
                }
            
            source_info[category]['count'] += 1
            if data_source != 'unknown':
                source_info[category]['sources'].add(data_source)
        
        # è½¬æ¢setä¸ºlistä»¥ä¾¿JSONåºåˆ—åŒ–
        for category in source_info:
            source_info[category]['sources'] = list(source_info[category]['sources'])
        
        return source_info

    def _rewrite_query(self, original_query: str) -> List[str]:
        """
        å°†åŸå§‹é—®é¢˜æ”¹å†™æˆå¤šä¸ªç›¸å…³é—®é¢˜ï¼Œæé«˜æœç´¢è¦†ç›–é¢ã€‚
        
        Args:
            original_query: ç”¨æˆ·çš„åŸå§‹é—®é¢˜
            
        Returns:
            åŒ…å«åŸå§‹é—®é¢˜å’Œæ”¹å†™é—®é¢˜çš„åˆ—è¡¨
        """
        if not config.ENABLE_QUERY_REWRITING:
            return [original_query]
        
        try:
            # ä½¿ç”¨æç¤ºè¯ç®¡ç†å™¨è·å–é—®é¢˜æ”¹å†™æç¤ºæ¨¡æ¿
            rewrite_prompt = get_query_rewrite_prompt_template()
            
            # è°ƒç”¨LLMè¿›è¡Œé—®é¢˜æ”¹å†™
            prompt = rewrite_prompt.format(
                original_query=original_query,
                count=config.QUERY_REWRITE_COUNT
            )
            
            response = self.llm.invoke(prompt)
            
            # è§£ææ”¹å†™ç»“æœ
            rewritten_queries = []
            if hasattr(response, 'content'):
                content = response.content.strip()
            else:
                content = str(response).strip()
            
            # åˆ†å‰²å¹¶æ¸…ç†æ”¹å†™çš„é—®é¢˜
            lines = [line.strip() for line in content.split('\n') if line.strip()]
            for line in lines:
                # ç§»é™¤å¯èƒ½çš„ç¼–å·æ ¼å¼
                cleaned_line = line
                if line and (line[0].isdigit() or line.startswith('-') or line.startswith('â€¢')):
                    # ç§»é™¤ç¼–å·å’Œæ ‡ç‚¹
                    cleaned_line = line.split('.', 1)[-1].strip()
                    cleaned_line = cleaned_line.lstrip('- â€¢').strip()
                
                if cleaned_line and cleaned_line not in rewritten_queries:
                    rewritten_queries.append(cleaned_line)
            
            # ç¡®ä¿åŒ…å«åŸå§‹é—®é¢˜
            all_queries = [original_query]
            all_queries.extend(rewritten_queries[:config.QUERY_REWRITE_COUNT])
            
            print(f"  - é—®é¢˜æ”¹å†™å®Œæˆï¼Œç”Ÿæˆäº† {len(all_queries)} ä¸ªæŸ¥è¯¢é—®é¢˜")
            for i, query in enumerate(all_queries):
                print(f"    [{i+1}] {query}")
            
            return all_queries
            
        except Exception as e:
            print(f"é—®é¢˜æ”¹å†™å¤±è´¥: {e}")
            return [original_query]

    def _retrieve_with_multiple_queries(self, queries: List[str]) -> List[Document]:
        """
        ä½¿ç”¨å¤šä¸ªæŸ¥è¯¢é—®é¢˜è¿›è¡Œæ£€ç´¢ï¼Œå¹¶åˆå¹¶ç»“æœã€‚
        
        Args:
            queries: æŸ¥è¯¢é—®é¢˜åˆ—è¡¨
            
        Returns:
            åˆå¹¶åçš„æ–‡æ¡£åˆ—è¡¨
        """
        all_documents = []
        seen_contents = set()  # ç”¨äºå»é‡
        
        for i, query in enumerate(queries):
            print(f"  - æ‰§è¡ŒæŸ¥è¯¢ {i+1}: {query}")
            
            try:
                # ä½¿ç”¨æ··åˆæ£€ç´¢å™¨è¿›è¡Œæ£€ç´¢
                hybrid_retriever = self._build_hybrid_retriever()
                
                # ä¸ºæ”¹å†™çš„æŸ¥è¯¢ä½¿ç”¨è¾ƒå°‘çš„æ£€ç´¢æ•°é‡
                if i == 0:  # åŸå§‹æŸ¥è¯¢ä½¿ç”¨æ­£å¸¸æ•°é‡
                    k = config.RETRIEVER_TOP_K
                else:  # æ”¹å†™æŸ¥è¯¢ä½¿ç”¨è¾ƒå°‘æ•°é‡
                    k = config.REWRITE_QUERY_TOP_K
                
                # ä¸´æ—¶è°ƒæ•´æ£€ç´¢å™¨çš„kå€¼
                if hasattr(hybrid_retriever, 'retrievers'):
                    # EnsembleRetriever
                    for retriever in hybrid_retriever.retrievers:
                        if hasattr(retriever, 'search_kwargs'):
                            retriever.search_kwargs['k'] = k
                        elif hasattr(retriever, 'k'):
                            retriever.k = k
                
                docs = hybrid_retriever.invoke(query)
                
                # å»é‡å¤„ç†
                for doc in docs:
                    content_hash = hash(doc.page_content)
                    if config.ENABLE_DOCUMENT_DEDUPLICATION:
                        if content_hash not in seen_contents:
                            all_documents.append(doc)
                            seen_contents.add(content_hash)
                    else:
                        all_documents.append(doc)
                        
                print(f"    æ£€ç´¢åˆ° {len(docs)} ä¸ªæ–‡æ¡£")
                
            except Exception as e:
                print(f"    æŸ¥è¯¢æ‰§è¡Œå¤±è´¥: {e}")
                continue
        
        print(f"  - å¤šæŸ¥è¯¢æ£€ç´¢å®Œæˆï¼Œå…±è·å¾— {len(all_documents)} ä¸ªæ–‡æ¡£")
        return all_documents

    def ask(self, question: str, use_memory: bool = True) -> Dict[str, Any]:
        """
        å¯¹å·²åŠ è½½çš„æ–‡æ¡£æå‡ºé—®é¢˜ï¼Œå¹¶è·å–ç­”æ¡ˆã€‚
        æ”¯æŒé—®é¢˜æ”¹å†™åŠŸèƒ½å’ŒçŸ­æœŸè®°å¿†åŠŸèƒ½ï¼Œæé«˜æœç´¢è¦†ç›–é¢ã€‚

        Args:
            question: ç”¨æˆ·æå‡ºçš„é—®é¢˜å­—ç¬¦ä¸²ã€‚
            use_memory: æ˜¯å¦ä½¿ç”¨çŸ­æœŸè®°å¿†åŠŸèƒ½

        Returns:
            ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«'result' (ç­”æ¡ˆ) å’Œ 'source_documents' (å‚è€ƒçš„æ–‡æ¡£ç‰‡æ®µ)ã€‚
        """
        if not self.qa_chain:
            error_msg = "é”™è¯¯: é—®ç­”é“¾å°šæœªåˆå§‹åŒ–ã€‚è¯·å…ˆè°ƒç”¨ `sync_data_directory` æ–¹æ³•åŠ è½½æ–‡æ¡£ã€‚"
            return {
                "result": error_msg,
                "source_documents": []
            }
        
        print(f"\næ­£åœ¨å¤„ç†é—®é¢˜: '{question}'...")
        
        # è·å–çŸ­æœŸè®°å¿†ä¸Šä¸‹æ–‡
        memory_context = ""
        if use_memory and config.ENABLE_SHORT_TERM_MEMORY:
            memory_context = memory_manager.get_conversation_context(include_count=None)  # ä½¿ç”¨æ‰€æœ‰å¯¹è¯è½®æ¬¡
            if memory_context:
                print("--- çŸ­æœŸè®°å¿†ä¸Šä¸‹æ–‡ ---")
                print(f"åŒ…å«æœ€è¿‘ {len(memory_manager.get_recent_conversations(5))} è½®å¯¹è¯ä½œä¸ºä¸Šä¸‹æ–‡")
        
        # å¦‚æœå¯ç”¨äº†é—®é¢˜æ”¹å†™åŠŸèƒ½
        if config.ENABLE_QUERY_REWRITING:
            print("--- é—®é¢˜æ”¹å†™é˜¶æ®µ ---")
            
            # 1. æ”¹å†™é—®é¢˜
            rewritten_queries = self._rewrite_query(question)
            
            # 2. ä½¿ç”¨å¤šä¸ªé—®é¢˜è¿›è¡Œæ£€ç´¢
            print("--- å¤šæŸ¥è¯¢æ£€ç´¢é˜¶æ®µ ---")
            retrieved_docs = self._retrieve_with_multiple_queries(rewritten_queries)
            
            # 3. é‡æ’åº
            print("--- é‡æ’åºé˜¶æ®µ ---")
            if retrieved_docs and self.reranker:
                try:
                    # ä½¿ç”¨é‡æ’åºå™¨å¯¹æ‰€æœ‰æ£€ç´¢åˆ°çš„æ–‡æ¡£è¿›è¡Œé‡æ’åº
                    reranked_docs = self.reranker.compress_documents(retrieved_docs, question)
                    final_docs = reranked_docs[:config.RERANKER_TOP_N]
                    print(f"  - é‡æ’åºå®Œæˆï¼Œæœ€ç»ˆé€‰æ‹© {len(final_docs)} ä¸ªæœ€ç›¸å…³æ–‡æ¡£")
                except Exception as e:
                    print(f"  - é‡æ’åºå¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹æ£€ç´¢ç»“æœ")
                    final_docs = retrieved_docs[:config.RERANKER_TOP_N]
            else:
                final_docs = retrieved_docs[:config.RERANKER_TOP_N]
            
            # 4. ç”Ÿæˆç­”æ¡ˆ
            print("--- ç­”æ¡ˆç”Ÿæˆé˜¶æ®µ ---")
            if final_docs:
                # æ„å»ºä¸Šä¸‹æ–‡
                context = "\n\n".join([doc.page_content for doc in final_docs])
                
                # æ„å»ºå®Œæ•´çš„ä¸Šä¸‹æ–‡ï¼ˆåŒ…å«è®°å¿†ä¸Šä¸‹æ–‡å’Œæ£€ç´¢ä¸Šä¸‹æ–‡ï¼‰
                full_context = context
                if memory_context:
                    full_context = f"å¯¹è¯å†å²:\n{memory_context}\n\nå½“å‰æ£€ç´¢åˆ°çš„ç›¸å…³ä¿¡æ¯:\n{context}"
                
                # ä½¿ç”¨æç¤ºè¯ç®¡ç†å™¨è·å–é—®ç­”æç¤ºæ¨¡æ¿
                qa_template = get_qa_prompt_template()
                prompt = qa_template.format(context=full_context, question=question)
                response = self.llm.invoke(prompt)
                
                if hasattr(response, 'content'):
                    answer = response.content.strip()
                else:
                    answer = str(response).strip()
                
                # ä¿å­˜å¯¹è¯åˆ°çŸ­æœŸè®°å¿†
                if use_memory and config.ENABLE_SHORT_TERM_MEMORY:
                    memory_manager.add_conversation(
                        question=question,
                        answer=answer,
                        metadata={
                            "used_query_rewriting": True,
                            "memory_context_included": bool(memory_context),
                            "source_documents_count": len(final_docs)
                        }
                    )
                
                return {
                    "result": answer,
                    "source_documents": final_docs
                }
            else:
                no_result_answer = "æ ¹æ®æä¾›çš„èµ„æ–™ï¼Œæˆ‘æ— æ³•å›ç­”è¯¥é—®é¢˜ã€‚"
                
                # ä¿å­˜å¯¹è¯åˆ°çŸ­æœŸè®°å¿†
                if use_memory and config.ENABLE_SHORT_TERM_MEMORY:
                    memory_manager.add_conversation(
                        question=question,
                        answer=no_result_answer,
                        metadata={
                            "used_query_rewriting": True,
                            "memory_context_included": bool(memory_context),
                            "source_documents_count": 0,
                            "no_result": True
                        }
                    )
                
                return {
                    "result": no_result_answer,
                    "source_documents": []
                }
        else:
            # ä½¿ç”¨åŸå§‹çš„é—®ç­”é“¾ï¼Œä½†éœ€è¦æ‰‹åŠ¨å¤„ç†è®°å¿†ä¸Šä¸‹æ–‡
            print("--- åŸå§‹é—®ç­”é“¾æ¨¡å¼ ---")
            
            # å…ˆè·å–ç›¸å…³æ–‡æ¡£
            retriever = self.qa_chain.retriever
            retrieved_docs = retriever.get_relevant_documents(question)
            
            if retrieved_docs:
                # æ„å»ºä¸Šä¸‹æ–‡
                context = "\n\n".join([doc.page_content for doc in retrieved_docs])
                
                # æ„å»ºå®Œæ•´çš„ä¸Šä¸‹æ–‡ï¼ˆåŒ…å«è®°å¿†ä¸Šä¸‹æ–‡å’Œæ£€ç´¢ä¸Šä¸‹æ–‡ï¼‰
                full_context = context
                if memory_context:
                    full_context = f"å¯¹è¯å†å²:\n{memory_context}\n\nå½“å‰æ£€ç´¢åˆ°çš„ç›¸å…³ä¿¡æ¯:\n{context}"
                
                # ä½¿ç”¨æç¤ºè¯ç®¡ç†å™¨è·å–é—®ç­”æç¤ºæ¨¡æ¿
                qa_template = get_qa_prompt_template()
                prompt = qa_template.format(context=full_context, question=question)
                response = self.llm.invoke(prompt)
                
                if hasattr(response, 'content'):
                    answer = response.content.strip()
                else:
                    answer = str(response).strip()
                
                result = {
                    "result": answer,
                    "source_documents": retrieved_docs
                }
            else:
                no_result_answer = "æ ¹æ®æä¾›çš„èµ„æ–™ï¼Œæˆ‘æ— æ³•å›ç­”è¯¥é—®é¢˜ã€‚"
                result = {
                    "result": no_result_answer,
                    "source_documents": []
                }
            
            # ä¿å­˜å¯¹è¯åˆ°çŸ­æœŸè®°å¿†
            if use_memory and config.ENABLE_SHORT_TERM_MEMORY:
                memory_manager.add_conversation(
                    question=question,
                    answer=result.get("result", ""),
                    metadata={
                        "used_query_rewriting": False,
                        "memory_context_included": bool(memory_context),
                        "source_documents_count": len(result.get("source_documents", []))
                    }
                )
            
            return result

```

## `rag/prompt_manager.py`

```python
# rag/prompt_manager.py

import os
from pathlib import Path
from typing import Dict, Optional, Any
from langchain_core.prompts import PromptTemplate


class PromptManager:
    """
    æç¤ºè¯ç®¡ç†å™¨ï¼Œè´Ÿè´£åŠ è½½å’Œç®¡ç†æ‰€æœ‰æç¤ºè¯æ¨¡æ¿ã€‚
    å®ç°æç¤ºè¯ä¸ä»£ç çš„è§£è€¦ã€‚
    """
    
    def __init__(self):
        """åˆå§‹åŒ–æç¤ºè¯ç®¡ç†å™¨ã€‚"""
        self.prompts_dir = Path(__file__).parent / "prompts"
        self._prompt_cache: Dict[str, str] = {}
        self._template_cache: Dict[str, PromptTemplate] = {}
        
        # ç¡®ä¿æç¤ºè¯ç›®å½•å­˜åœ¨
        self.prompts_dir.mkdir(exist_ok=True)
    
    def load_prompt(self, prompt_name: str) -> str:
        """
        åŠ è½½æŒ‡å®šçš„æç¤ºè¯å†…å®¹ã€‚
        
        Args:
            prompt_name: æç¤ºè¯æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
            
        Returns:
            æç¤ºè¯å†…å®¹å­—ç¬¦ä¸²
            
        Raises:
            FileNotFoundError: å¦‚æœæç¤ºè¯æ–‡ä»¶ä¸å­˜åœ¨
        """
        # æ£€æŸ¥ç¼“å­˜
        if prompt_name in self._prompt_cache:
            return self._prompt_cache[prompt_name]
        
        # æ„å»ºæ–‡ä»¶è·¯å¾„
        prompt_file = self.prompts_dir / f"{prompt_name}.txt"
        
        if not prompt_file.exists():
            raise FileNotFoundError(f"æç¤ºè¯æ–‡ä»¶ä¸å­˜åœ¨: {prompt_file}")
        
        # è¯»å–æ–‡ä»¶å†…å®¹
        try:
            with open(prompt_file, 'r', encoding='utf-8') as f:
                content = f.read().strip()
            
            # ç¼“å­˜å†…å®¹
            self._prompt_cache[prompt_name] = content
            return content
            
        except Exception as e:
            raise RuntimeError(f"è¯»å–æç¤ºè¯æ–‡ä»¶å¤±è´¥ {prompt_file}: {e}")
    
    def get_template(self, prompt_name: str) -> PromptTemplate:
        """
        è·å–æŒ‡å®šçš„æç¤ºè¯æ¨¡æ¿å¯¹è±¡ã€‚
        
        Args:
            prompt_name: æç¤ºè¯æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
            
        Returns:
            LangChain PromptTemplate å¯¹è±¡
        """
        # æ£€æŸ¥ç¼“å­˜
        if prompt_name in self._template_cache:
            return self._template_cache[prompt_name]
        
        # åŠ è½½æç¤ºè¯å†…å®¹
        prompt_content = self.load_prompt(prompt_name)
        
        # åˆ›å»ºæ¨¡æ¿å¯¹è±¡
        template = PromptTemplate.from_template(prompt_content)
        
        # ç¼“å­˜æ¨¡æ¿
        self._template_cache[prompt_name] = template
        return template
    
    def reload_prompt(self, prompt_name: str) -> str:
        """
        é‡æ–°åŠ è½½æŒ‡å®šçš„æç¤ºè¯ï¼ˆæ¸…é™¤ç¼“å­˜åé‡æ–°è¯»å–ï¼‰ã€‚
        
        Args:
            prompt_name: æç¤ºè¯æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
            
        Returns:
            æç¤ºè¯å†…å®¹å­—ç¬¦ä¸²
        """
        # æ¸…é™¤ç¼“å­˜
        self._prompt_cache.pop(prompt_name, None)
        self._template_cache.pop(prompt_name, None)
        
        # é‡æ–°åŠ è½½
        return self.load_prompt(prompt_name)
    
    def list_available_prompts(self) -> list:
        """
        åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æç¤ºè¯æ–‡ä»¶ã€‚
        
        Returns:
            æç¤ºè¯æ–‡ä»¶ååˆ—è¡¨ï¼ˆä¸å«æ‰©å±•åï¼‰
        """
        prompt_files = []
        # ä½¿ç”¨ pathlib.Path.glob() æ–¹æ³• (æ¨è)
        for file_path in self.prompts_dir.glob("*.txt"):
            prompt_files.append(file_path.stem)  # .stem è·å–ä¸å«æ‰©å±•åçš„æ–‡ä»¶å
        return sorted(prompt_files)
        
        # å¦‚æœä½¿ç”¨æ ‡å‡†åº“ glob çš„ç­‰ä»·å†™æ³•ï¼š
        # import glob
        # pattern = str(self.prompts_dir / "*.txt")
        # for file_path in glob.glob(pattern):
        #     filename = os.path.splitext(os.path.basename(file_path))[0]
        #     prompt_files.append(filename)
    
    def save_prompt(self, prompt_name: str, content: str) -> None:
        """
        ä¿å­˜æç¤ºè¯åˆ°æ–‡ä»¶ã€‚
        
        Args:
            prompt_name: æç¤ºè¯æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
            content: æç¤ºè¯å†…å®¹
        """
        prompt_file = self.prompts_dir / f"{prompt_name}.txt"
        
        try:
            with open(prompt_file, 'w', encoding='utf-8') as f:
                f.write(content.strip())
            
            # æ¸…é™¤ç¼“å­˜ï¼Œç¡®ä¿ä¸‹æ¬¡åŠ è½½æ—¶ä½¿ç”¨æ–°å†…å®¹
            self._prompt_cache.pop(prompt_name, None)
            self._template_cache.pop(prompt_name, None)
            
            print(f"æç¤ºè¯å·²ä¿å­˜åˆ°: {prompt_file}")
            
        except Exception as e:
            raise RuntimeError(f"ä¿å­˜æç¤ºè¯æ–‡ä»¶å¤±è´¥ {prompt_file}: {e}")
    
    def clear_cache(self) -> None:
        """æ¸…é™¤æ‰€æœ‰ç¼“å­˜ã€‚"""
        self._prompt_cache.clear()
        self._template_cache.clear()
        print("æç¤ºè¯ç¼“å­˜å·²æ¸…é™¤")
    
    def reload_all_prompts(self) -> Dict[str, str]:
        """
        é‡æ–°åŠ è½½æ‰€æœ‰æç¤ºè¯ã€‚
        
        Returns:
            é‡æ–°åŠ è½½çš„æç¤ºè¯å­—å…¸
        """
        # æ¸…é™¤æ‰€æœ‰ç¼“å­˜
        self.clear_cache()
        
        # é‡æ–°åŠ è½½æ‰€æœ‰æç¤ºè¯
        reloaded_prompts = {}
        for prompt_name in self.list_available_prompts():
            try:
                content = self.load_prompt(prompt_name)
                reloaded_prompts[prompt_name] = content
                print(f"âœ… é‡æ–°åŠ è½½: {prompt_name}")
            except Exception as e:
                print(f"âŒ é‡æ–°åŠ è½½å¤±è´¥ {prompt_name}: {e}")
        
        return reloaded_prompts
    
    def get_prompt_info(self, prompt_name: str) -> Dict[str, Any]:
        """
        è·å–æç¤ºè¯çš„è¯¦ç»†ä¿¡æ¯ã€‚
        
        Args:
            prompt_name: æç¤ºè¯æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
            
        Returns:
            æç¤ºè¯ä¿¡æ¯å­—å…¸
        """
        prompt_file = self.prompts_dir / f"{prompt_name}.txt"
        
        if not prompt_file.exists():
            return {"exists": False, "error": f"æç¤ºè¯æ–‡ä»¶ä¸å­˜åœ¨: {prompt_file}"}
        
        try:
            stat = prompt_file.stat()
            content = self.load_prompt(prompt_name)
            template = self.get_template(prompt_name)
            
            return {
                "exists": True,
                "file_path": str(prompt_file),
                "file_size": stat.st_size,
                "modified_time": stat.st_mtime,
                "content_length": len(content),
                "content_preview": content[:100] + "..." if len(content) > 100 else content,
                "template_variables": template.input_variables,
                "is_cached": prompt_name in self._prompt_cache
            }
        except Exception as e:
            return {"exists": True, "error": f"è·å–æç¤ºè¯ä¿¡æ¯å¤±è´¥: {e}"}
    
    def validate_prompt(self, prompt_name: str) -> Dict[str, Any]:
        """
        éªŒè¯æç¤ºè¯æ¨¡æ¿çš„æœ‰æ•ˆæ€§ã€‚
        
        Args:
            prompt_name: æç¤ºè¯æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
            
        Returns:
            éªŒè¯ç»“æœå­—å…¸
        """
        try:
            template = self.get_template(prompt_name)
            
            # æ£€æŸ¥å¿…éœ€çš„å˜é‡
            required_vars = {"context", "question"}  # é—®ç­”æç¤ºè¯çš„å¿…éœ€å˜é‡
            missing_vars = required_vars - set(template.input_variables)
            extra_vars = set(template.input_variables) - required_vars
            
            # å°è¯•æ ¼å¼åŒ–æµ‹è¯•
            test_values = {var: f"test_{var}" for var in template.input_variables}
            try:
                formatted = template.format(**test_values)
                format_test = {"success": True, "formatted_length": len(formatted)}
            except Exception as e:
                format_test = {"success": False, "error": str(e)}
            
            return {
                "valid": len(missing_vars) == 0 and format_test["success"],
                "template_variables": template.input_variables,
                "missing_variables": list(missing_vars),
                "extra_variables": list(extra_vars),
                "format_test": format_test
            }
            
        except Exception as e:
            return {
                "valid": False,
                "error": f"éªŒè¯æç¤ºè¯å¤±è´¥: {e}"
            }


# åˆ›å»ºå…¨å±€æç¤ºè¯ç®¡ç†å™¨å®ä¾‹
prompt_manager = PromptManager()


def get_qa_prompt_template() -> PromptTemplate:
    """è·å–é—®ç­”æç¤ºè¯æ¨¡æ¿ã€‚"""
    return prompt_manager.get_template("qa_prompt")


def get_query_rewrite_prompt_template() -> PromptTemplate:
    """è·å–é—®é¢˜æ”¹å†™æç¤ºè¯æ¨¡æ¿ã€‚"""
    return prompt_manager.get_template("query_rewrite_prompt")


def load_qa_prompt() -> str:
    """åŠ è½½é—®ç­”æç¤ºè¯å†…å®¹ã€‚"""
    return prompt_manager.load_prompt("qa_prompt")


def load_query_rewrite_prompt() -> str:
    """åŠ è½½é—®é¢˜æ”¹å†™æç¤ºè¯å†…å®¹ã€‚"""
    return prompt_manager.load_prompt("query_rewrite_prompt")
```

## `rag/prompts/prompt_README.md`

```markdown
# æç¤ºè¯ç®¡ç†ç³»ç»Ÿ

æœ¬ç›®å½•åŒ…å«äº†RAGç³»ç»Ÿä¸­ä½¿ç”¨çš„æ‰€æœ‰æç¤ºè¯æ¨¡æ¿ï¼Œå®ç°äº†æç¤ºè¯ä¸ä»£ç çš„è§£è€¦ã€‚

## æ–‡ä»¶ç»“æ„

```
prompts/
â”œâ”€â”€ README.md                 # æœ¬è¯´æ˜æ–‡ä»¶
â”œâ”€â”€ qa_prompt.txt            # é—®ç­”æç¤ºè¯æ¨¡æ¿
â””â”€â”€ query_rewrite_prompt.txt # é—®é¢˜æ”¹å†™æç¤ºè¯æ¨¡æ¿
```

## æç¤ºè¯æ–‡ä»¶è¯´æ˜

### qa_prompt.txt
ç”¨äºæŒ‡å¯¼LLMå¦‚ä½•åŸºäºæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

**å˜é‡:**
- `{context}`: æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£å†…å®¹
- `{question}`: ç”¨æˆ·æå‡ºçš„é—®é¢˜

### query_rewrite_prompt.txt
ç”¨äºæŒ‡å¯¼LLMå¦‚ä½•å°†ç”¨æˆ·çš„åŸå§‹é—®é¢˜æ”¹å†™æˆå¤šä¸ªç›¸å…³é—®é¢˜ï¼Œä»¥æé«˜æ£€ç´¢è¦†ç›–é¢ã€‚

**å˜é‡:**
- `{original_query}`: ç”¨æˆ·çš„åŸå§‹é—®é¢˜
- `{count}`: éœ€è¦ç”Ÿæˆçš„æ”¹å†™é—®é¢˜æ•°é‡

## ä½¿ç”¨æ–¹æ³•

### åœ¨ä»£ç ä¸­ä½¿ç”¨æç¤ºè¯ç®¡ç†å™¨

```python
from rag.prompt_manager import (
    get_qa_prompt_template,
    get_query_rewrite_prompt_template,
    load_qa_prompt,
    load_query_rewrite_prompt
)

# è·å–æç¤ºè¯æ¨¡æ¿å¯¹è±¡
qa_template = get_qa_prompt_template()
rewrite_template = get_query_rewrite_prompt_template()

# æ ¼å¼åŒ–æç¤ºè¯
formatted_prompt = qa_template.format(
    context="ç›¸å…³æ–‡æ¡£å†…å®¹",
    question="ç”¨æˆ·é—®é¢˜"
)

# ç›´æ¥åŠ è½½æç¤ºè¯å†…å®¹
qa_prompt_content = load_qa_prompt()
```

### ä¿®æ”¹æç¤ºè¯

1. ç›´æ¥ç¼–è¾‘å¯¹åº”çš„ `.txt` æ–‡ä»¶
2. ä¿å­˜æ–‡ä»¶åï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨ä½¿ç”¨æ–°çš„æç¤ºè¯å†…å®¹
3. å¦‚æœéœ€è¦ç«‹å³ç”Ÿæ•ˆï¼Œå¯ä»¥è°ƒç”¨ `prompt_manager.reload_prompt(prompt_name)`

### æ·»åŠ æ–°çš„æç¤ºè¯

1. åœ¨ `prompts/` ç›®å½•ä¸‹åˆ›å»ºæ–°çš„ `.txt` æ–‡ä»¶
2. åœ¨ `prompt_manager.py` ä¸­æ·»åŠ å¯¹åº”çš„è¾…åŠ©å‡½æ•°
3. åœ¨éœ€è¦ä½¿ç”¨çš„åœ°æ–¹å¯¼å…¥å¹¶ä½¿ç”¨

## ä¼˜åŠ¿

1. **è§£è€¦æ€§**: æç¤ºè¯ä¸ä»£ç åˆ†ç¦»ï¼Œä¾¿äºç»´æŠ¤å’Œä¿®æ”¹
2. **å¯ç»´æŠ¤æ€§**: é›†ä¸­ç®¡ç†æ‰€æœ‰æç¤ºè¯ï¼Œä¾¿äºç‰ˆæœ¬æ§åˆ¶å’Œåä½œ
3. **æ€§èƒ½ä¼˜åŒ–**: å†…ç½®ç¼“å­˜æœºåˆ¶ï¼Œé¿å…é‡å¤è¯»å–æ–‡ä»¶
4. **çµæ´»æ€§**: æ”¯æŒåŠ¨æ€é‡æ–°åŠ è½½ï¼Œä¾¿äºè°ƒè¯•å’Œä¼˜åŒ–
5. **ä¸€è‡´æ€§**: ç»Ÿä¸€çš„æ¥å£å’Œä½¿ç”¨æ–¹å¼

## æœ€ä½³å®è·µ

1. **å‘½åè§„èŒƒ**: ä½¿ç”¨æè¿°æ€§çš„æ–‡ä»¶åï¼Œå¦‚ `qa_prompt.txt`ã€`summary_prompt.txt`
2. **å˜é‡æ ‡è®°**: ä½¿ç”¨ `{variable_name}` æ ¼å¼æ ‡è®°æ¨¡æ¿å˜é‡
3. **æ–‡æ¡£æ³¨é‡Š**: åœ¨æç¤ºè¯æ–‡ä»¶å¼€å¤´æ·»åŠ æ³¨é‡Šè¯´æ˜ç”¨é€”å’Œå˜é‡
4. **ç‰ˆæœ¬æ§åˆ¶**: å°†æç¤ºè¯æ–‡ä»¶çº³å…¥ç‰ˆæœ¬æ§åˆ¶ï¼Œè·Ÿè¸ªå˜æ›´å†å²
5. **æµ‹è¯•éªŒè¯**: ä¿®æ”¹æç¤ºè¯åè¿›è¡Œå……åˆ†æµ‹è¯•ï¼Œç¡®ä¿æ•ˆæœç¬¦åˆé¢„æœŸ

## ç¤ºä¾‹

### åˆ›å»ºæ–°çš„æç¤ºè¯æ–‡ä»¶

```bash
# åˆ›å»ºæ‘˜è¦æç¤ºè¯æ–‡ä»¶
echo "è¯·å¯¹ä»¥ä¸‹å†…å®¹è¿›è¡Œç®€æ´çš„æ‘˜è¦ï¼š

{content}

æ‘˜è¦ï¼š" > prompts/summary_prompt.txt
```

### åœ¨ä»£ç ä¸­ä½¿ç”¨æ–°æç¤ºè¯

```python
# åœ¨ prompt_manager.py ä¸­æ·»åŠ 
def get_summary_prompt_template():
    return prompt_manager.get_template("summary_prompt")

# åœ¨ä¸šåŠ¡ä»£ç ä¸­ä½¿ç”¨
from rag.prompt_manager import get_summary_prompt_template

summary_template = get_summary_prompt_template()
formatted_prompt = summary_template.format(content="è¦æ‘˜è¦çš„å†…å®¹")
```
```

## `rag/prompts/qa_prompt.txt`

```
è¯·ä½ æ‰®æ¼”ä¸€ä¸ªä¸¥è°¨çš„æ–‡æ¡£é—®ç­”æœºå™¨äººã€‚
è¯·ä¸¥æ ¼æ ¹æ®ä¸‹é¢æä¾›çš„"ä¸Šä¸‹æ–‡ä¿¡æ¯"æ¥å›ç­”"é—®é¢˜"ã€‚åœ¨ç­”æ¡ˆå‰åŠ ä¸Š"æ ¹æ®çŸ¥è¯†åº“èµ„æ–™ï¼š"
å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰è¶³å¤Ÿçš„ä¿¡æ¯æ¥å›ç­”é—®é¢˜ï¼Œè¯·ç›´æ¥è¯´ï¼š"æ ¹æ®æä¾›çš„èµ„æ–™ï¼Œæˆ‘æ— æ³•å›ç­”è¯¥é—®é¢˜ã€‚"
ä¸å…è®¸ç¼–é€ æˆ–æ·»åŠ ä¸Šä¸‹æ–‡ä¹‹å¤–çš„ä»»ä½•ä¿¡æ¯ã€‚

---
ä¸Šä¸‹æ–‡ä¿¡æ¯:
{context}
---

é—®é¢˜: {question}

å›ç­”:
```

## `rag/prompts/query_rewrite_prompt.txt`

```
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é—®é¢˜æ”¹å†™åŠ©æ‰‹ã€‚è¯·å°†ç”¨æˆ·çš„é—®é¢˜æ”¹å†™æˆ{count}ä¸ªä¸åŒè§’åº¦ä½†ç›¸å…³çš„é—®é¢˜ï¼Œä»¥æé«˜ä¿¡æ¯æ£€ç´¢çš„è¦†ç›–é¢ã€‚

è¦æ±‚ï¼š
1. ä¿æŒé—®é¢˜çš„æ ¸å¿ƒæ„å›¾ä¸å˜
2. ä»ä¸åŒè§’åº¦æˆ–å±‚é¢æ¥è¡¨è¾¾åŒä¸€ä¸ªéœ€æ±‚
3. ä½¿ç”¨ä¸åŒçš„å…³é”®è¯å’Œè¡¨è¾¾æ–¹å¼
4. æ¯ä¸ªé—®é¢˜éƒ½åº”è¯¥æ˜¯å®Œæ•´ã€æ¸…æ™°çš„
5. é—®é¢˜ä¹‹é—´è¦æœ‰ä¸€å®šçš„å·®å¼‚æ€§

åŸå§‹é—®é¢˜ï¼š{original_query}

è¯·ç”Ÿæˆ{count}ä¸ªæ”¹å†™é—®é¢˜ï¼Œæ¯è¡Œä¸€ä¸ªé—®é¢˜ï¼Œä¸è¦æ·»åŠ ç¼–å·æˆ–å…¶ä»–æ ¼å¼ï¼š
```

## `rag/streaming_pipeline.py`

```python
# rag/streaming_pipeline_v2.py - æ­£ç¡®çš„æµå¼å“åº”å®ç°

import asyncio
import time
from typing import AsyncGenerator, Dict, Any, List, Optional
from dataclasses import dataclass
from enum import Enum

# ç»§æ‰¿å¼‚æ­¥RAGæµç¨‹
from .async_pipeline import AsyncRagPipeline
from . import config
# å¯¼å…¥æç¤ºè¯ç®¡ç†å™¨
from .prompt_manager import get_qa_prompt_template, get_query_rewrite_prompt_template
# å¯¼å…¥çŸ­æœŸè®°å¿†ç®¡ç†å™¨
from .memory_manager import memory_manager

# å¯¼å…¥éœ€è¦çš„ç»„ä»¶
from langchain_core.documents import Document


class StreamEventType(Enum):
    """æµå¼äº‹ä»¶ç±»å‹ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
    PROCESSING = "processing"           # å¤„ç†çŠ¶æ€æ›´æ–°
    GENERATION_START = "generation_start"  # å¼€å§‹ç”Ÿæˆç­”æ¡ˆ
    GENERATION_CHUNK = "generation_chunk"  # ç­”æ¡ˆç‰‡æ®µ
    GENERATION_END = "generation_end"      # ç”Ÿæˆå®Œæˆ
    ERROR = "error"                        # é”™è¯¯
    COMPLETE = "complete"                  # å®Œæˆ


@dataclass
class StreamEvent:
    """æµå¼äº‹ä»¶æ•°æ®ç»“æ„"""
    type: StreamEventType
    data: Any
    timestamp: float
    metadata: Optional[Dict[str, Any]] = None

    def to_dict(self) -> Dict[str, Any]:
        return {
            "type": self.type.value,
            "data": self.data,
            "timestamp": self.timestamp,
            "metadata": self.metadata or {}
        }


class StreamingRagPipeline(AsyncRagPipeline):
    """
    æ­£ç¡®çš„æµå¼å“åº”RAGç³»ç»Ÿ
    - ä¸­é—´å¤„ç†è¿‡ç¨‹ä¸æµå¼ï¼ŒåªåšçŠ¶æ€é€šçŸ¥
    - åªæœ‰æœ€ç»ˆç­”æ¡ˆç”Ÿæˆæ˜¯çœŸæ­£çš„æµå¼è¾“å‡º
    """
    
    def __init__(self):
        print("æ­£åœ¨åˆå§‹åŒ–æµå¼RAGç³»ç»Ÿ...")
        super().__init__()
        print("æµå¼RAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆã€‚")
    
    async def ask_stream(self, question: str, use_memory: bool = True) -> AsyncGenerator[StreamEvent, None]:
        """
        æµå¼é—®ç­” - åªæœ‰ç­”æ¡ˆç”Ÿæˆæ˜¯æµå¼çš„ï¼Œæ”¯æŒçŸ­æœŸè®°å¿†åŠŸèƒ½
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            use_memory: æ˜¯å¦ä½¿ç”¨çŸ­æœŸè®°å¿†åŠŸèƒ½
            
        Yields:
            StreamEvent: æµå¼äº‹ä»¶
        """
        if not self.qa_chain:
            yield StreamEvent(
                type=StreamEventType.ERROR,
                data={"error": "é—®ç­”é“¾å°šæœªåˆå§‹åŒ–"},
                timestamp=time.time()
            )
            return
        
        try:
            # 1. å¤„ç†é˜¶æ®µ - å†…éƒ¨å¤„ç†ï¼Œåªå‘é€çŠ¶æ€æ›´æ–°
            yield StreamEvent(
                type=StreamEventType.PROCESSING,
                data={"message": "æ­£åœ¨å¤„ç†æ‚¨çš„é—®é¢˜..."},
                timestamp=time.time()
            )
            
            # å†…éƒ¨å¤„ç†ï¼šé—®é¢˜æ”¹å†™ã€æ£€ç´¢ã€é‡æ’åºï¼ˆéæµå¼ï¼‰
            if config.ENABLE_QUERY_REWRITING:
                # é—®é¢˜æ”¹å†™
                rewritten_queries = await self._rewrite_query_async(question)
                
                # æ£€ç´¢
                retrieved_docs = await self._retrieve_with_multiple_queries_async(rewritten_queries)
                
                # é‡æ’åº
                if retrieved_docs and self.reranker:
                    try:
                        reranked_docs = await self._run_in_executor(
                            self.reranker.compress_documents, retrieved_docs, question
                        )
                        final_docs = reranked_docs[:config.RERANKER_TOP_N]
                    except Exception:
                        final_docs = retrieved_docs[:config.RERANKER_TOP_N]
                else:
                    final_docs = retrieved_docs[:config.RERANKER_TOP_N]
            else:
                # âœ… ä½¿ç”¨å¼‚æ­¥æ£€ç´¢åŠŸèƒ½ï¼Œé¿å…è°ƒç”¨LLM
                retriever = self.qa_chain.retriever
                
                # ä½¿ç”¨å¼‚æ­¥æ£€ç´¢æ–¹æ³•
                if hasattr(retriever, 'ainvoke'):
                    final_docs = await retriever.ainvoke(question)
                elif hasattr(retriever, 'aget_relevant_documents'):
                    final_docs = await retriever.aget_relevant_documents(question)
                else:
                    # å¦‚æœæ²¡æœ‰å¼‚æ­¥æ–¹æ³•ï¼Œå›é€€åˆ°çº¿ç¨‹æ± 
                    final_docs = await self._run_in_executor(
                        retriever.get_relevant_documents, question
                    )
                
                # ä½¿ç”¨çœŸæ­£çš„æµå¼ç”Ÿæˆï¼ˆLLMåªè¢«è°ƒç”¨ä¸€æ¬¡ï¼Œä¸”æ˜¯æµå¼çš„ï¼‰
                # if final_docs:
                #     async for event in self._generate_streaming_answer(question, final_docs):
                #         yield event
                # else:
                #     async for event in self._stream_existing_answer("æ ¹æ®æä¾›çš„èµ„æ–™ï¼Œæˆ‘æ— æ³•æ‰¾åˆ°ç›¸å…³ä¿¡æ¯æ¥å›ç­”è¯¥é—®é¢˜ã€‚"):
                #         yield event

                # yield StreamEvent(
                #         type=StreamEventType.COMPLETE,
                #         data={"message": "å›ç­”å®Œæˆ"},
                #         timestamp=time.time()
                #     )
                # return
            
            # 2. æµå¼ç”Ÿæˆé˜¶æ®µ - è¿™é‡Œæ‰æ˜¯çœŸæ­£çš„æµå¼
            if final_docs:
                # å…ˆå°è¯•åŸºäºæ–‡æ¡£ç”Ÿæˆç­”æ¡ˆ
                knowledge_base_answer = ""
                answer_events = []
                
                async for event in self._generate_streaming_answer(question, final_docs, use_memory):
                    answer_events.append(event)
                    if event.type.value == "generation_chunk":
                        knowledge_base_answer += event.data.get("chunk", "")
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯"æ— æ³•å›ç­”"çš„å›å¤
                if "æ ¹æ®æä¾›çš„èµ„æ–™ï¼Œæˆ‘æ— æ³•å›ç­”è¯¥é—®é¢˜" in knowledge_base_answer:
                    # çŸ¥è¯†åº“æ–‡æ¡£ä¸ç›¸å…³ï¼Œä½¿ç”¨å¤§æ¨¡å‹è‡ªèº«çŸ¥è¯†
                    async for event in self._stream_no_result_answer(question, use_memory):
                        yield event
                else:
                    # çŸ¥è¯†åº“æ–‡æ¡£ç›¸å…³ï¼Œè¾“å‡ºä¹‹å‰æ”¶é›†çš„äº‹ä»¶
                    for event in answer_events:
                        yield event
            else:
                # æ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£
                async for event in self._stream_no_result_answer(question, use_memory):
                    yield event
            
            # 3. å®Œæˆ
            yield StreamEvent(
                type=StreamEventType.COMPLETE,
                data={"message": "å›ç­”å®Œæˆ"},
                timestamp=time.time()
            )
            
        except Exception as e:
            yield StreamEvent(
                type=StreamEventType.ERROR,
                data={"error": str(e)},
                timestamp=time.time()
            )
    
    async def ask_with_categories_stream(self, question: str, categories: List[str] = None) -> AsyncGenerator[StreamEvent, None]:
        """
        åˆ†ç±»æµå¼é—®ç­”
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            categories: æŒ‡å®šç±»åˆ«
            
        Yields:
            StreamEvent: æµå¼äº‹ä»¶
        """
        if not self.qa_chain:
            yield StreamEvent(
                type=StreamEventType.ERROR,
                data={"error": "é—®ç­”é“¾å°šæœªåˆå§‹åŒ–"},
                timestamp=time.time()
            )
            return
        
        if categories is None:
            categories = config.DEFAULT_SEARCH_CATEGORIES
        
        try:
            # å¤„ç†é˜¶æ®µ - å†…éƒ¨å¤„ç†
            yield StreamEvent(
                type=StreamEventType.PROCESSING,
                data={"message": f"æ­£åœ¨å¤„ç†æ‚¨çš„é—®é¢˜ï¼ˆç±»åˆ«: {categories or 'æ‰€æœ‰'})..."},
                timestamp=time.time()
            )
            
            # å†…éƒ¨å¤„ç†ï¼šåˆ†ç±»æ£€ç´¢ç­‰
            if config.ENABLE_QUERY_REWRITING:
                rewritten_queries = await self._rewrite_query_async(question)
                retrieved_docs = await self._retrieve_with_multiple_queries_and_categories_async(rewritten_queries, categories)
                
                if retrieved_docs and self.reranker:
                    try:
                        reranked_docs = await self._run_in_executor(
                            self.reranker.compress_documents, retrieved_docs, question
                        )
                        final_docs = reranked_docs[:config.RERANKER_TOP_N]
                    except Exception:
                        final_docs = retrieved_docs[:config.RERANKER_TOP_N]
                else:
                    final_docs = retrieved_docs[:config.RERANKER_TOP_N]
            else:
                # âœ… æ”¹è¿›ï¼šä½¿ç”¨åˆ†ç±»æ£€ç´¢ä½†ä»ç„¶ä½¿ç”¨çœŸæ­£çš„æµå¼ç”Ÿæˆ
                if categories:
                    # è·å–åˆ†ç±»ç›¸å…³çš„æ–‡æ¡£
                    def get_category_docs():
                        category_retriever = self._build_category_retriever(categories)
                        if self.reranker:
                            from langchain.retrievers import ContextualCompressionRetriever
                            compression_retriever = ContextualCompressionRetriever(
                                base_compressor=self.reranker,
                                base_retriever=category_retriever
                            )
                            return compression_retriever.get_relevant_documents(question)
                        else:
                            return category_retriever.get_relevant_documents(question)
                    
                    final_docs = await self._run_in_executor(get_category_docs)
                    
                    # ä½¿ç”¨çœŸæ­£çš„æµå¼ç”Ÿæˆ
                    # if category_docs:
                    #     async for event in self._generate_streaming_answer(question, category_docs):
                    #         yield event
                    # else:
                    #     async for event in self._stream_existing_answer("æ ¹æ®æä¾›çš„èµ„æ–™ï¼Œæˆ‘æ— æ³•æ‰¾åˆ°ç›¸å…³ä¿¡æ¯æ¥å›ç­”è¯¥é—®é¢˜ã€‚"):
                    #         yield event

                    # yield StreamEvent(
                    #         type=StreamEventType.COMPLETE,
                    #         data={"message": "å›ç­”å®Œæˆ"},
                    #         timestamp=time.time()
                    #     )
                    # return
                else:
                    # âœ… ä½¿ç”¨å¼‚æ­¥æ£€ç´¢åŠŸèƒ½ï¼Œé¿å…è°ƒç”¨LLM
                    retriever = self.qa_chain.retriever
                    
                    # ä½¿ç”¨å¼‚æ­¥æ£€ç´¢æ–¹æ³•
                    if hasattr(retriever, 'ainvoke'):
                        final_docs = await retriever.ainvoke(question)
                    elif hasattr(retriever, 'aget_relevant_documents'):
                        final_docs = await retriever.aget_relevant_documents(question)
                    else:
                        # å¦‚æœæ²¡æœ‰å¼‚æ­¥æ–¹æ³•ï¼Œå›é€€åˆ°çº¿ç¨‹æ± 
                        final_docs = await self._run_in_executor(
                            retriever.get_relevant_documents, question
                        )
                    
                    # ä½¿ç”¨çœŸæ­£çš„æµå¼ç”Ÿæˆï¼ˆLLMåªè¢«è°ƒç”¨ä¸€æ¬¡ï¼Œä¸”æ˜¯æµå¼çš„ï¼‰
                    # if final_docs:
                    #     async for event in self._generate_streaming_answer(question, final_docs):
                    #         yield event
                    # else:
                    #     async for event in self._stream_existing_answer("æ ¹æ®æä¾›çš„èµ„æ–™ï¼Œæˆ‘æ— æ³•æ‰¾åˆ°ç›¸å…³ä¿¡æ¯æ¥å›ç­”è¯¥é—®é¢˜ã€‚"):
                    #         yield event
                    
                    # yield StreamEvent(
                    #         type=StreamEventType.COMPLETE,
                    #         data={"message": "å›ç­”å®Œæˆ"},
                    #         timestamp=time.time()
                    #     )
                    # return
            
            # æµå¼ç”Ÿæˆç­”æ¡ˆ
            if final_docs:
                async for event in self._generate_streaming_answer(question, final_docs):
                    yield event
            else:
                async for event in self._stream_no_result_answer(question, True):
                    yield event
            
            yield StreamEvent(
                type=StreamEventType.COMPLETE,
                data={"message": "å›ç­”å®Œæˆ"},
                timestamp=time.time()
            )
            
        except Exception as e:
            yield StreamEvent(
                type=StreamEventType.ERROR,
                data={"error": str(e)},
                timestamp=time.time()
            )
    
    async def _generate_streaming_answer(self, question: str, documents: List[Document], use_memory: bool = True) -> AsyncGenerator[StreamEvent, None]:
        """
        ç”Ÿæˆæµå¼ç­”æ¡ˆ - åŸºäºçŸ¥è¯†åº“æ–‡æ¡£çš„å›ç­”ï¼Œæ”¯æŒçŸ­æœŸè®°å¿†åŠŸèƒ½
        
        Args:
            question: é—®é¢˜
            documents: ç›¸å…³æ–‡æ¡£
            use_memory: æ˜¯å¦ä½¿ç”¨çŸ­æœŸè®°å¿†åŠŸèƒ½
            
        Yields:
            StreamEvent: æµå¼äº‹ä»¶
        """
        yield StreamEvent(
            type=StreamEventType.GENERATION_START,
            data={"message": "åŸºäºçŸ¥è¯†åº“æ–‡æ¡£ç”Ÿæˆç­”æ¡ˆ"},
            timestamp=time.time()
        )
        
        try:
            # æ„å»ºä¸Šä¸‹æ–‡
            context = "\n\n".join([doc.page_content for doc in documents])
            
            # è·å–çŸ­æœŸè®°å¿†ä¸Šä¸‹æ–‡
            memory_context = ""
            if use_memory and config.ENABLE_SHORT_TERM_MEMORY:
                memory_context = memory_manager.get_conversation_context(include_count=None)  # ä½¿ç”¨æ‰€æœ‰å¯¹è¯è½®æ¬¡
            
            # æ„å»ºå®Œæ•´çš„ä¸Šä¸‹æ–‡ï¼ˆåŒ…å«è®°å¿†ä¸Šä¸‹æ–‡å’Œæ£€ç´¢ä¸Šä¸‹æ–‡ï¼‰
            full_context = context
            if memory_context:
                full_context = f"å¯¹è¯å†å²:\n{memory_context}\n\nå½“å‰æ£€ç´¢åˆ°çš„ç›¸å…³ä¿¡æ¯:\n{context}"
            
            # æ„å»ºæç¤º - ä½¿ç”¨æç¤ºè¯æ¨¡æ¿
            try:
                qa_template = get_qa_prompt_template()
                knowledge_base_prompt = qa_template.format(
                    context=full_context,
                    question=question
                )
            except Exception:
                # å¦‚æœæç¤ºè¯æ¨¡æ¿åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æç¤ºè¯
                knowledge_base_prompt = f"""è¯·ä¸¥æ ¼æ ¹æ®ä¸‹é¢æä¾›çš„"ä¸Šä¸‹æ–‡ä¿¡æ¯"æ¥å›ç­”"é—®é¢˜"ã€‚
                    è¯·åœ¨å›ç­”å‰åŠ ä¸Š"æ ¹æ®çŸ¥è¯†åº“èµ„æ–™ï¼š"
                    å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰è¶³å¤Ÿçš„ä¿¡æ¯æ¥å›ç­”é—®é¢˜ï¼Œè¯·ç›´æ¥è¯´ï¼š"æ ¹æ®æä¾›çš„èµ„æ–™ï¼Œæˆ‘æ— æ³•å›ç­”è¯¥é—®é¢˜ã€‚"
                    ä¸å…è®¸ç¼–é€ æˆ–æ·»åŠ ä¸Šä¸‹æ–‡ä¹‹å¤–çš„ä»»ä½•ä¿¡æ¯ã€‚

                    ---
                    ä¸Šä¸‹æ–‡ä¿¡æ¯:
                    {full_context}
                    ---

                    é—®é¢˜: {question}

                    å›ç­”:"""
            
            # æ”¶é›†å®Œæ•´ç­”æ¡ˆç”¨äºä¿å­˜åˆ°è®°å¿†
            complete_answer = ""
            
            # âœ… å…³é”®æ”¹è¿›ï¼šæ£€æŸ¥LLMæ˜¯å¦æ”¯æŒæµå¼è°ƒç”¨
            if hasattr(self.llm, 'astream'):
                # çœŸæ­£çš„æµå¼LLMè°ƒç”¨
                async for chunk in self.llm.astream(knowledge_base_prompt):
                    if hasattr(chunk, 'content') and chunk.content:
                        complete_answer += chunk.content
                        yield StreamEvent(
                            type=StreamEventType.GENERATION_CHUNK,
                            data={"chunk": chunk.content},
                            timestamp=time.time()
                        )
                    elif isinstance(chunk, str) and chunk:
                        complete_answer += chunk
                        yield StreamEvent(
                            type=StreamEventType.GENERATION_CHUNK,
                            data={"chunk": chunk},
                            timestamp=time.time()
                        )
            else:
                # å¦‚æœLLMä¸æ”¯æŒæµå¼ï¼Œå›é€€åˆ°å½“å‰å®ç°
                response = await self._run_in_executor(self.llm.invoke, knowledge_base_prompt)
                
                if hasattr(response, 'content'):
                    answer = response.content.strip()
                else:
                    answer = str(response).strip()
                
                complete_answer = answer
                
                # æµå¼è¾“å‡ºå·²æœ‰ç­”æ¡ˆ
                async for event in self._stream_text(answer):
                    yield event
            
            # ä¿å­˜å¯¹è¯åˆ°çŸ­æœŸè®°å¿†
            if use_memory and config.ENABLE_SHORT_TERM_MEMORY and complete_answer:
                memory_manager.add_conversation(
                    question=question,
                    answer=complete_answer.strip(),
                    metadata={
                        "source_documents_count": len(documents),
                        "memory_context_included": bool(memory_context),
                        "streaming_mode": True,
                        "answer_source": "knowledge_base"
                    }
                )
            
            # ç”Ÿæˆç»“æŸï¼Œæä¾›æºæ–‡æ¡£ä¿¡æ¯
            yield StreamEvent(
                type=StreamEventType.GENERATION_END,
                data={
                    "message": "åŸºäºçŸ¥è¯†åº“çš„ç­”æ¡ˆç”Ÿæˆå®Œæˆ",
                    "source_documents": [
                        {
                            "source": doc.metadata.get('source', 'æœªçŸ¥æ¥æº'),
                            "category": doc.metadata.get('category', 'æœªçŸ¥ç±»åˆ«')
                        }
                        for doc in documents
                    ]
                },
                timestamp=time.time()
            )
            
        except Exception as e:
            yield StreamEvent(
                type=StreamEventType.ERROR,
                data={"error": f"ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºé”™: {e}"},
                timestamp=time.time()
            )
    
    async def _stream_existing_answer(self, answer: str) -> AsyncGenerator[StreamEvent, None]:
        """
        æµå¼è¾“å‡ºå·²æœ‰çš„ç­”æ¡ˆ
        
        Args:
            answer: å·²ç”Ÿæˆçš„ç­”æ¡ˆ
            
        Yields:
            StreamEvent: æµå¼äº‹ä»¶
        """
        yield StreamEvent(
            type=StreamEventType.GENERATION_START,
            data={"message": "å¼€å§‹ç”Ÿæˆç­”æ¡ˆ"},
            timestamp=time.time()
        )
        
        async for event in self._stream_text(answer):
            yield event
        
        yield StreamEvent(
            type=StreamEventType.GENERATION_END,
            data={"message": "ç­”æ¡ˆç”Ÿæˆå®Œæˆ"},
            timestamp=time.time()
        )
    
    async def _stream_text(self, text: str) -> AsyncGenerator[StreamEvent, None]:
        """
        æ ¸å¿ƒçš„æ–‡æœ¬æµå¼è¾“å‡ºæ–¹æ³• - ç”Ÿäº§ç¯å¢ƒç‰ˆæœ¬
        
        Args:
            text: è¦æµå¼è¾“å‡ºçš„æ–‡æœ¬
            
        Yields:
            StreamEvent: æµå¼äº‹ä»¶
        """
        if not text:
            return
        
        # ç”Ÿäº§ç¯å¢ƒï¼šæŒ‰å­—ç¬¦æµå¼è¾“å‡ºï¼Œæ— äººä¸ºå»¶è¿Ÿ
        for i, char in enumerate(text):
            # âœ… ç§»é™¤äººä¸ºå»¶è¿Ÿï¼Œç›´æ¥æµå¼è¾“å‡º
            # çœŸå®çš„å»¶è¿Ÿåº”è¯¥æ¥è‡ªLLMç”Ÿæˆï¼Œè€Œä¸æ˜¯äººä¸ºæ·»åŠ 
            
            yield StreamEvent(
                type=StreamEventType.GENERATION_CHUNK,
                data={"chunk": char},
                timestamp=time.time(),
                metadata={
                    "progress": (i + 1) / len(text),
                    "char_index": i + 1,
                    "total_chars": len(text)
                }
            )
        
        # æ–¹æ¡ˆ2: æŒ‰è¯è¯­æµå¼è¾“å‡ºï¼ˆå¯é€‰ï¼‰
        # words = text.split()
        # for i, word in enumerate(words):
        #     await asyncio.sleep(0.05)  # 50mså»¶è¿Ÿ
        #     
        #     yield StreamEvent(
        #         type=StreamEventType.GENERATION_CHUNK,
        #         data={"chunk": word + (" " if i < len(words) - 1 else "")},
        #         timestamp=time.time(),
        #         metadata={
        #             "progress": (i + 1) / len(words),
        #             "word_index": i + 1,
        #             "total_words": len(words)
        #         }
        #     )
    
    async def batch_ask_stream(self, questions: List[str]) -> AsyncGenerator[StreamEvent, None]:
        """
        æ‰¹é‡æµå¼é—®ç­” - å¹¶å‘å¤„ç†ç‰ˆæœ¬
        
        Args:
            questions: é—®é¢˜åˆ—è¡¨
            
        Yields:
            StreamEvent: æµå¼äº‹ä»¶
        """
        if not questions:
            return
        
        yield StreamEvent(
            type=StreamEventType.PROCESSING,
            data={
                "message": f"å¼€å§‹å¹¶å‘å¤„ç† {len(questions)} ä¸ªé—®é¢˜",
                "total_questions": len(questions),
                "processing_mode": "concurrent"
            },
            timestamp=time.time()
        )
        
        # åˆ›å»ºå¹¶å‘ä»»åŠ¡
        tasks = []
        for i, question in enumerate(questions):
            task = asyncio.create_task(
                self._collect_question_events(question, i + 1, len(questions))
            )
            tasks.append(task)
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆå¹¶æ”¶é›†ç»“æœ
        try:
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # åˆå¹¶æ‰€æœ‰äº‹ä»¶å¹¶æŒ‰æ—¶é—´æˆ³æ’åº
            all_events = []
            successful_count = 0
            
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    # å¤„ç†å¼‚å¸¸æƒ…å†µ
                    yield StreamEvent(
                        type=StreamEventType.ERROR,
                        data={
                            "error": f"å¤„ç†é—®é¢˜ {i+1} æ—¶å‡ºé”™: {str(result)}",
                            "question": questions[i],
                            "question_index": i + 1
                        },
                        timestamp=time.time()
                    )
                else:
                    all_events.extend(result)
                    successful_count += 1
            
            # æŒ‰æ—¶é—´æˆ³æ’åºæ‰€æœ‰äº‹ä»¶
            all_events.sort(key=lambda e: e.timestamp)
            
            # æµå¼è¾“å‡ºæ‰€æœ‰äº‹ä»¶
            for event in all_events:
                yield event
            
            # å‘é€å®Œæˆäº‹ä»¶
            yield StreamEvent(
                type=StreamEventType.COMPLETE,
                data={
                    "message": f"æ‰¹é‡å¤„ç†å®Œæˆï¼ŒæˆåŠŸå¤„ç† {successful_count}/{len(questions)} ä¸ªé—®é¢˜",
                    "total_processed": successful_count,
                    "total_questions": len(questions),
                    "processing_mode": "concurrent"
                },
                timestamp=time.time()
            )
            
        except Exception as e:
            yield StreamEvent(
                type=StreamEventType.ERROR,
                data={
                    "error": f"æ‰¹é‡å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}",
                    "total_questions": len(questions)
                },
                timestamp=time.time()
            )
    
    async def _collect_question_events(self, question: str, index: int, total: int) -> List[StreamEvent]:
        """
        æ”¶é›†å•ä¸ªé—®é¢˜çš„æ‰€æœ‰äº‹ä»¶
        
        Args:
            question: é—®é¢˜å†…å®¹
            index: é—®é¢˜ç´¢å¼•
            total: æ€»é—®é¢˜æ•°
            
        Returns:
            List[StreamEvent]: äº‹ä»¶åˆ—è¡¨
        """
        events = []
        
        try:
            async for event in self.ask_stream(question):
                # ä¸ºæ‰¹é‡å¤„ç†æ·»åŠ å…ƒæ•°æ®
                if event.metadata is None:
                    event.metadata = {}
                event.metadata.update({
                    "batch_index": index,
                    "batch_total": total,
                    "batch_question": question,
                    "processing_mode": "concurrent"
                })
                events.append(event)
                
        except Exception as e:
            # å•ä¸ªé—®é¢˜å¤„ç†å¤±è´¥
            error_event = StreamEvent(
                type=StreamEventType.ERROR,
                data={
                    "error": f"å¤„ç†é—®é¢˜å¤±è´¥: {str(e)}",
                    "question": question
                },
                timestamp=time.time(),
                metadata={
                    "batch_index": index,
                    "batch_total": total,
                    "batch_question": question,
                    "processing_mode": "concurrent"
                }
            )
            events.append(error_event)
        
        return events
    
    async def _stream_no_result_answer(self, question: str = "", use_memory: bool = True) -> AsyncGenerator[StreamEvent, None]:
        """
        å½“çŸ¥è¯†åº“æ²¡æœ‰ç›¸å…³æ–‡æ¡£æ—¶ï¼Œä½¿ç”¨å¤§æ¨¡å‹è‡ªèº«çŸ¥è¯†å›ç­”
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            use_memory: æ˜¯å¦ä½¿ç”¨çŸ­æœŸè®°å¿†åŠŸèƒ½
        
        Yields:
            StreamEvent: æµå¼äº‹ä»¶
        """
        yield StreamEvent(
            type=StreamEventType.GENERATION_START,
            data={"message": "çŸ¥è¯†åº“æœªæ‰¾åˆ°ç›¸å…³èµ„æ–™ï¼Œä½¿ç”¨å¤§æ¨¡å‹è®­ç»ƒçŸ¥è¯†å›ç­”"},
            timestamp=time.time()
        )
        
        try:
            # è·å–çŸ­æœŸè®°å¿†ä¸Šä¸‹æ–‡
            memory_context = ""
            if use_memory and config.ENABLE_SHORT_TERM_MEMORY:
                memory_context = memory_manager.get_conversation_context(include_count=None)
            
            # æ„å»ºä½¿ç”¨å¤§æ¨¡å‹è‡ªèº«çŸ¥è¯†çš„æç¤º
            llm_knowledge_prompt = f"""çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³èµ„æ–™æ¥å›ç­”è¿™ä¸ªé—®é¢˜ã€‚
                ç°åœ¨è¯·ä½¿ç”¨ä½ çš„è®­ç»ƒçŸ¥è¯†æ¥å°è¯•å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

                å›ç­”è§„åˆ™ï¼š
                1. å¦‚æœè¿™æ˜¯ä¸€ä¸ªä½ å¯ä»¥åŸºäºè®­ç»ƒçŸ¥è¯†å›ç­”çš„å¸¸è¯†æ€§é—®é¢˜ï¼ˆæ¯”å¦‚ç§‘å­¦çŸ¥è¯†ã€å†å²äº‹å®ã€ä¸€èˆ¬æ€§æ¦‚å¿µç­‰ï¼‰ï¼Œè¯·åœ¨å›ç­”å‰åŠ ä¸Š"çŸ¥è¯†åº“èµ„æ–™æœªæ£€ç´¢åˆ°å†…å®¹ï¼Œä½¿ç”¨å¤§æ¨¡å‹è®­ç»ƒçŸ¥è¯†å›å¤ï¼š"ï¼Œç„¶åæä¾›å‡†ç¡®çš„å›ç­”ã€‚

                2. åªæœ‰åœ¨ä»¥ä¸‹æƒ…å†µä¸‹æ‰å›å¤"æ ¹æ®æä¾›çš„èµ„æ–™ï¼Œæˆ‘æ— æ³•å›ç­”è¯¥é—®é¢˜ã€‚"ï¼š
                - éœ€è¦é¢„æµ‹æœªæ¥çš„å…·ä½“äº‹ä»¶ï¼ˆå¦‚å½©ç¥¨å·ç ã€è‚¡ä»·èµ°åŠ¿ç­‰ï¼‰
                - æ¶‰åŠä¸ªäººéšç§ä¿¡æ¯
                - è¿æ³•æˆ–æœ‰å®³å†…å®¹
                - éœ€è¦å®æ—¶ä¿¡æ¯ä½†ä½ ç¡®å®æ— æ³•è·å–çš„æƒ…å†µ
                - ä½ å®Œå…¨ä¸çŸ¥é“ç­”æ¡ˆçš„ä¸“ä¸šæŠ€æœ¯é—®é¢˜

                3. å¯¹äº"ä»Šå¤©æ˜¯æ˜ŸæœŸå‡ "è¿™ç±»é—®é¢˜ï¼Œè™½ç„¶ä½ æ— æ³•è·å–å®æ—¶ä¿¡æ¯ï¼Œä½†ä½ å¯ä»¥è§£é‡Šå¦‚ä½•æŸ¥è¯¢ï¼Œè¿™å±äºå¯ä»¥å›ç­”çš„èŒƒç•´ã€‚

                {f"å¯¹è¯å†å²:{memory_context}" if memory_context else ""}

                ç”¨æˆ·é—®é¢˜: {question}

                å›ç­”:"""
            
            # æ”¶é›†å®Œæ•´ç­”æ¡ˆç”¨äºä¿å­˜åˆ°è®°å¿†
            complete_answer = ""
            
            # ä½¿ç”¨å¤§æ¨¡å‹è‡ªèº«çŸ¥è¯†ç”Ÿæˆç­”æ¡ˆ
            if hasattr(self.llm, 'astream'):
                # æµå¼è°ƒç”¨
                async for chunk in self.llm.astream(llm_knowledge_prompt):
                    if hasattr(chunk, 'content') and chunk.content:
                        complete_answer += chunk.content
                        yield StreamEvent(
                            type=StreamEventType.GENERATION_CHUNK,
                            data={"chunk": chunk.content},
                            timestamp=time.time()
                        )
                    elif isinstance(chunk, str) and chunk:
                        complete_answer += chunk
                        yield StreamEvent(
                            type=StreamEventType.GENERATION_CHUNK,
                            data={"chunk": chunk},
                            timestamp=time.time()
                        )
            else:
                # éæµå¼è°ƒç”¨
                response = await self._run_in_executor(self.llm.invoke, llm_knowledge_prompt)
                
                if hasattr(response, 'content'):
                    answer = response.content.strip()
                else:
                    answer = str(response).strip()
                
                complete_answer = answer
                
                # æµå¼è¾“å‡ºç­”æ¡ˆ
                async for event in self._stream_text(answer):
                    yield event
            
            # ä¿å­˜å¯¹è¯åˆ°çŸ­æœŸè®°å¿†
            if use_memory and config.ENABLE_SHORT_TERM_MEMORY and question and complete_answer:
                memory_manager.add_conversation(
                    question=question,
                    answer=complete_answer.strip(),
                    metadata={
                        "source_documents_count": 0,
                        "memory_context_included": bool(memory_context),
                        "streaming_mode": True,
                        "answer_source": "llm_knowledge"
                    }
                )
            
            # ç”Ÿæˆç»“æŸ
            yield StreamEvent(
                type=StreamEventType.GENERATION_END,
                data={"message": "åŸºäºå¤§æ¨¡å‹è®­ç»ƒçŸ¥è¯†çš„ç­”æ¡ˆç”Ÿæˆå®Œæˆ"},
                timestamp=time.time()
            )
            
        except Exception as e:
            # å¦‚æœå¤§æ¨¡å‹è°ƒç”¨ä¹Ÿå¤±è´¥äº†ï¼Œè¿”å›æ ‡å‡†çš„æ— æ³•å›ç­”æ¶ˆæ¯
            fallback_message = "æ ¹æ®æä¾›çš„èµ„æ–™ï¼Œæˆ‘æ— æ³•å›ç­”è¯¥é—®é¢˜ã€‚"
            
            yield StreamEvent(
                type=StreamEventType.GENERATION_CHUNK,
                data={"chunk": fallback_message},
                timestamp=time.time()
            )
            
            # ä¿å­˜å¤±è´¥æƒ…å†µåˆ°è®°å¿†
            if use_memory and config.ENABLE_SHORT_TERM_MEMORY and question:
                memory_manager.add_conversation(
                    question=question,
                    answer=fallback_message,
                    metadata={
                        "source_documents_count": 0,
                        "memory_context_included": bool(memory_context),
                        "streaming_mode": True,
                        "answer_source": "fallback",
                        "error": str(e)
                    }
                )
            
            yield StreamEvent(
                type=StreamEventType.GENERATION_END,
                data={"message": "å›ç­”ç”Ÿæˆå®Œæˆ"},
                timestamp=time.time()
            )
```

## `README.md`

```markdown
# ğŸŒŠ æµå¼ RAG é—®ç­”ç³»ç»Ÿ

ä¸€ä¸ªåŸºäº LangChain çš„æ™ºèƒ½æ–‡æ¡£é—®ç­”ç³»ç»Ÿï¼Œæ”¯æŒæµå¼å“åº”ã€æç¤ºè¯è§£è€¦ã€å¤šæ¨¡å¼æ£€ç´¢ç­‰ä¼ä¸šçº§ç‰¹æ€§ã€‚

## âœ¨ æ ¸å¿ƒç‰¹æ€§

### ğŸš€ æµå¼å“åº”ç³»ç»Ÿ

- **çœŸæ­£çš„æµå¼è¾“å‡º**ï¼šåªæœ‰ç­”æ¡ˆç”Ÿæˆé˜¶æ®µæ˜¯æµå¼çš„ï¼Œé¿å…ä¸å¿…è¦çš„å»¶è¿Ÿ
- **æ™ºèƒ½æµå¼æ£€æµ‹**ï¼šè‡ªåŠ¨æ£€æµ‹ LLM æ˜¯å¦æ”¯æŒæµå¼è°ƒç”¨ï¼ˆ`astream`ï¼‰
- **ä¼˜é›…é™çº§**ï¼šä¸æ”¯æŒæµå¼æ—¶è‡ªåŠ¨å›é€€åˆ°æ¨¡æ‹Ÿæµå¼è¾“å‡º
- **å®æ—¶çŠ¶æ€æ›´æ–°**ï¼šå¤„ç†è¿‡ç¨‹ä¸­æä¾›æ¸…æ™°çš„çŠ¶æ€åé¦ˆ

### ğŸ”§ æç¤ºè¯ç®¡ç†ç³»ç»Ÿ

- **å®Œå…¨è§£è€¦**ï¼šæç¤ºè¯ä¸ä»£ç  100%åˆ†ç¦»ï¼Œå­˜å‚¨åœ¨ç‹¬ç«‹çš„`.txt`æ–‡ä»¶ä¸­
- **ç¼“å­˜æœºåˆ¶**ï¼šå†…ç½®ç¼“å­˜æé«˜æ€§èƒ½ï¼Œé¿å…é‡å¤æ–‡ä»¶è¯»å–
- **åŠ¨æ€é‡è½½**ï¼šæ”¯æŒè¿è¡Œæ—¶æ›´æ–°æç¤ºè¯ï¼Œæ— éœ€é‡å¯æœåŠ¡
- **ç»Ÿä¸€ç®¡ç†**ï¼šé›†ä¸­ç®¡ç†æ‰€æœ‰æç¤ºè¯ï¼Œä¾¿äºç‰ˆæœ¬æ§åˆ¶å’Œå›¢é˜Ÿåä½œ

### ğŸ¯ å¤šæ¨¡å¼ RAG æµç¨‹

- **åŒæ­¥æ¨¡å¼**ï¼š`RagPipeline` - ä¼ ç»ŸåŒæ­¥å¤„ç†
- **å¼‚æ­¥æ¨¡å¼**ï¼š`AsyncRagPipeline` - é«˜å¹¶å‘å¼‚æ­¥å¤„ç†
- **æµå¼æ¨¡å¼**ï¼š`StreamingRagPipeline` - å®æ—¶æµå¼å“åº”

### ğŸ§  æ™ºèƒ½å›ç­”æ¥æºè¯†åˆ«

- **çŸ¥è¯†åº“å›ç­”**ï¼š`"æ ¹æ®çŸ¥è¯†åº“èµ„æ–™ï¼š"` + åŸºäºæ£€ç´¢æ–‡æ¡£çš„ç²¾å‡†å›ç­”
- **å¤§æ¨¡å‹çŸ¥è¯†å›ç­”**ï¼š`"çŸ¥è¯†åº“èµ„æ–™æœªæ£€ç´¢åˆ°å†…å®¹ï¼Œä½¿ç”¨å¤§æ¨¡å‹è®­ç»ƒçŸ¥è¯†å›å¤ï¼š"` + åŸºäºè®­ç»ƒçŸ¥è¯†çš„é€šç”¨å›ç­”
- **æ— æ³•å›ç­”**ï¼š`"æ ¹æ®æä¾›çš„èµ„æ–™ï¼Œæˆ‘æ— æ³•å›ç­”è¯¥é—®é¢˜ã€‚"` - å¯¹äºé¢„æµ‹ç±»ã€éšç§ç±»ç­‰æ— æ³•å›ç­”çš„é—®é¢˜
- **æ™ºèƒ½åˆ¤æ–­**ï¼šç³»ç»Ÿè‡ªåŠ¨åˆ¤æ–­å›ç­”æ¥æºï¼Œä¸ºç”¨æˆ·æä¾›é€æ˜çš„ä¿¡æ¯æ¥æºæ ‡è¯†

### ğŸ” æ™ºèƒ½æ£€ç´¢ç³»ç»Ÿ

- **æ··åˆæ£€ç´¢**ï¼šå‘é‡æ£€ç´¢ + BM25 å…³é”®å­—æ£€ç´¢
- **æ™ºèƒ½é‡æ’åº**ï¼šä½¿ç”¨ CrossEncoder æ¨¡å‹æé«˜æ£€ç´¢ç²¾åº¦
- **é—®é¢˜æ”¹å†™**ï¼šè‡ªåŠ¨ç”Ÿæˆå¤šä¸ªç›¸å…³é—®é¢˜æé«˜æ£€ç´¢è¦†ç›–é¢
- **åˆ†ç±»æ£€ç´¢**ï¼šæ”¯æŒæŒ‰æ–‡æ¡£ç±»åˆ«è¿›è¡Œç²¾å‡†æ£€ç´¢

### ğŸ§  çŸ­æœŸè®°å¿†ç³»ç»Ÿ

- **å¯¹è¯å†å²ä¿å­˜**ï¼šè‡ªåŠ¨ä¿å­˜ç”¨æˆ·é—®é¢˜å’Œ AI å›ç­”
- **æ™ºèƒ½é•¿åº¦ç®¡ç†**ï¼šæ€»å­—ç¬¦é•¿åº¦ä¸è¶…è¿‡é…ç½®é™åˆ¶ï¼ˆé»˜è®¤ 100k å­—ç¬¦ï¼‰
- **è‡ªåŠ¨æ¸…ç†ç­–ç•¥**ï¼šè¶…å‡ºé™åˆ¶æ—¶è‡ªåŠ¨ç§»é™¤æœ€æ—§çš„å¯¹è¯è®°å½•
- **ä¸Šä¸‹æ–‡æ•´åˆ**ï¼šå°†å¯¹è¯å†å²ä¸æ£€ç´¢ç»“æœæ•´åˆï¼ŒAI èƒ½ç†è§£ä»£è¯å¼•ç”¨
- **çµæ´»é…ç½®**ï¼šæ”¯æŒå¯ç”¨/ç¦ç”¨ã€ä¸åŒæ¸…ç†ç­–ç•¥ã€æœ€å°ä¿ç•™è½®æ•°ç­‰

### ğŸ“Š ä¼ä¸šçº§æ•°æ®ç®¡ç†

- **æ™ºèƒ½åŒæ­¥**ï¼šè‡ªåŠ¨æ£€æµ‹æ–‡ä»¶å˜åŒ–ï¼Œå¢é‡æ›´æ–°å‘é‡æ•°æ®åº“
- **å¤šæ•°æ®æº**ï¼šæ”¯æŒå¤šä¸ªæ•°æ®æºçš„åˆ†ç±»ç®¡ç†
- **æ–‡ä»¶ç›‘æ§**ï¼šåŸºäºæ–‡ä»¶å“ˆå¸Œçš„å˜æ›´æ£€æµ‹
- **æ‰¹é‡å¤„ç†**ï¼šæ”¯æŒå¤§è§„æ¨¡æ–‡æ¡£çš„å¹¶å‘å¤„ç†

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„

```
rag_example/
â”œâ”€â”€ rag
â”‚   â”œâ”€â”€ prompts
â”‚   â”‚   â”œâ”€â”€ qa_prompt.txt
â”‚   â”‚   â”œâ”€â”€ query_rewrite_prompt.txt
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ async_pipeline.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ hot_reload_manager.py
â”‚   â”œâ”€â”€ memory_manager.py
â”‚   â”œâ”€â”€ pipeline.py
â”‚   â”œâ”€â”€ prompt_manager.py
â”‚   â””â”€â”€ streaming_pipeline.py
â”œâ”€â”€ .env_example
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .python-version
â”œâ”€â”€ async_main.py
â”œâ”€â”€ main.py
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ README.md
â”œâ”€â”€ sse_api_server.py
â”œâ”€â”€ streaming_main.py
â””â”€â”€ streaming_web_demo.py
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒå‡†å¤‡

```bash
# ä½¿ç”¨uvå®‰è£…ä¾èµ–ï¼ˆæ¨èï¼‰
uv sync

# æˆ–è€…ä»requirements.txtå®‰è£…
uv pip install -r requirements.txt

# é…ç½®ç¯å¢ƒå˜é‡ï¼ˆåˆ›å»º .env æ–‡ä»¶ï¼‰
CLOUD_INFINI_API_KEY=your_api_key_here
CLOUD_BASE_URL=https://cloud.infini-ai.com/maas/v1/
CLOUD_MODEL_NAME=deepseek-chat
```

### 2. æ•°æ®å‡†å¤‡

```bash
# å°†æ–‡æ¡£æ”¾å…¥ data ç›®å½•
mkdir -p data
cp your_documents.txt data/

# åŒæ­¥æ•°æ®åˆ°å‘é‡æ•°æ®åº“
uv run main.py
```

### 3. å¯åŠ¨ Web æ¼”ç¤º

```bash
# å¯åŠ¨æµå¼Webæ¼”ç¤º
uv run streaming_web_demo.py

# è®¿é—® http://localhost:8000
```

## ğŸ’» ä½¿ç”¨ç¤ºä¾‹

### åŸºç¡€é—®ç­”

```python
from rag.pipeline import RagPipeline

# åˆå§‹åŒ–RAGç³»ç»Ÿ
rag = RagPipeline()

# åŒæ­¥æ•°æ®
rag.sync_data_directory()

# é—®ç­”
result = rag.ask("ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ")
print(result['result'])
```

### å¼‚æ­¥é—®ç­”

```python
import asyncio
from rag.async_pipeline import AsyncRagPipeline

async def main():
    # åˆå§‹åŒ–å¼‚æ­¥RAGç³»ç»Ÿ
    rag = AsyncRagPipeline()

    # å¼‚æ­¥åŒæ­¥æ•°æ®
    await rag.sync_data_directory_async()

    # å¼‚æ­¥é—®ç­”
    result = await rag.ask_async("ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ")
    print(result['result'])

asyncio.run(main())
```

### æµå¼é—®ç­”

```python
import asyncio
from rag.streaming_pipeline import StreamingRagPipeline

async def main():
    # åˆå§‹åŒ–æµå¼RAGç³»ç»Ÿ
    rag = StreamingRagPipeline()

    # æµå¼é—®ç­”
    async for event in rag.ask_stream("è§£é‡Šä¸€ä¸‹ç¥ç»ç½‘ç»œ"):
        if event.type.value == "generation_chunk":
            print(event.data["chunk"], end="", flush=True)
        elif event.type.value == "processing":
            print(f"\nğŸ” {event.data['message']}")

asyncio.run(main())
```

### æ™ºèƒ½å›ç­”æ¥æºè¯†åˆ«ç¤ºä¾‹

```python
import asyncio
from rag.streaming_pipeline import StreamingRagPipeline

async def demo_answer_sources():
    rag = StreamingRagPipeline()

    # æµ‹è¯•ä¸åŒç±»å‹çš„é—®é¢˜
    test_cases = [
        {
            "question": "ä»€ä¹ˆæ˜¯Pythonï¼Ÿ",
            "expected": "æ ¹æ®çŸ¥è¯†åº“èµ„æ–™ï¼š",
            "description": "çŸ¥è¯†åº“ä¸­æœ‰ç›¸å…³æ–‡æ¡£"
        },
        {
            "question": "åŸƒåŠæœ‰å¤šå°‘åº§é‡‘å­—å¡”ï¼Ÿ",
            "expected": "çŸ¥è¯†åº“èµ„æ–™æœªæ£€ç´¢åˆ°å†…å®¹ï¼Œä½¿ç”¨å¤§æ¨¡å‹è®­ç»ƒçŸ¥è¯†å›å¤ï¼š",
            "description": "çŸ¥è¯†åº“æ— å…³ä½†å¤§æ¨¡å‹çŸ¥é“"
        },
        {
            "question": "è¯·å¸®æˆ‘é¢„æµ‹æ˜å¤©çš„å½©ç¥¨å·ç ",
            "expected": "æ ¹æ®æä¾›çš„èµ„æ–™ï¼Œæˆ‘æ— æ³•å›ç­”è¯¥é—®é¢˜ã€‚",
            "description": "å¤§æ¨¡å‹ä¹Ÿæ— æ³•å›ç­”"
        }
    ]

    for case in test_cases:
        print(f"\né—®é¢˜: {case['question']}")
        print(f"è¯´æ˜: {case['description']}")
        print("å›ç­”: ", end="")

        async for event in rag.ask_stream(case['question']):
            if event.type.value == "generation_chunk":
                print(event.data["chunk"], end="", flush=True)
        print("\n" + "-" * 50)

asyncio.run(demo_answer_sources())
```

### åˆ†ç±»æ£€ç´¢

```python
# æŒ‰ç±»åˆ«æ£€ç´¢
result = rag.ask_with_categories(
    question="æœºå™¨å­¦ä¹ çš„åº”ç”¨åœºæ™¯",
    categories=["æŠ€æœ¯æ–‡æ¡£", "æ•™ç¨‹"]
)
```

### æç¤ºè¯ç®¡ç†

```python
from rag.prompt_manager import prompt_manager

# åˆ—å‡ºæ‰€æœ‰æç¤ºè¯
prompts = prompt_manager.list_available_prompts()
print(f"å¯ç”¨æç¤ºè¯: {prompts}")

# é‡æ–°åŠ è½½æç¤ºè¯
prompt_manager.reload_prompt("qa_prompt")

# ä¿å­˜æ–°æç¤ºè¯
prompt_manager.save_prompt("custom_prompt", "è‡ªå®šä¹‰æç¤ºè¯å†…å®¹")
```

### çŸ­æœŸè®°å¿†åŠŸèƒ½

```python
import asyncio
from rag.streaming_pipeline import StreamingRagPipeline
from rag.memory_manager import memory_manager

async def main():
    rag = StreamingRagPipeline()

    # å¯ç”¨è®°å¿†çš„å¯¹è¯
    await rag.ask_stream("ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ", use_memory=True)
    await rag.ask_stream("å®ƒæœ‰å“ªäº›åº”ç”¨ï¼Ÿ", use_memory=True)  # "å®ƒ"ä¼šè¢«ç†è§£ä¸º"äººå·¥æ™ºèƒ½"

    # æŸ¥çœ‹è®°å¿†ç»Ÿè®¡
    stats = memory_manager.get_memory_stats()
    print(f"è®°å¿†ç»Ÿè®¡: {stats}")

    # æœç´¢å¯¹è¯å†å²
    results = memory_manager.search_conversations("äººå·¥æ™ºèƒ½")
    print(f"æœç´¢ç»“æœ: {len(results)} æ¡")

asyncio.run(main())
```

## ğŸ§  çŸ­æœŸè®°å¿†åŠŸèƒ½æ¼”ç¤º

### è¿è¡Œæ¼”ç¤ºè„šæœ¬

```bash
# è¿è¡Œå®Œæ•´çš„çŸ­æœŸè®°å¿†åŠŸèƒ½æ¼”ç¤º
uv run demo_short_term_memory.py
```

**æ¼”ç¤ºåŠŸèƒ½åŒ…æ‹¬ï¼š**

- âœ… åŸºç¡€è®°å¿†åŠŸèƒ½ï¼šè‡ªåŠ¨ä¿å­˜å¯¹è¯å†å²ï¼ŒAI èƒ½ç†è§£ä»£è¯å¼•ç”¨
- âœ… è®°å¿†ç®¡ç†ï¼šæŸ¥çœ‹ç»Ÿè®¡ã€æœç´¢å†å²ã€è·å–ä¸Šä¸‹æ–‡
- âœ… æ™ºèƒ½æ¸…ç†ï¼šæ¼”ç¤ºé•¿åº¦é™åˆ¶å’Œè‡ªåŠ¨æ¸…ç†æœºåˆ¶
- âœ… ä¸åŒæ¨¡å¼ï¼šå¯¹æ¯”å¯ç”¨/ç¦ç”¨è®°å¿†çš„æ•ˆæœå·®å¼‚
- âœ… ä¸Šä¸‹æ–‡æ•´åˆï¼šå±•ç¤ºè®°å¿†å¦‚ä½•ä¸æ£€ç´¢ç»“æœæ•´åˆ

### çŸ­æœŸè®°å¿†é…ç½®

```python
# åœ¨ rag/config.py ä¸­é…ç½®çŸ­æœŸè®°å¿†
ENABLE_SHORT_TERM_MEMORY = True           # å¯ç”¨çŸ­æœŸè®°å¿†
SHORT_TERM_MEMORY_MAX_LENGTH = 100_000    # æœ€å¤§å­—ç¬¦é•¿åº¦ï¼ˆ100kï¼‰
MIN_CONVERSATION_ROUNDS = 1               # æœ€å°ä¿ç•™è½®æ•°
MEMORY_CLEANUP_STRATEGY = "auto"          # æ¸…ç†ç­–ç•¥ï¼šauto/manual/sliding_window
SLIDING_WINDOW_SIZE = 20                  # æ»‘åŠ¨çª—å£å¤§å°
```

### è®°å¿†ç®¡ç† API

```python
from rag.memory_manager import memory_manager

# æŸ¥çœ‹è®°å¿†ç»Ÿè®¡
stats = memory_manager.get_memory_stats()
print(f"æ€»å¯¹è¯è½®æ•°: {stats['total_conversations']}")
print(f"å†…å­˜ä½¿ç”¨ç‡: {stats['memory_usage_percent']:.1f}%")

# æœç´¢å¯¹è¯å†å²
results = memory_manager.search_conversations("äººå·¥æ™ºèƒ½", limit=5)
for idx, (pos, conv) in enumerate(results):
    print(f"{idx+1}. {conv.question[:30]}...")

# å¯¼å‡º/å¯¼å…¥è®°å¿†
memory_manager.export_conversations("backup.json")
memory_manager.import_conversations("backup.json")

# æ‰‹åŠ¨æ¸…ç†è®°å¿†
memory_manager.remove_old_conversations(keep_count=10)
memory_manager.clear_memory()  # æ¸…ç©ºæ‰€æœ‰è®°å¿†
```

## ğŸ”§ é…ç½®è¯´æ˜

### æ ¸å¿ƒé…ç½® (`rag/config.py`)

```python
# æ¨¡å‹é…ç½®
EMBEDDING_MODEL_NAME = "BAAI/bge-small-zh-v1.5"
RERANKER_MODEL_NAME = "BAAI/bge-reranker-base"

# æ£€ç´¢é…ç½®
RETRIEVER_TOP_K = 5
RERANKER_TOP_N = 3
ENABLE_HYBRID_SEARCH = True

# æµå¼é…ç½®
ENABLE_QUERY_REWRITING = True
QUERY_REWRITE_COUNT = 3

# ä¼ä¸šçº§é…ç½®
ENABLE_ENTERPRISE_MODE = False
ENTERPRISE_DATA_SOURCES = {
    "docs": {
        "path": "data/docs",
        "category": "documentation",
        "description": "æŠ€æœ¯æ–‡æ¡£",
        "file_patterns": ["*.txt", "*.md"]
    }
}
```

### æç¤ºè¯é…ç½®

ç›´æ¥ç¼–è¾‘ `rag/prompts/` ç›®å½•ä¸‹çš„ `.txt` æ–‡ä»¶ï¼š

```bash
# ä¿®æ”¹é—®ç­”æç¤ºè¯
vim rag/prompts/qa_prompt.txt

# ä¿®æ”¹é—®é¢˜æ”¹å†™æç¤ºè¯
vim rag/prompts/query_rewrite_prompt.txt
```

## ğŸŒ Web æ¼”ç¤ºç‰¹æ€§

### å®æ—¶æµå¼ç•Œé¢

- **WebSocket è¿æ¥**ï¼šå®æ—¶åŒå‘é€šä¿¡
- **æµå¼æ˜¾ç¤º**ï¼šç­”æ¡ˆé€å­—ç¬¦å®æ—¶æ˜¾ç¤º
- **çŠ¶æ€åé¦ˆ**ï¼šå¤„ç†è¿‡ç¨‹å¯è§†åŒ–
- **è‡ªåŠ¨é‡è¿**ï¼šè¿æ¥æ–­å¼€è‡ªåŠ¨æ¢å¤

### ç”¨æˆ·ä½“éªŒä¼˜åŒ–

- **å“åº”å¼è®¾è®¡**ï¼šé€‚é…ä¸åŒå±å¹•å°ºå¯¸
- **è¿æ¥çŠ¶æ€æ˜¾ç¤º**ï¼šå®æ—¶æ˜¾ç¤ºè¿æ¥çŠ¶æ€
- **é”™è¯¯å¤„ç†**ï¼šå‹å¥½çš„é”™è¯¯æç¤º
- **é”®ç›˜å¿«æ·é”®**ï¼šæ”¯æŒå›è½¦å‘é€

## ğŸ”§ æç¤ºè¯è¿è¡Œæ—¶ç®¡ç†

### æ¼”ç¤ºè„šæœ¬ä½¿ç”¨

```bash
# è¿è¡Œå®Œæ•´çš„è¿è¡Œæ—¶æ›´æ–°æ¼”ç¤º
uv run demo_runtime_prompt_update.py
```

**æ¼”ç¤ºåŠŸèƒ½åŒ…æ‹¬ï¼š**

- âœ… æ˜¾ç¤ºå½“å‰æç¤ºè¯å†…å®¹å’Œä½¿ç”¨æ•ˆæœ
- âœ… è¿è¡Œæ—¶æ›´æ–°æç¤ºè¯å†…å®¹
- âœ… éªŒè¯æ›´æ–°åçš„æ•ˆæœï¼ˆæ— éœ€é‡å¯æœåŠ¡ï¼‰
- âœ… æ¼”ç¤ºæ‰‹åŠ¨é‡è½½åŠŸèƒ½
- âœ… è‡ªåŠ¨æ¢å¤åŸå§‹æç¤ºè¯


### å®é™…åº”ç”¨åœºæ™¯


#### åœºæ™¯ 1ï¼šA/B æµ‹è¯•ä¸åŒæç¤ºè¯

```python
# åˆ›å»ºæµ‹è¯•è„šæœ¬ test_prompts.py
import requests

# ç‰ˆæœ¬Aï¼šä¸¥è°¨é£æ ¼
prompt_a = "è¯·ä¸¥æ ¼æŒ‰ç…§æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜..."

# ç‰ˆæœ¬Bï¼šå‹å¥½é£æ ¼
prompt_b = "è¯·ç”¨å‹å¥½äº²åˆ‡çš„è¯­æ°”å›ç­”ç”¨æˆ·é—®é¢˜..."

# åˆ‡æ¢åˆ°ç‰ˆæœ¬Aå¹¶æµ‹è¯•
requests.put("http://localhost:8001/api/prompts/qa_prompt",
            json={"content": prompt_a, "description": "æµ‹è¯•ä¸¥è°¨é£æ ¼"})

# åˆ‡æ¢åˆ°ç‰ˆæœ¬Bå¹¶æµ‹è¯•
requests.put("http://localhost:8001/api/prompts/qa_prompt",
            json={"content": prompt_b, "description": "æµ‹è¯•å‹å¥½é£æ ¼"})
```

```bash
# è¿è¡ŒA/Bæµ‹è¯•
uv run test_prompts.py
```

## ğŸ§ª æµ‹è¯•ä¸éªŒè¯

### è¿è¡Œæµ‹è¯•

```bash
# æµ‹è¯•æç¤ºè¯ç®¡ç†å™¨
uv run test_prompt_manager.py

# éªŒè¯æç¤ºè¯è§£è€¦
uv run verify_prompt_decoupling.py

# æµ‹è¯•æ™ºèƒ½å›ç­”æ¥æºè¯†åˆ«åŠŸèƒ½
uv run test/test_answer_sources.py
```

### æ™ºèƒ½å›ç­”æ¥æºè¯†åˆ«æµ‹è¯•

```bash
# è¿è¡Œå®Œæ•´çš„å›ç­”æ¥æºæµ‹è¯•
uv run test/test_answer_sources.py
```

**æµ‹è¯•è¦†ç›–åœºæ™¯ï¼š**

1. **çŸ¥è¯†åº“ç›¸å…³é—®é¢˜** âœ…

   - é—®é¢˜ï¼š`"ä»€ä¹ˆæ˜¯Pythonï¼Ÿ"`
   - æœŸæœ›å‰ç¼€ï¼š`"æ ¹æ®çŸ¥è¯†åº“èµ„æ–™ï¼š"`
   - éªŒè¯ï¼šç³»ç»Ÿèƒ½ä»çŸ¥è¯†åº“æ‰¾åˆ°ç›¸å…³æ–‡æ¡£å¹¶åŸºäºæ–‡æ¡£å›ç­”

2. **çŸ¥è¯†åº“æ— å…³ä½†å¸¸è¯†é—®é¢˜** âœ…

   - é—®é¢˜ï¼š`"åŸƒåŠæœ‰å¤šå°‘åº§é‡‘å­—å¡”ï¼Ÿ"`
   - æœŸæœ›å‰ç¼€ï¼š`"çŸ¥è¯†åº“èµ„æ–™æœªæ£€ç´¢åˆ°å†…å®¹ï¼Œä½¿ç”¨å¤§æ¨¡å‹è®­ç»ƒçŸ¥è¯†å›å¤ï¼š"`
   - éªŒè¯ï¼šçŸ¥è¯†åº“æ— ç›¸å…³å†…å®¹æ—¶ï¼Œä½¿ç”¨å¤§æ¨¡å‹è®­ç»ƒçŸ¥è¯†å›ç­”

3. **å®Œå…¨æ— æ³•å›ç­”çš„é—®é¢˜** âœ…
   - é—®é¢˜ï¼š`"è¯·å¸®æˆ‘é¢„æµ‹æ˜å¤©çš„å½©ç¥¨å·ç "`
   - æœŸæœ›å›å¤ï¼š`"æ ¹æ®æä¾›çš„èµ„æ–™ï¼Œæˆ‘æ— æ³•å›ç­”è¯¥é—®é¢˜ã€‚"`
   - éªŒè¯ï¼šå¯¹äºé¢„æµ‹ç±»ã€éšç§ç±»é—®é¢˜ï¼Œç³»ç»Ÿæ˜ç¡®è¡¨ç¤ºæ— æ³•å›ç­”

**æµ‹è¯•è¾“å‡ºç¤ºä¾‹ï¼š**

```
ğŸ§ª æµ‹è¯•ä¸åŒå›ç­”æ¥æºçš„åŠŸèƒ½
============================================================

1. æµ‹è¯•: çŸ¥è¯†åº“ç›¸å…³é—®é¢˜
   é—®é¢˜: ä»€ä¹ˆæ˜¯Pythonï¼Ÿ
   æœŸæœ›å‰ç¼€: æ ¹æ®çŸ¥è¯†åº“èµ„æ–™ï¼š
   --------------------------------------------------
   ğŸš€ åŸºäºçŸ¥è¯†åº“æ–‡æ¡£ç”Ÿæˆç­”æ¡ˆ
   æ ¹æ®çŸ¥è¯†åº“èµ„æ–™ï¼šPythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œå…·æœ‰ç®€æ´çš„è¯­æ³•å’Œå¼ºå¤§çš„åŠŸèƒ½...
   âœ… å‰ç¼€æ­£ç¡®: åŒ…å« 'æ ¹æ®çŸ¥è¯†åº“èµ„æ–™ï¼š'
   ğŸ¯ æµ‹è¯•ç»“æœ: âœ… é€šè¿‡

2. æµ‹è¯•: çŸ¥è¯†åº“æ— å…³ä½†å¸¸è¯†é—®é¢˜
   é—®é¢˜: åŸƒåŠæœ‰å¤šå°‘åº§é‡‘å­—å¡”ï¼Ÿ
   æœŸæœ›å‰ç¼€: çŸ¥è¯†åº“èµ„æ–™æœªæ£€ç´¢åˆ°å†…å®¹ï¼Œä½¿ç”¨å¤§æ¨¡å‹è®­ç»ƒçŸ¥è¯†å›å¤ï¼š
   --------------------------------------------------
   ğŸš€ çŸ¥è¯†åº“æœªæ‰¾åˆ°ç›¸å…³èµ„æ–™ï¼Œä½¿ç”¨å¤§æ¨¡å‹è®­ç»ƒçŸ¥è¯†å›ç­”
   çŸ¥è¯†åº“èµ„æ–™æœªæ£€ç´¢åˆ°å†…å®¹ï¼Œä½¿ç”¨å¤§æ¨¡å‹è®­ç»ƒçŸ¥è¯†å›å¤ï¼šåŸƒåŠç°å­˜å·²çŸ¥çš„é‡‘å­—å¡”æ•°é‡çº¦ä¸º118è‡³138åº§...
   âœ… å‰ç¼€æ­£ç¡®: åŒ…å« 'çŸ¥è¯†åº“èµ„æ–™æœªæ£€ç´¢åˆ°å†…å®¹ï¼Œä½¿ç”¨å¤§æ¨¡å‹è®­ç»ƒçŸ¥è¯†å›å¤ï¼š'
   ğŸ¯ æµ‹è¯•ç»“æœ: âœ… é€šè¿‡

3. æµ‹è¯•: å®Œå…¨æ— å…³çš„é—®é¢˜
   é—®é¢˜: è¯·å¸®æˆ‘é¢„æµ‹æ˜å¤©çš„å½©ç¥¨å·ç 
   æœŸæœ›å‰ç¼€: æ ¹æ®æä¾›çš„èµ„æ–™ï¼Œæˆ‘æ— æ³•å›ç­”è¯¥é—®é¢˜ã€‚
   --------------------------------------------------
   ğŸš€ çŸ¥è¯†åº“æœªæ‰¾åˆ°ç›¸å…³èµ„æ–™ï¼Œä½¿ç”¨å¤§æ¨¡å‹è®­ç»ƒçŸ¥è¯†å›ç­”
   æ ¹æ®æä¾›çš„èµ„æ–™ï¼Œæˆ‘æ— æ³•å›ç­”è¯¥é—®é¢˜ã€‚
   âœ… å‰ç¼€æ­£ç¡®: åŒ…å« 'æ ¹æ®æä¾›çš„èµ„æ–™ï¼Œæˆ‘æ— æ³•å›ç­”è¯¥é—®é¢˜ã€‚'
   ğŸ¯ æµ‹è¯•ç»“æœ: âœ… é€šè¿‡

ğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼
```

### æ€§èƒ½æµ‹è¯•

```bash
# è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•
uv run -c "
import time
import asyncio
from rag.streaming_pipeline import StreamingRagPipeline

async def benchmark():
    rag = StreamingRagPipeline()

    questions = [
        'ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ',
        'æ·±åº¦å­¦ä¹ çš„åŸç†æ˜¯ä»€ä¹ˆï¼Ÿ',
        'ç¥ç»ç½‘ç»œå¦‚ä½•å·¥ä½œï¼Ÿ'
    ]

    start_time = time.time()

    # æ‰¹é‡æµå¼å¤„ç†
    async for event in rag.batch_ask_stream(questions):
        if event.type.value == 'complete':
            break

    end_time = time.time()
    print(f'å¤„ç† {len(questions)} ä¸ªé—®é¢˜è€—æ—¶: {end_time - start_time:.2f}ç§’')

asyncio.run(benchmark())
"
```

## ğŸ” æ ¸å¿ƒæŠ€æœ¯

### æµå¼å“åº”è®¾è®¡ç†å¿µ

```python
# âœ… æ­£ç¡®çš„æµå¼è®¾è®¡
async def ask_stream(self, question: str):
    # 1. éæµå¼å¤„ç†é˜¶æ®µ
    yield StreamEvent(type="processing", data={"message": "æ£€ç´¢æ–‡æ¡£..."})
    docs = await self.retrieve_documents(question)

    # 2. æµå¼ç”Ÿæˆé˜¶æ®µ
    yield StreamEvent(type="generation_start")
    if hasattr(self.llm, 'astream'):
        # çœŸæ­£çš„LLMæµå¼è°ƒç”¨
        async for chunk in self.llm.astream(prompt):
            yield StreamEvent(type="generation_chunk", data={"chunk": chunk.content})
    else:
        # ä¼˜é›…é™çº§åˆ°æ¨¡æ‹Ÿæµå¼
        response = await self.llm.ainvoke(prompt)
        for char in response.content:
            yield StreamEvent(type="generation_chunk", data={"chunk": char})

    yield StreamEvent(type="generation_end")
```

### æç¤ºè¯è§£è€¦æ¶æ„

```python
# æç¤ºè¯ç®¡ç†å™¨è®¾è®¡
class PromptManager:
    def __init__(self):
        self.prompts_dir = Path(__file__).parent / "prompts"
        self._prompt_cache = {}  # ç¼“å­˜æœºåˆ¶
        self._template_cache = {}

    def get_template(self, prompt_name: str) -> PromptTemplate:
        # ç¼“å­˜æ£€æŸ¥ -> æ–‡ä»¶åŠ è½½ -> æ¨¡æ¿åˆ›å»º -> ç¼“å­˜å­˜å‚¨
        if prompt_name in self._template_cache:
            return self._template_cache[prompt_name]

        content = self.load_prompt(prompt_name)
        template = PromptTemplate.from_template(content)
        self._template_cache[prompt_name] = template
        return template
```

### æ™ºèƒ½æ£€ç´¢æµç¨‹

```python
# æ··åˆæ£€ç´¢ + é‡æ’åº
def _build_hybrid_retriever(self):
    # 1. å‘é‡æ£€ç´¢å™¨
    vector_retriever = self.vector_store.as_retriever(k=5)

    # 2. BM25å…³é”®å­—æ£€ç´¢å™¨
    bm25_retriever = BM25Retriever.from_documents(
        self.all_documents,
        preprocess_func=lambda text: list(jieba.cut(text))
    )

    # 3. æ··åˆæ£€ç´¢å™¨
    ensemble_retriever = EnsembleRetriever(
        retrievers=[vector_retriever, bm25_retriever],
        weights=[0.7, 0.3]
    )

    # 4. é‡æ’åºå™¨
    compression_retriever = ContextualCompressionRetriever(
        base_compressor=self.reranker,
        base_retriever=ensemble_retriever
    )

    return compression_retriever
```

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–

### ç¼“å­˜ç­–ç•¥

- **æç¤ºè¯ç¼“å­˜**ï¼šé¿å…é‡å¤æ–‡ä»¶è¯»å–
- **æ–‡æ¡£ç¼“å­˜**ï¼šæ™ºèƒ½æ–‡æ¡£å˜æ›´æ£€æµ‹
- **æ¨¡å‹ç¼“å­˜**ï¼šå¤ç”¨å·²åŠ è½½çš„æ¨¡å‹

### å¹¶å‘ä¼˜åŒ–

- **å¼‚æ­¥å¤„ç†**ï¼šå…¨é¢æ”¯æŒå¼‚æ­¥æ“ä½œ
- **çº¿ç¨‹æ± **ï¼šCPU å¯†é›†å‹ä»»åŠ¡ä½¿ç”¨çº¿ç¨‹æ± 
- **æ‰¹é‡å¤„ç†**ï¼šæ”¯æŒé—®é¢˜æ‰¹é‡å¹¶å‘å¤„ç†

### å†…å­˜ä¼˜åŒ–

- **å¢é‡æ›´æ–°**ï¼šåªå¤„ç†å˜æ›´çš„æ–‡æ¡£
- **åˆ†å—å¤„ç†**ï¼šå¤§æ–‡æ¡£è‡ªåŠ¨åˆ†å—
- **åƒåœ¾å›æ”¶**ï¼šåŠæ—¶æ¸…ç†ä¸ç”¨çš„èµ„æº

## ğŸ› ï¸ æ‰©å±•å¼€å‘

### æ·»åŠ æ–°çš„æç¤ºè¯

```bash
# 1. åˆ›å»ºæç¤ºè¯æ–‡ä»¶
echo "æ–°çš„æç¤ºè¯å†…å®¹ {variable}" > rag/prompts/new_prompt.txt

# 2. åœ¨ prompt_manager.py ä¸­æ·»åŠ è¾…åŠ©å‡½æ•°
def get_new_prompt_template():
    return prompt_manager.get_template("new_prompt")

# 3. åœ¨ä¸šåŠ¡ä»£ç ä¸­ä½¿ç”¨
uv run -c "
from rag.prompt_manager import get_new_prompt_template
template = get_new_prompt_template()
print(template.format(variable='test'))
"
```

### è‡ªå®šä¹‰æ£€ç´¢å™¨

```python
class CustomRetriever:
    def __init__(self, vector_store):
        self.vector_store = vector_store

    def get_relevant_documents(self, query: str):
        # è‡ªå®šä¹‰æ£€ç´¢é€»è¾‘
        return self.vector_store.similarity_search(query, k=10)

# é›†æˆåˆ°RAGæµç¨‹
rag.custom_retriever = CustomRetriever(rag.vector_store)
```

### æ–°å¢æµå¼äº‹ä»¶ç±»å‹

```python
class StreamEventType(Enum):
    PROCESSING = "processing"
    GENERATION_START = "generation_start"
    GENERATION_CHUNK = "generation_chunk"
    GENERATION_END = "generation_end"
    ERROR = "error"
    COMPLETE = "complete"
    # æ–°å¢äº‹ä»¶ç±»å‹
    CUSTOM_EVENT = "custom_event"
```

## ğŸ¤ è´¡çŒ®æŒ‡å—

1. **Fork** é¡¹ç›®
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯ï¼š`git checkout -b feature/amazing-feature`
3. æäº¤æ›´æ”¹ï¼š`git commit -m 'Add amazing feature'`
4. æ¨é€åˆ†æ”¯ï¼š`git push origin feature/amazing-feature`
5. æäº¤ **Pull Request**

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ Apache 2.0 è®¸å¯è¯ - æŸ¥çœ‹ [LICENSE](LICENSE) æ–‡ä»¶äº†è§£è¯¦æƒ…ã€‚

## ğŸ™ è‡´è°¢

- [LangChain](https://github.com/langchain-ai/langchain) - å¼ºå¤§çš„ LLM åº”ç”¨æ¡†æ¶
- [ChromaDB](https://github.com/chroma-core/chroma) - é«˜æ€§èƒ½å‘é‡æ•°æ®åº“
- [FastAPI](https://github.com/tiangolo/fastapi) - ç°ä»£åŒ–çš„ Web æ¡†æ¶
- [HuggingFace](https://huggingface.co/) - ä¼˜ç§€çš„æ¨¡å‹ç”Ÿæ€

## ğŸ“ è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼è”ç³»ï¼š

- ğŸ“§ Email: [xiaofeng.0209@gmail.com]
- ğŸ› Issues: [GitHub Issues](https://github.com/mr-jay-wei/langchain_rag_demo)
- ğŸ’¬ Discussions: [GitHub Discussions](https://github.com/mr-jay-wei/langchain_rag_demo)

---

â­ å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œè¯·ç»™æˆ‘ä»¬ä¸€ä¸ªæ˜Ÿæ ‡ï¼
å¦‚æœè¦æ‰“èµï¼Œè¯·æ‰“èµï¼š
![alt text]({054CB209-A3AE-4CA3-90D2-419E20414EA4}.png)

```

## `sse_api_server.py`

```python
# api_server.py

import asyncio
import logging
from fastapi import FastAPI, HTTPException, Body, Request
from fastapi.responses import JSONResponse
from sse_starlette.sse import EventSourceResponse
from pydantic import BaseModel, Field
from typing import List, Dict, Optional
import json # <--- å¯¼å…¥jsonåº“
# å¯¼å…¥æˆ‘ä»¬çš„æ ¸å¿ƒ RAG å¼•æ“
from rag.streaming_pipeline import StreamingRagPipeline, StreamEventType, StreamEvent

# --- æ—¥å¿—é…ç½® ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- FastAPI åº”ç”¨å®ä¾‹ ---
app = FastAPI(
    title="ä¼ä¸šçº§é«˜æ€§èƒ½æµå¼RAG API",
    description="ä¸€ä¸ªåŸºäºFastAPIçš„ã€æ”¯æŒæµå¼å“åº”ã€æ‰¹é‡å¤„ç†å’Œæ™ºèƒ½åŒæ­¥çš„RAGç³»ç»ŸAPIã€‚",
    version="5.0.0",
)

# --- æ•°æ®æ¨¡å‹ (ç”¨äºè¯·æ±‚å’Œå“åº”ä½“) ---
class AskRequest(BaseModel):
    question: str = Field(..., min_length=1, description="ç”¨æˆ·æå‡ºçš„é—®é¢˜")
    categories: Optional[List[str]] = Field(None, description="é™å®šæ£€ç´¢çš„ç±»åˆ«åˆ—è¡¨ï¼Œä¸ºç©ºåˆ™æ£€ç´¢æ‰€æœ‰ç±»åˆ«")

class BatchAskRequest(BaseModel):
    questions: List[str] = Field(..., min_items=1, description="éœ€è¦æ‰¹é‡å¤„ç†çš„é—®é¢˜åˆ—è¡¨")

# --- å…¨å±€å•ä¾‹ï¼šRAG Pipeline ---
# åœ¨åº”ç”¨å¯åŠ¨æ—¶åˆ›å»ºpipelineå®ä¾‹ï¼Œç¡®ä¿å…¨å±€åªæœ‰ä¸€ä¸ªï¼Œé¿å…é‡å¤åŠ è½½æ¨¡å‹
pipeline: Optional[StreamingRagPipeline] = None

@app.on_event("startup")
async def startup_event():
    """FastAPIåº”ç”¨å¯åŠ¨æ—¶æ‰§è¡Œçš„äº‹ä»¶"""
    global pipeline
    logger.info("åº”ç”¨å¯åŠ¨ï¼Œæ­£åœ¨åˆå§‹åŒ–RAG Pipeline...")
    try:
        pipeline = StreamingRagPipeline()
        # é¦–æ¬¡å¯åŠ¨æ—¶ï¼Œå»ºè®®æ‰§è¡Œä¸€æ¬¡åŒæ­¥
        logger.info("é¦–æ¬¡å¯åŠ¨ï¼Œæ‰§è¡Œä¸€æ¬¡çŸ¥è¯†åº“åŒæ­¥...")
        await pipeline.sync_data_directory_async()
        logger.info("RAG Pipeline åˆå§‹åŒ–å’Œé¦–æ¬¡åŒæ­¥å®Œæˆã€‚")
    except Exception as e:
        logger.error(f"Pipelineåˆå§‹åŒ–å¤±è´¥: {e}", exc_info=True)
        # åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œåç»­çš„APIè°ƒç”¨ä¼šå¤±è´¥ï¼Œè¿™æ˜¯é¢„æœŸçš„
        pipeline = None

@app.on_event("shutdown")
async def shutdown_event():
    """FastAPIåº”ç”¨å…³é—­æ—¶æ‰§è¡Œçš„äº‹ä»¶"""
    if pipeline and hasattr(pipeline, 'executor'):
        logger.info("åº”ç”¨å…³é—­ï¼Œæ­£åœ¨å…³é—­çº¿ç¨‹æ± ...")
        pipeline.executor.shutdown(wait=True)
        logger.info("çº¿ç¨‹æ± å·²å…³é—­ã€‚")

# --- API Endpoints ---

@app.get("/", summary="å¥åº·æ£€æŸ¥", description="æ£€æŸ¥APIæœåŠ¡æ˜¯å¦æ­£åœ¨è¿è¡Œã€‚")
async def health_check():
    """æ ¹è·¯å¾„ï¼Œç”¨äºç®€å•çš„å¥åº·æ£€æŸ¥ã€‚"""
    if pipeline is None:
        raise HTTPException(status_code=503, detail="æœåŠ¡ä¸å¯ç”¨ï¼šRAG Pipelineåˆå§‹åŒ–å¤±è´¥ã€‚")
    return {"status": "ok", "message": "RAG API Service is running."}

@app.post("/sync", summary="åŒæ­¥çŸ¥è¯†åº“", description="å¼‚æ­¥è§¦å‘ä¸€æ¬¡çŸ¥è¯†åº“çš„å®Œå…¨åŒæ­¥ã€‚")
async def sync_knowledge_base():
    """
    è§¦å‘å¯¹æ‰€æœ‰æ•°æ®æºçš„æ™ºèƒ½åŒæ­¥ã€‚è¿™æ˜¯ä¸€ä¸ªè€—æ—¶æ“ä½œï¼ŒAPIä¼šç«‹å³è¿”å›ï¼ŒåŒæ­¥åœ¨åå°è¿›è¡Œã€‚
    """
    if pipeline is None:
        raise HTTPException(status_code=503, detail="æœåŠ¡ä¸å¯ç”¨ã€‚")
    
    # åœ¨åå°å¼‚æ­¥æ‰§è¡ŒåŒæ­¥ä»»åŠ¡ï¼Œä¸é˜»å¡APIå“åº”
    asyncio.create_task(pipeline.sync_data_directory_async())
    
    return JSONResponse(
        status_code=202, # 202 Accepted: è¯·æ±‚å·²è¢«æ¥å—ï¼Œä½†å¤„ç†å°šæœªå®Œæˆ
        content={"message": "çŸ¥è¯†åº“åŒæ­¥ä»»åŠ¡å·²åœ¨åå°å¯åŠ¨ã€‚"}
    )

@app.get("/stats", summary="è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯", description="è·å–å½“å‰çŸ¥è¯†åº“çš„è¯¦ç»†æ„æˆä¿¡æ¯ã€‚")
async def get_stats():
    """
    è¿”å›çŸ¥è¯†åº“çš„è¯¦ç»†ç»Ÿè®¡æ•°æ®ï¼ŒåŒ…æ‹¬å„ä¸ªç±»åˆ«çš„æ–‡æ¡£æ•°é‡å’Œæ¥æºã€‚
    """
    if pipeline is None:
        raise HTTPException(status_code=503, detail="æœåŠ¡ä¸å¯ç”¨ã€‚")
    
    # è¿™æ˜¯ä¸€ä¸ªå¿«é€Ÿçš„åŒæ­¥æ–¹æ³•ï¼Œå¯ä»¥ç›´æ¥è°ƒç”¨
    stats = pipeline.get_data_source_info()
    return JSONResponse(content=stats)

@app.get("/categories", summary="è·å–å¯ç”¨ç±»åˆ«", description="è·å–çŸ¥è¯†åº“ä¸­æ‰€æœ‰å¯ç”¨çš„æ–‡æ¡£ç±»åˆ«ã€‚")
async def get_categories():
    """
    è¿”å›ä¸€ä¸ªåŒ…å«æ‰€æœ‰å¯ç”¨ç±»åˆ«åŠå…¶æ–‡æ¡£å—æ•°é‡çš„å­—å…¸ã€‚
    """
    if pipeline is None:
        raise HTTPException(status_code=503, detail="æœåŠ¡ä¸å¯ç”¨ã€‚")
        
    categories = pipeline.get_available_categories()
    return JSONResponse(content=categories)

@app.post("/ask/stream", summary="æµå¼é—®ç­”", description="æ ¸å¿ƒçš„æµå¼é—®ç­”æ¥å£ï¼Œä½¿ç”¨Server-Sent Events (SSE)è¿›è¡Œæµå¼å“åº”ã€‚")
async def ask_streaming(request: AskRequest):
    """
    æ¥æ”¶ä¸€ä¸ªé—®é¢˜å’Œå¯é€‰çš„ç±»åˆ«ï¼Œé€šè¿‡SSEè¿”å›ä¸€ä¸ªäº‹ä»¶æµã€‚
    äº‹ä»¶ç±»å‹åŒ…æ‹¬ï¼šprocessing, generation_start, generation_chunk, generation_end, error, completeã€‚
    """
    if pipeline is None:
        raise HTTPException(status_code=503, detail="æœåŠ¡ä¸å¯ç”¨ã€‚")

    async def event_generator():
        try:
            if request.categories is not None:
                stream = pipeline.ask_with_categories_stream(request.question, request.categories)
            else:
                stream = pipeline.ask_stream(request.question)
            
            async for event in stream:
                # === ã€å…³é”®ä¿®æ­£ã€‘ ===
                # å°†æˆ‘ä»¬è‡ªå·±çš„StreamEventå¯¹è±¡è½¬æ¢ä¸ºå­—å…¸ï¼Œç„¶åå†åºåˆ—åŒ–æˆJSONå­—ç¬¦ä¸²
                # ä½œä¸ºsse-starletteçš„"data"å­—æ®µå‘é€å‡ºå»ã€‚
                # æˆ‘ä»¬è¿˜å¯ä»¥æŒ‡å®šä¸€ä¸ªäº‹ä»¶åç§°ï¼Œæ–¹ä¾¿å‰ç«¯æ ¹æ®åç§°æ¥ç›‘å¬ã€‚
                yield {
                    "event": event.type.value, # ä½¿ç”¨æˆ‘ä»¬è‡ªå·±çš„äº‹ä»¶ç±»å‹ä½œä¸ºSSEçš„äº‹ä»¶å
                    "data": json.dumps(event.to_dict()) # å°†æ•´ä¸ªäº‹ä»¶å¯¹è±¡ä½œä¸ºJSONæ•°æ®å‘é€
                }
        except Exception as e:
            logger.error(f"æµå¼é—®ç­”å¤„ç†å¤±è´¥: {e}", exc_info=True)
            # å¯¹äºé”™è¯¯ï¼Œä¹Ÿéµå¾ªåŒæ ·çš„æ ¼å¼
            error_event_data = StreamEvent(
                type=StreamEventType.ERROR,
                data={"error": f"æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {e}"},
                timestamp=time.time()
            ).to_dict()
            yield {
                "event": "error",
                "data": json.dumps(error_event_data)
            }

    return EventSourceResponse(event_generator())

@app.post("/ask/batch-stream", summary="å¹¶å‘æ‰¹é‡æµå¼é—®ç­”", description="å¹¶å‘å¤„ç†å¤šä¸ªé—®é¢˜ï¼Œå¹¶é€šè¿‡SSEæµå¼è¿”å›æ‰€æœ‰äº‹ä»¶ã€‚")
async def batch_ask_streaming(request: BatchAskRequest):
    """
    æ¥æ”¶ä¸€ä¸ªé—®é¢˜åˆ—è¡¨ï¼Œå¹¶å‘åœ°å¤„ç†å®ƒä»¬ï¼Œå¹¶é€šè¿‡å•ä¸ªSSEè¿æ¥æµå¼è¿”å›æ‰€æœ‰é—®é¢˜çš„äº‹ä»¶ã€‚
    äº‹ä»¶ä¼šåŒ…å«`batch_index`ç­‰å…ƒæ•°æ®ï¼Œä»¥ä¾¿å®¢æˆ·ç«¯åŒºåˆ†ã€‚
    """
    if pipeline is None:
        raise HTTPException(status_code=503, detail="æœåŠ¡ä¸å¯ç”¨ã€‚")

    async def batch_event_generator():
        try:
            async for event in pipeline.batch_ask_stream(request.questions):
                # === ã€å…³é”®ä¿®æ­£ã€‘ ===
                # å¯¹æ‰¹é‡æ¥å£ä¹Ÿåº”ç”¨åŒæ ·çš„æ ¼å¼è½¬æ¢
                yield {
                    "event": event.type.value,
                    "data": json.dumps(event.to_dict())
                }
        except Exception as e:
            logger.error(f"æ‰¹é‡æµå¼é—®ç­”å¤„ç†å¤±è´¥: {e}", exc_info=True)
            error_event_data = StreamEvent(
                type=StreamEventType.ERROR,
                data={"error": f"æœåŠ¡å™¨å†…éƒ¨é”™è¯¯: {e}"},
                timestamp=time.time()
            ).to_dict()
            yield {
                "event": "error",
                "data": json.dumps(error_event_data)
            }

    return EventSourceResponse(batch_event_generator())
# --- å¦‚ä½•è¿è¡Œ ---
# åœ¨ç»ˆç«¯ä¸­ï¼Œè¿›å…¥é¡¹ç›®æ ¹ç›®å½•ï¼Œç„¶åè¿è¡Œä»¥ä¸‹å‘½ä»¤:
# uvicorn api_server:app --reload
#
# --reload: ä»£ç æ›´æ”¹æ—¶è‡ªåŠ¨é‡å¯æœåŠ¡å™¨ï¼Œæ–¹ä¾¿å¼€å‘
#
# APIæ–‡æ¡£å°†è‡ªåŠ¨ç”Ÿæˆåœ¨: http://127.0.0.1:8000/docs
# å¦ä¸€ä¸ªUIç•Œé¢åœ¨: http://127.0.0.1:8000/redoc

if __name__ == "__main__":
    import uvicorn
    
    logger.info("Starting server programmatically...")
    
    # ä»¥ç¼–ç¨‹æ–¹å¼å¯åŠ¨Uvicorn
    # è¿™ç§æ–¹å¼å¯¹ç¯å¢ƒçš„ä¾èµ–æœ€å°
    uvicorn.run(
        # ç¬¬ä¸€ä¸ªå‚æ•° "api_server:app" å‘Šè¯‰uvicornåº”ç”¨å®ä¾‹åœ¨å“ªé‡Œ
        # å¦‚æœæ­¤æ–‡ä»¶å°±å«api_server.pyï¼Œå¯ä»¥ç›´æ¥å†™ "app"
        # ä½†ä¸ºäº†æ˜ç¡®ï¼Œå†™å…¨ "api_server:app" æ˜¯æœ€å¥½çš„å®è·µ
        "api_server:app", 
        host="127.0.0.1", 
        port=8000, 
        reload=True,  # å¼€å¯å†…ç½®çš„çƒ­é‡è½½åŠŸèƒ½
        log_level="info"
    )
```

## `streaming_main.py`

```python
# streaming_main.py - æ­£ç¡®çš„æµå¼å“åº”æ¼”ç¤º

import asyncio
import time
from rag.streaming_pipeline import StreamingRagPipeline, StreamEvent, StreamEventType


class StreamingDemo:
    """æµå¼å“åº”æ¼”ç¤º"""
    
    def __init__(self):
        self.rag = None
    
    async def initialize(self):
        """åˆå§‹åŒ–RAGç³»ç»Ÿ"""
        print("ğŸš€ åˆå§‹åŒ–æµå¼RAGç³»ç»Ÿ...")
        self.rag = StreamingRagPipeline()
        
        print("ğŸ“ åŒæ­¥æ•°æ®ç›®å½•...")
        await self.rag.sync_data_directory_async()
        print("âœ… åˆå§‹åŒ–å®Œæˆï¼\n")
    
    def display_event(self, event: StreamEvent):
        """æ˜¾ç¤ºäº‹ä»¶"""
        timestamp = time.strftime("%H:%M:%S", time.localtime(event.timestamp))
        
        if event.type == StreamEventType.PROCESSING:
            print(f"ğŸ”„ [{timestamp}] {event.data.get('message', '')}")
        
        elif event.type == StreamEventType.GENERATION_START:
            print(f"ğŸ’­ [{timestamp}] {event.data.get('message', '')}")
            print("ğŸ“ ç­”æ¡ˆ: ", end='', flush=True)  # å¼€å§‹ç­”æ¡ˆè¾“å‡ºè¡Œ
        
        elif event.type == StreamEventType.GENERATION_CHUNK:
            # âœ… è¿™é‡Œå±•ç¤ºçœŸæ­£çš„æµå¼è¾“å‡ºæ•ˆæœ
            chunk = event.data.get('chunk', '')
            print(chunk, end='', flush=True)
            
            # å¦‚æœæ˜¯çœŸæ­£çš„æµå¼LLMï¼Œchunkå¯èƒ½æ˜¯tokenè€Œä¸æ˜¯å­—ç¬¦
            # è¿™é‡Œå¯ä»¥æ ¹æ®chunkçš„é•¿åº¦æ¥åˆ¤æ–­æ˜¯å­—ç¬¦æµå¼è¿˜æ˜¯tokenæµå¼
            if len(chunk) > 1:
                # å¯èƒ½æ˜¯tokenæµå¼ï¼Œæ·»åŠ å°å»¶è¿Ÿä»¥ä¾¿è§‚å¯Ÿæ•ˆæœ
                import time as time_module
                time_module.sleep(0.01)
        
        elif event.type == StreamEventType.GENERATION_END:
            print()  # æ¢è¡Œ
            sources = event.data.get('source_documents', [])
            if sources:
                print(f"\nğŸ“š [{timestamp}] å‚è€ƒæ–‡æ¡£:")
                for doc in sources:
                    print(f"    ğŸ“„ {doc['source']} (ç±»åˆ«: {doc['category']})")
            else:
                print(f"\nâœ… [{timestamp}] {event.data.get('message', '')}")
        
        elif event.type == StreamEventType.ERROR:
            print(f"\nâŒ [{timestamp}] é”™è¯¯: {event.data.get('error', '')}")
        
        elif event.type == StreamEventType.COMPLETE:
            print(f"\nğŸ‰ [{timestamp}] {event.data.get('message', '')}")
    
    async def demo_correct_streaming(self):
        """æ¼”ç¤ºæ­£ç¡®çš„æµå¼å“åº”"""
        print("=" * 60)
        print("âœ… æ­£ç¡®çš„æµå¼å“åº”æ¼”ç¤º")
        print("=" * 60)
        print("ğŸ’¡ ç‰¹ç‚¹: åªæœ‰æœ€ç»ˆç­”æ¡ˆæ˜¯æµå¼çš„ï¼Œä¸­é—´å¤„ç†ä¸æµå¼")
        
        question = "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"
        print(f"\nâ“ é—®é¢˜: {question}\n")
        
        async for event in self.rag.ask_stream(question):
            self.display_event(event)
    
    async def demo_performance_comparison(self):
        """æ¼”ç¤ºæ€§èƒ½å¯¹æ¯”ï¼šæ­£ç¡®æµå¼ vs éæµå¼"""
        print("\n" + "=" * 60)
        print("âš¡ æ€§èƒ½å¯¹æ¯”ï¼šæ­£ç¡®æµå¼ vs éæµå¼")
        print("=" * 60)
        
        question = "Pythonæœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ"
        print(f"â“ æµ‹è¯•é—®é¢˜: {question}\n")
        
        # 1. éæµå¼ç‰ˆæœ¬
        print("ğŸ”„ éæµå¼ç‰ˆæœ¬:")
        start_time = time.time()
        result = await self.rag.ask_async(question)
        non_streaming_time = time.time() - start_time
        
        print(f"  â±ï¸ æ€»è€—æ—¶: {non_streaming_time:.2f}ç§’")
        print(f"  ğŸ“ ç­”æ¡ˆ: {result['result'][:100]}...")
        
        # 2. æ­£ç¡®çš„æµå¼ç‰ˆæœ¬
        print(f"\nâš¡ æ­£ç¡®çš„æµå¼ç‰ˆæœ¬:")
        start_time = time.time()
        first_chunk_time = None
        processing_done_time = None
        generation_start_time = None
        
        async for event in self.rag.ask_stream(question):
            current_time = time.time()
            
            if event.type == StreamEventType.PROCESSING:
                print(f"  ğŸ”„ [{current_time - start_time:.2f}s] {event.data['message']}")
            
            elif event.type == StreamEventType.GENERATION_START:
                generation_start_time = current_time
                processing_done_time = current_time - start_time
                print(f"  ğŸ’­ [{processing_done_time:.2f}s] å¼€å§‹æµå¼ç”Ÿæˆç­”æ¡ˆ...")
            
            elif event.type == StreamEventType.GENERATION_CHUNK:
                if first_chunk_time is None:
                    first_chunk_time = current_time
                    first_chunk_latency = first_chunk_time - start_time
                    print(f"  âš¡ [{first_chunk_latency:.2f}s] é¦–ä¸ªå­—ç¬¦è¾“å‡ºï¼")
                    print(f"  ğŸ“ ç­”æ¡ˆ: ", end='', flush=True)
                
                print(event.data.get('chunk', ''), end='', flush=True)
            
            elif event.type == StreamEventType.COMPLETE:
                total_time = current_time - start_time
                print(f"\n  âœ… [{total_time:.2f}s] å®Œæˆ")
        
        # 3. æ€§èƒ½åˆ†æ
        if first_chunk_time and processing_done_time:
            print(f"\nğŸ“Š æ€§èƒ½åˆ†æ:")
            print(f"  å¤„ç†é˜¶æ®µè€—æ—¶: {processing_done_time:.2f}ç§’")
            print(f"  é¦–å­—ç¬¦å»¶è¿Ÿ: {first_chunk_time - start_time:.2f}ç§’")
            print(f"  ç”¨æˆ·æ„ŸçŸ¥å»¶è¿Ÿ: {first_chunk_time - generation_start_time:.2f}ç§’ (ç”Ÿæˆå¼€å§‹åˆ°é¦–å­—ç¬¦)")
            
            print(f"\nğŸ’¡ å…³é”®æ´å¯Ÿ:")
            print(f"  - ä¸­é—´å¤„ç†ä¸éœ€è¦æµå¼ï¼Œç”¨æˆ·ä¸å…³å¿ƒå…·ä½“æ­¥éª¤")
            print(f"  - æµå¼çš„ä»·å€¼åœ¨äºç­”æ¡ˆç”Ÿæˆé˜¶æ®µ")
            print(f"  - ç”¨æˆ·çœ‹åˆ°ç­”æ¡ˆå¼€å§‹ç”Ÿæˆçš„å»¶è¿Ÿå¾ˆçŸ­")
    
    async def demo_interactive_experience(self):
        """æ¼”ç¤ºäº¤äº’ä½“éªŒ"""
        print("\n" + "=" * 60)
        print("ğŸ’¬ äº¤äº’ä½“éªŒæ¼”ç¤º")
        print("=" * 60)
        print("è¾“å…¥é—®é¢˜ä½“éªŒæ­£ç¡®çš„æµå¼å“åº”ï¼ˆè¾“å…¥ 'quit' é€€å‡ºï¼‰:")
        
        while True:
            try:
                question = input("\nâ“ è¯·è¾“å…¥é—®é¢˜: ").strip()
                
                if question.lower() in ['quit', 'exit', 'é€€å‡º']:
                    print("ğŸ‘‹ å†è§ï¼")
                    break
                
                if not question:
                    continue
                
                print()  # ç©ºè¡Œ
                
                async for event in self.rag.ask_stream(question):
                    self.display_event(event)
                
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­ï¼Œé€€å‡ºç¨‹åº")
                break
            except Exception as e:
                print(f"\nâŒ å¤„ç†é”™è¯¯: {e}")
    
    async def demo_batch_processing(self):
        """æ¼”ç¤ºæ‰¹é‡å¤„ç†"""
        print("\n" + "=" * 60)
        print("ğŸ“¦ æ‰¹é‡å¤„ç†æ¼”ç¤º")
        print("=" * 60)
        
        questions = [
            "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ",
            "Pythonçš„ä¸»è¦ç‰¹ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ",
            "å¦‚ä½•ä¼˜åŒ–ç¨‹åºæ€§èƒ½ï¼Ÿ"
        ]
        
        print(f"ğŸ“ æ‰¹é‡å¤„ç† {len(questions)} ä¸ªé—®é¢˜:")
        for i, q in enumerate(questions, 1):
            print(f"  {i}. {q}")
        print()
        
        async for event in self.rag.batch_ask_stream(questions):
            self.display_event(event)


async def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    demo = StreamingDemo()
    
    try:
        # åˆå§‹åŒ–ç³»ç»Ÿ
        await demo.initialize()
        
        # è¿è¡Œæ¼”ç¤º
        await demo.demo_correct_streaming()
        await demo.demo_performance_comparison()
        await demo.demo_batch_processing()
        
        # å¯é€‰ï¼šäº¤äº’æ¼”ç¤º
        # await demo.demo_interactive_experience()
        
        print("\n" + "=" * 60)
        print("ğŸ‰ æ­£ç¡®çš„æµå¼å“åº”æ¼”ç¤ºå®Œæˆï¼")
        print("=" * 60)
        print("ğŸ’¡ æ­£ç¡®æµå¼å“åº”çš„ç‰¹ç‚¹:")
        print("  1. ä¸­é—´å¤„ç†è¿‡ç¨‹ä¸æµå¼ï¼ŒåªåšçŠ¶æ€é€šçŸ¥")
        print("  2. åªæœ‰æœ€ç»ˆç­”æ¡ˆç”Ÿæˆæ˜¯çœŸæ­£çš„æµå¼è¾“å‡º")
        print("  3. ç”¨æˆ·ä½“éªŒèšç„¦åœ¨çœ‹åˆ°ç­”æ¡ˆé€æ­¥ç”Ÿæˆ")
        print("  4. å‡å°‘ä¸å¿…è¦çš„äº‹ä»¶ï¼Œæé«˜æ•ˆç‡")
        print("  5. æ›´ç¬¦åˆç”¨æˆ·çš„å®é™…éœ€æ±‚å’ŒæœŸæœ›")
        
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­ï¼Œç¨‹åºé€€å‡º")
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    # è¿è¡Œæ­£ç¡®çš„æµå¼å“åº”æ¼”ç¤º
    asyncio.run(main())
```

## `streaming_web_demo.py`

```python
"""
æµå¼RAG Webæ¼”ç¤º - FastAPI + WebSocketå®ç°
å±•ç¤ºæ­£ç¡®çš„æµå¼å“åº”ç†å¿µï¼šåªæœ‰ç­”æ¡ˆç”Ÿæˆæ˜¯æµå¼çš„
"""

import asyncio
import json
import logging
from pathlib import Path
from typing import AsyncGenerator

from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from fastapi.responses import HTMLResponse
from fastapi.staticfiles import StaticFiles

# å¯¼å…¥æˆ‘ä»¬çš„æµå¼RAGç®¡é“
import sys
sys.path.append(str(Path(__file__).parent))

from rag.streaming_pipeline import StreamingRagPipeline

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="æµå¼RAGæ¼”ç¤º", description="åŸºäºFastAPI + WebSocketçš„æµå¼é—®ç­”ç³»ç»Ÿ")

# å…¨å±€RAGç®¡é“å®ä¾‹
rag_pipeline = None

@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨æ—¶åˆå§‹åŒ–RAGç®¡é“å’Œçƒ­é‡è½½åŠŸèƒ½"""
    global rag_pipeline
    try:
        logger.info("æ­£åœ¨åˆå§‹åŒ–RAGç®¡é“...")
        rag_pipeline = StreamingRagPipeline()
        logger.info("RAGç®¡é“åˆå§‹åŒ–å®Œæˆ")
        
        # å¯ç”¨çƒ­é‡è½½åŠŸèƒ½
        from rag.hot_reload_manager import enable_hot_reload
        if enable_hot_reload():
            logger.info("ğŸ”¥ çƒ­é‡è½½åŠŸèƒ½å·²å¯ç”¨")
        else:
            logger.warning("âš ï¸ çƒ­é‡è½½åŠŸèƒ½å¯ç”¨å¤±è´¥")
            
    except Exception as e:
        logger.error(f"åˆå§‹åŒ–å¤±è´¥: {e}")
        raise

# === æ·»åŠ  shutdown äº‹ä»¶ ===
@app.on_event("shutdown")
async def shutdown_event():
    """åº”ç”¨å…³é—­æ—¶æ¸…ç†èµ„æº"""
    global rag_pipeline
    
    # åœæ­¢çƒ­é‡è½½ç›‘æ§
    from rag.hot_reload_manager import disable_hot_reload
    disable_hot_reload()
    logger.info("ğŸ›‘ çƒ­é‡è½½ç›‘æ§å·²åœæ­¢")
    
    # æ¸…ç†çº¿ç¨‹æ± 
    if rag_pipeline and hasattr(rag_pipeline, 'executor'):
        logger.info("åº”ç”¨æ­£åœ¨å…³é—­ï¼Œæ¸…ç†çº¿ç¨‹æ± ...")
        rag_pipeline.executor.shutdown(wait=True)
        logger.info("çº¿ç¨‹æ± å·²æˆåŠŸå…³é—­ã€‚")

@app.get("/")
async def get_homepage():
    """è¿”å›Webç•Œé¢HTML"""
    html_content = """
    <!DOCTYPE html>
    <html>
    <head>
        <title>æµå¼RAGé—®ç­”ç³»ç»Ÿ</title>
        <meta charset="utf-8">
        <style>
            body {
                font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
                max-width: 800px;
                margin: 0 auto;
                padding: 20px;
                background-color: #f5f5f5;
            }
            .container {
                background: white;
                border-radius: 10px;
                padding: 30px;
                box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            }
            h1 {
                color: #333;
                text-align: center;
                margin-bottom: 30px;
            }
            .chat-container {
                height: 400px;
                border: 1px solid #ddd;
                border-radius: 8px;
                padding: 15px;
                overflow-y: auto;
                background-color: #fafafa;
                margin-bottom: 20px;
            }
            .message {
                margin-bottom: 15px;
                padding: 10px;
                border-radius: 8px;
            }
            .user-message {
                background-color: #007bff;
                color: white;
                margin-left: 20%;
                text-align: right;
            }
            .bot-message {
                background-color: #e9ecef;
                color: #333;
                margin-right: 20%;
            }
            .status-message {
                background-color: #fff3cd;
                color: #856404;
                font-style: italic;
                text-align: center;
                border: 1px solid #ffeaa7;
            }
            .input-container {
                display: flex;
                gap: 10px;
            }
            #questionInput {
                flex: 1;
                padding: 12px;
                border: 1px solid #ddd;
                border-radius: 6px;
                font-size: 16px;
            }
            #sendButton {
                padding: 12px 24px;
                background-color: #007bff;
                color: white;
                border: none;
                border-radius: 6px;
                cursor: pointer;
                font-size: 16px;
            }
            #sendButton:hover {
                background-color: #0056b3;
            }
            #sendButton:disabled {
                background-color: #6c757d;
                cursor: not-allowed;
            }
            .connection-status {
                text-align: center;
                padding: 10px;
                margin-bottom: 20px;
                border-radius: 6px;
            }
            .connected {
                background-color: #d4edda;
                color: #155724;
                border: 1px solid #c3e6cb;
            }
            .disconnected {
                background-color: #f8d7da;
                color: #721c24;
                border: 1px solid #f5c6cb;
            }
        </style>
    </head>
    <body>
        <div class="container">
            <h1>ğŸŒŠ æµå¼RAGé—®ç­”ç³»ç»Ÿ</h1>
            
            <div id="connectionStatus" class="connection-status disconnected">
                æ­£åœ¨è¿æ¥...
            </div>
            
            <div id="chatContainer" class="chat-container">
                <div class="message status-message">
                    æ¬¢è¿ä½¿ç”¨æµå¼RAGé—®ç­”ç³»ç»Ÿï¼è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ã€‚
                </div>
            </div>
            
            <div class="input-container">
                <input type="text" id="questionInput" placeholder="è¯·è¾“å…¥æ‚¨çš„é—®é¢˜..." />
                <button id="sendButton" disabled>å‘é€</button>
            </div>
        </div>

        <script>
            let ws = null;
            let isConnected = false;
            
            const chatContainer = document.getElementById('chatContainer');
            const questionInput = document.getElementById('questionInput');
            const sendButton = document.getElementById('sendButton');
            const connectionStatus = document.getElementById('connectionStatus');
            
            function connectWebSocket() {
                const wsUrl = `ws://${window.location.host}/ws`;
                ws = new WebSocket(wsUrl);
                
                ws.onopen = function(event) {
                    console.log('WebSocketè¿æ¥å·²å»ºç«‹');
                    isConnected = true;
                    updateConnectionStatus(true);
                    sendButton.disabled = false;
                };
                
                ws.onmessage = function(event) {
                    const data = JSON.parse(event.data);
                    handleMessage(data);
                };
                
                ws.onclose = function(event) {
                    console.log('WebSocketè¿æ¥å·²å…³é—­');
                    isConnected = false;
                    updateConnectionStatus(false);
                    sendButton.disabled = true;
                    
                    // å°è¯•é‡è¿
                    setTimeout(connectWebSocket, 3000);
                };
                
                ws.onerror = function(error) {
                    console.error('WebSocketé”™è¯¯:', error);
                };
            }
            
            function updateConnectionStatus(connected) {
                if (connected) {
                    connectionStatus.textContent = 'âœ… å·²è¿æ¥';
                    connectionStatus.className = 'connection-status connected';
                } else {
                    connectionStatus.textContent = 'âŒ è¿æ¥æ–­å¼€';
                    connectionStatus.className = 'connection-status disconnected';
                }
            }
            
            function addMessage(content, type) {
                const messageDiv = document.createElement('div');
                messageDiv.className = `message ${type}-message`;
                messageDiv.textContent = content;
                chatContainer.appendChild(messageDiv);
                chatContainer.scrollTop = chatContainer.scrollHeight;
                return messageDiv;
            }
            
            let currentBotMessage = null;
            
            function handleMessage(data) {
                switch (data.type) {
                    case 'status':
                        addMessage(data.message, 'status');
                        break;
                        
                    case 'answer_start':
                        // å¼€å§‹æ¥æ”¶ç­”æ¡ˆï¼Œåˆ›å»ºæ–°çš„æ¶ˆæ¯å®¹å™¨
                        currentBotMessage = addMessage('', 'bot');
                        break;
                        
                    case 'answer_chunk':
                        // æµå¼æ›´æ–°ç­”æ¡ˆå†…å®¹
                        if (currentBotMessage) {
                            currentBotMessage.textContent += data.content;
                            chatContainer.scrollTop = chatContainer.scrollHeight;
                        }
                        break;
                        
                    case 'answer_complete':
                        // ç­”æ¡ˆç”Ÿæˆå®Œæˆ
                        currentBotMessage = null;
                        sendButton.disabled = false;
                        sendButton.textContent = 'å‘é€';
                        break;
                        
                    case 'error':
                        addMessage(`é”™è¯¯: ${data.message}`, 'status');
                        sendButton.disabled = false;
                        sendButton.textContent = 'å‘é€';
                        break;
                }
            }
            
            function sendQuestion() {
                const question = questionInput.value.trim();
                if (!question || !isConnected) return;
                
                // æ˜¾ç¤ºç”¨æˆ·é—®é¢˜
                addMessage(question, 'user');
                
                // å‘é€åˆ°æœåŠ¡å™¨
                ws.send(JSON.stringify({
                    type: 'question',
                    content: question
                }));
                
                // æ¸…ç©ºè¾“å…¥æ¡†å¹¶ç¦ç”¨å‘é€æŒ‰é’®
                questionInput.value = '';
                sendButton.disabled = true;
                sendButton.textContent = 'å¤„ç†ä¸­...';
            }
            
            // äº‹ä»¶ç›‘å¬
            sendButton.addEventListener('click', sendQuestion);
            
            questionInput.addEventListener('keypress', function(e) {
                if (e.key === 'Enter') {
                    sendQuestion();
                }
            });
            
            // åˆå§‹åŒ–è¿æ¥
            connectWebSocket();
        </script>
    </body>
    </html>
    """
    return HTMLResponse(content=html_content)

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    """WebSocketç«¯ç‚¹å¤„ç†æµå¼é—®ç­”"""
    await websocket.accept()
    logger.info("WebSocketè¿æ¥å·²å»ºç«‹")
    
    try:
        while True:
            # æ¥æ”¶å®¢æˆ·ç«¯æ¶ˆæ¯
            data = await websocket.receive_text()
            message = json.loads(data)
            
            if message["type"] == "question":
                question = message["content"]
                logger.info(f"æ”¶åˆ°é—®é¢˜: {question}")
                
                # å¤„ç†é—®é¢˜å¹¶æµå¼è¿”å›ç­”æ¡ˆ
                await handle_question(websocket, question)
                
    except WebSocketDisconnect:
        logger.info("WebSocketè¿æ¥å·²æ–­å¼€")
    except Exception as e:
        logger.error(f"WebSocketå¤„ç†é”™è¯¯: {e}")
        await websocket.send_text(json.dumps({
            "type": "error",
            "message": str(e)
        }))

async def handle_question(websocket: WebSocket, question: str):
    """å¤„ç†é—®é¢˜å¹¶æµå¼è¿”å›ç­”æ¡ˆ"""
    try:
        answer_started = False
        
        # ä½¿ç”¨æµå¼RAGç®¡é“ç”Ÿæˆç­”æ¡ˆ
        async for event in rag_pipeline.ask_stream(question):
            if event.type.value == "processing":
                # å¤„ç†çŠ¶æ€æ›´æ–°
                await websocket.send_text(json.dumps({
                    "type": "status",
                    "message": f"ğŸ” {event.data.get('message', 'æ­£åœ¨å¤„ç†...')}"
                }))
                
            elif event.type.value == "generation_start":
                # å¼€å§‹ç”Ÿæˆç­”æ¡ˆ
                if not answer_started:
                    await websocket.send_text(json.dumps({
                        "type": "answer_start"
                    }))
                    answer_started = True
                    
            elif event.type.value == "generation_chunk":
                # æµå¼ç­”æ¡ˆç‰‡æ®µ
                chunk = event.data.get("chunk", "")
                if chunk.strip():  # åªå‘é€éç©ºå†…å®¹
                    await websocket.send_text(json.dumps({
                        "type": "answer_chunk",
                        "content": chunk
                    }))
                    
            elif event.type.value == "generation_end":
                # ç­”æ¡ˆç”Ÿæˆå®Œæˆ
                await websocket.send_text(json.dumps({
                    "type": "answer_complete"
                }))
                
            elif event.type.value == "error":
                # é”™è¯¯å¤„ç†
                await websocket.send_text(json.dumps({
                    "type": "error",
                    "message": event.data.get("error", "æœªçŸ¥é”™è¯¯")
                }))
                return
                
            elif event.type.value == "complete":
                # æ•´ä¸ªæµç¨‹å®Œæˆ
                if not answer_started:
                    # å¦‚æœæ²¡æœ‰æµå¼ç­”æ¡ˆï¼Œå¯èƒ½æ˜¯ç›´æ¥è¿”å›äº†ç»“æœ
                    await websocket.send_text(json.dumps({
                        "type": "answer_complete"
                    }))
        
    except Exception as e:
        logger.error(f"å¤„ç†é—®é¢˜æ—¶å‡ºé”™: {e}")
        await websocket.send_text(json.dumps({
            "type": "error",
            "message": f"å¤„ç†é—®é¢˜æ—¶å‡ºé”™: {str(e)}"
        }))

if __name__ == "__main__":
    import uvicorn
    
    print("ğŸŒŠ å¯åŠ¨æµå¼RAG Webæ¼”ç¤º...")
    print("ğŸ“± è®¿é—®åœ°å€: http://localhost:8000")
    print("ğŸ’¡ è¿™ä¸ªæ¼”ç¤ºå±•ç¤ºäº†æ­£ç¡®çš„æµå¼å“åº”ç†å¿µï¼šåªæœ‰ç­”æ¡ˆç”Ÿæˆæ˜¯æµå¼çš„")
    
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        log_level="info"
    )
```

